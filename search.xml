<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2022/05/18/a-li-mian-shi-zi-liao/"/>
      <url>/2022/05/18/a-li-mian-shi-zi-liao/</url>
      
        <content type="html"><![CDATA[<p><strong>导读</strong></p><p>  最近几年，江湖中一直流传，算法岗内卷严重的传闻。让无数想转行、或者毕业的侠士，望而却步。但机会是留给有准备的人的，如何有计划的准备非常关键。</p><p>  因此，大白产生构建一套算法岗武林秘籍的想法，将江湖中各个大厂可以搜集到的面试资料，按照逻辑框架，每个大厂都整理成一篇面经。</p><p>  让大家有计划，有目标的准备，希望为大家行走江湖，提供一些便利。</p><p><strong>阿里巴巴算法秘籍目录</strong></p><blockquote><p><strong>1 阿里巴巴面经汇总资料</strong></p><p>1.1 面经汇总参考资料</p><p>1.2 面经涉及招聘岗位</p><p>1.3 面试流程时间安排</p><p>1.4 阿里巴巴面试心得汇总</p><p><strong>2 阿里巴巴面经涉及基础知识点</strong></p><p>2.1 图像处理基础</p><p>2.2 深度学习：CNN卷积神经网络方面</p><p>2.3 深度学习：RNN递归神经网络方面</p><p>2.4 深度学习：CNN&amp;RNN常问通用知识点</p><p>2.5 机器学习方面</p><p>2.6 深度学习&amp;机器学习面经通用知识点</p><p><strong>3 阿里巴巴面经涉及项目知识点</strong></p><p>3.1 深度学习：CNN卷积神经网络方面</p><p>3.2 深度学习：RNN递归神经网络方面</p><p>3.3 强化学习方面</p><p>3.4 机器学习方面</p><p><strong>4 数据结构与算法分析相关知识点</strong></p><p>4.1 数据结构与算法分析</p><p>4.2 算法思想实战及智力题</p><p>4.3 其他方面</p><p>4.4 Leetcode&amp;剑指offer原题</p><p><strong>5 编程高频问题：Python&amp;C/C++方面</strong></p><p>5.1 python方面</p><p>5.2 C/C++方面</p><p><strong>6  操作系统高频问题：数据库&amp;线程&amp;常用命令等</strong></p><p>6.1 数据库方面</p><p>6.2 操作系统方面</p><p><strong>7 技术&amp;产品&amp;开放性问题</strong></p><p>7.1 技术方面</p><p>7.2 产品方面</p><p>7.3 开放性问题            </p></blockquote><h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="1-阿里巴巴面经汇总资料"><a href="#1-阿里巴巴面经汇总资料" class="headerlink" title="1 阿里巴巴面经汇总资料"></a><strong>1</strong> <strong>阿里巴巴面经汇总资料</strong></h1><p><img src="https://mmbiz.qpic.cn/mmbiz_png/Zibun5D7npIMPE0m2oQe9ExZ7Qgz2QbgicQEfmTtDY6IF5xBvbPNxIpSadEkvrzWS0uGQLCNibu353H4skQcjqvFA/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></p><h2 id="1-1-面经汇总参考资料"><a href="#1-1-面经汇总参考资料" class="headerlink" title="1.1 面经汇总参考资料"></a><strong>1.1</strong> <strong>面经汇总参考资料</strong></h2><p>**<br>**</p><p><strong>① 参考资料：</strong></p><p>（1）牛客网：阿里巴巴面经-201篇：</p><p><a href="https://www.nowcoder.com/discuss/experience?tagId=645&amp;order=3&amp;companyId=134&amp;phaseId=0">https://www.nowcoder.com/discuss/experience?tagId=645&amp;order=3&amp;companyId=134&amp;phaseId=0</a></p><p>（2）知乎面经：</p><p><a href="https://www.zhihu.com/search?type=content&amp;q=%E7%AE%97%E6%B3%95%E9%9D%A2%E7%BB%8F">https://www.zhihu.com/search?type=content&amp;q=%E7%AE%97%E6%B3%95%E9%9D%A2%E7%BB%8F</a></p><p>（3）面试圈：</p><p><a href="http://www.mianshigee.com/interview/t17">http://www.mianshigee.com/interview/t17</a></p><p><strong>②</strong> <strong>面经框架&amp;答案&amp;目录&amp;心得：</strong></p><p>（1）面经框架及参考答案：</p><p><a href="https://docs.qq.com/doc/DWGZGc0ZVYmZCUFNO">https://docs.qq.com/doc/DWGZGc0ZVYmZCUFNO</a></p><p>（2）大厂目录及整理心得：</p><p><a href="https://docs.qq.com/doc/DWGZGc0ZVYmZCUFNO">https://docs.qq.com/doc/DWGZGc0ZVYmZCUFNO</a></p><h2 id="1-2-面经涉及招聘岗位"><a href="#1-2-面经涉及招聘岗位" class="headerlink" title="1.2 面经涉及招聘岗位"></a><strong>1.2 面经涉及招聘岗位</strong></h2><p>**<br>**</p><p><strong>（1）</strong>  <strong>实习岗位类</strong></p><p>【蚂蚁金服机器学习实习】、【达摩院研究型实习生】、【达摩院自然语言实习生】、【蚂蚁金服风控算法实习生】、【阿里云实习】</p><p><strong>（2）</strong>  <strong>全职岗位类</strong></p><p>【机器视觉算法工程师】、【智能事业服务部算法工程师】、【NLP算法工程师】、【达摩院计算机视觉】、【onsite算法工程师】、【消费者bg的软件算法工程师】、【蚂蚁金服算法工程师】、【机器学习算法工程师】、【大文娱算法工程师】、【阿里UC神马搜索】、【阿里口碑机器学习】、【阿里新零售天猫机器学习】、【阿里搜索部算法工程师】、【新零售技术事业群CCO技术部自然语言处理】、【阿里大文娱机器学习】、【淘宝算法岗】、【阿里优酷用户增长组】、【阿里飞猪算法工程师】、【企业金融工程师】、【达摩院NLP工程师】、【数据分析/机器学习工程师】、【阿里健康算法工程师】、【推荐算法工程师】、【lazada算法工程师】、【蚂蚁金服人工智能部门算法工程师】、【淘宝技术部算法岗】</p><h2 id="1-3-面试流程时间安排"><a href="#1-3-面试流程时间安排" class="headerlink" title="1.3 面试流程时间安排"></a><strong>1.3</strong> <strong>面试流程时间安排</strong></h2><p><img src="https://mmbiz.qpic.cn/mmbiz_png/Zibun5D7npIMPE0m2oQe9ExZ7Qgz2QbgicyNg5DDdYcmDvY2RUKrjuIIlmEjmUkpf0VMen3Z3ia3NMmPuM7ha6cvw/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></p><p><strong>PS****：</strong>以上流程为大白总结归纳所得，以供参考。</p><p><strong>其他注意点：</strong></p><p>● 有时是2技术+交叉面+HR面、有的也是3技术+交叉+HR</p><p>● 第一面也可能是简历面</p><p>● 个别部门参照：</p><p>达摩院实习生是3面技术+1交叉面+HR面</p><p>蚂蚁金服是1笔试+6面试</p><p>搜索部是3面+交叉面+HR面</p><h2 id="1-4-阿里巴巴面试心得汇总"><a href="#1-4-阿里巴巴面试心得汇总" class="headerlink" title="1.4 阿里巴巴面试心得汇总"></a><strong>1.4</strong> <strong>阿里巴巴面试心得汇总</strong></h2><p>**<br>**</p><p>阿里巴巴的面试资料很多，下面大白将很多面试者的心得也提取出来，便于大家在准备面试时，有所倾重：</p><p>★ 项目经历很重要，但是一定要对自己项目的每个细节了如指掌，挨个聊项目，每一个都问得非常细,要有一些有深度的思考，这是面试官期待看到的</p><p>★ 阿里流程会比较长，需要耐心一些，如果可以的话，大家一定要找一个靠谱的师兄师姐内推，通过他们可以提前知道接下来的流程。</p><p>★ 自己的论文，项目一定要讲清楚（背的滚瓜烂熟，阿里还是很重视论文项目的，对算法题要求没有那么高）</p><p>★ 每一轮面试都要认真对待，即使是交叉面和hr面也要好好准备 （我就是到了交叉面有点浮躁，面试完感觉要凉凉了）， hr面我就老实了（毕竟hr具有一票否决权）， 提前准备了好多、常问的问题（你的优点缺点，最近看什么书，对福报的理解，部门的职责，团队的职责，价值观相关，还好问的我问题比较常规）</p><p>★ 阿里笔试要认真做（题目灵活性还是很大），然后积极找找师兄内推还是很有机会参加面试的。面试官很看重基本功，和逻辑思考能力。有些项目的问题可能自己也没考虑过，不过也不要担心，重要的是如何去分析和解决那个问题。所以感觉考察的方面还是挺综合的。</p><p>★ 对于简历上的知识点，在深挖的同时，注意广度，并且还要注意凸出自己的方向，不能因为广而浅尝辄止，那样容易给面试官留下不好的感受。</p><p>★ 阿里的考察非常全面，一二面考察偏重基础，四面考察偏重业务，交叉面比较简短。笔者为通信专业，非科班，这里要提醒非科班的同学一定要重视基础，尤其是数据结构，不能光刷题，要系统的仔细的学习这门课程！！！</p><p>★ 面试官的问题更加全局化，站在业务和整体的角度看问题，比起算法细节，更多的问的是一种思维方式，为什么选择这个办法而不选择那个办法？为什么这个方法的结果比那个好？还是要多学习多思考。</p><p>★ 阿里的感觉是比较重视工程的能力，你是不是可以应对量级较大的数据，很多模型的存在很依赖于要达到什么样的目的，数据的关注的重点：数据质量，数据你怎么处理，怎么使用，比模型调参更体现了你的能力。</p><p>★ 不会的千万不要往简历上放，会的能说的就多说点，阿里的题比较开放，给你大的发挥空间，也可以很快的测出你的能力，阿里的实习给我实践性的启迪很大，就是让我觉得，我是来解决问题的，那解决问题比较重要的点在哪，不能只关注部分环节。</p><p>★ 简历首先最好有点能了解透彻的项目，简历上涉及的基础算法都要搞懂，面试有一定技巧性所以可以先去面几家练练手，算法题肯定要刷的，统计学习方法要看的，深度学习模型调优什么的。</p><p>★ 既要深度也要广度，发散性的问题，常常会问，为什么要这样？为什么不要那样？有什么好处？</p><p>★ 简洁，不要扯东扯西，不要发散思维，这样会妨碍面试官了解到他想得到的信息。当然，如果你知道他想问什么，往那方面发散是最好不过的了。但是一开始最好还是让面试官引导，自己先摸清面试官的意图，面试官的技术栈。   </p><p>★ 蚂蚁非常看重的是你对于自己研究领域的思考和实践，比较注重你是否具有较高的学习热情和个人潜力</p><p>★ 阿里巴巴在编程语言上甚至落地分布式的问题都可能问的很深。。一个学弟搞图像检测的，都聊到分布式负载均衡和反向代理了。</p><p>★ 深挖你的项目经历，首先让你概括性描述一下你的项目主要内容，以及你在里边儿承担的具体工作，然后开始一点一点扣项目经历。（真的要好好准备，而且还要思考一下为什么这么做，以及还有其他什么方法，各种不同方法之间的对比，问的特别细致反正，但感觉这也看面试官吧）</p><p>★ 阿里的HR权利很大，即使面到了最后一面HR面还是有很大可能会被挂掉，一句话夸张的概括一下阿里HR的存在吧，阿里技术部的面试官是在给HR部门招人，而不是HR再给技术部招人。</p><h1 id="2-阿里巴巴面经涉及基础知识点"><a href="#2-阿里巴巴面经涉及基础知识点" class="headerlink" title="2 阿里巴巴面经涉及基础知识点"></a><strong>2</strong> <strong>阿里巴巴面经涉及基础知识点</strong></h1><p><img src="https://mmbiz.qpic.cn/mmbiz_png/Zibun5D7npIMPE0m2oQe9ExZ7Qgz2QbgicMXgCICZcAF2OPgNAg48ialGbnlicfJzoRIpoTVDzyDvCCE518GJV7arg/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></p><h2 id="2-1-图像处理基础"><a href="#2-1-图像处理基础" class="headerlink" title="2.1 图像处理基础"></a><strong>2.1</strong> <strong>图像处理基础</strong></h2><h3 id="2-1-1-讲解相关原理"><a href="#2-1-1-讲解相关原理" class="headerlink" title="2.1.1 讲解相关原理"></a><strong>2.1.1 讲解相关原理</strong></h3><p>**<br>**</p><p>● 计算连通域的个数？算法复杂度？</p><p>● 如何求边缘，45°边缘，高斯滤波喝双边滤波？</p><h3 id="2-1-2-手写算法代码"><a href="#2-1-2-手写算法代码" class="headerlink" title="2.1.2 手写算法代码"></a><strong>2.1.2 手写算法代码</strong></h3><p>**<br>**</p><p>● 灰度图求直方图</p><p>● 给你个0 1 二维矩阵，寻找1的连通域有几个？算法复杂度？怎么优化加速呢？</p><h2 id="2-2-深度学习：CNN卷积神经网络方面"><a href="#2-2-深度学习：CNN卷积神经网络方面" class="headerlink" title="2.2 深度学习：CNN卷积神经网络方面"></a><strong>2.2</strong> <strong>深度学习：CNN卷积神经网络方面</strong></h2><h3 id="2-2-1-讲解相关原理"><a href="#2-2-1-讲解相关原理" class="headerlink" title="2.2.1 讲解相关原理"></a><strong>2.2.1 讲解相关原理</strong></h3><h4 id="2-2-1-1-卷积方面"><a href="#2-2-1-1-卷积方面" class="headerlink" title="2.2.1.1 卷积方面"></a><strong>2.2.1.1 卷积方面</strong></h4><p>**<br>**</p><p>● 卷积的作用，以及局限</p><p>● 比较VGG和LeNet，VGG使用3*3的卷积核最大的优势是什么？</p><p>● CNN，CNN的核心是什么？卷积操作是怎样进行的？卷积反向传播过程？Tensorflow中卷积操作是怎样实现的？（感觉这种问题是逃不掉的），池化又是怎样操作的，反向传播过程中池化层怎么接受后面传过来的损失的？</p><p>● CNN参数共享什么意思？</p><p>● 反卷积和上采样什么意思？</p><p>● Dropout跟浅层网络区别，dropout后验证集如何处理?</p><h4 id="2-2-1-2-池化方面"><a href="#2-2-1-2-池化方面" class="headerlink" title="2.2.1.2 池化方面"></a><strong>2.2.1.2 池化方面</strong></h4><p>**<br>**</p><p>● 池化层的作用？</p><p>● maxpooling 和 ave pooling 有啥区别，说一下？</p><p>● pooling层为什么用max pool？有哪些pooling层技术？各自对比、使用场合等等？</p><p>● Max Pooling 是如何反向传递梯度的？</p><h4 id="2-2-1-3-网络结构方面"><a href="#2-2-1-3-网络结构方面" class="headerlink" title="2.2.1.3 网络结构方面"></a><strong>2.2.1.3 网络结构方面</strong></h4><p>**<br>**</p><p>● ResNet，让我介绍了一下ResNet主要解决的问题是什么，然后又问我对看完ResNet有什么看法</p><p>● Alexnet的问题，大概是什么结构，和LeNet比有什么改进的地方，问了Relu比sigmoid好在哪？</p><p>● skipconnection有什么好处？</p><p>● mobileNet、shufflenet的原理？说了下原理</p><p>● 为什么mobileNet在理论上速度很快，工程上并没有特别大的提升？先说了卷积源码上的实现，两个超大矩阵相乘，可能是group操作，是一些零散的卷积操作，速度会慢。   </p><p>● 说下VGG ResNet网络结构？</p><p>● 介绍一下各种轻量级网络？</p><p>● 讲一下MobileNet的原理？深度可分离卷积，从参数数量以及计算量角度与传统卷积对比分析。</p><p>● MobileNet与Xception以及ShuffleNet的对比？是否测试过MobileNet在不同计算设备上的运行速度？</p><h4 id="2-2-1-4-其他方面"><a href="#2-2-1-4-其他方面" class="headerlink" title="2.2.1.4 其他方面"></a><strong>2.2.1.4 其他方面</strong></h4><p>**<br>**</p><p>● 简单谈了一下对梯度消失和如何防止过拟合的看法？</p><p>● CNN为什么比DNN在图像识别上更好？.</p><p>● 神经网络的反向传播机制？pooling和卷积。</p><p>● 解释一下BN?为什么用BN?BN层，归一化后的操作?</p><p>● 梯度爆炸和梯度消失的原因？</p><p>● 神经网络为什么要加sigmod函数？</p><p>● 如何防止过拟合？</p><p>● 初始学习率怎么设？这个没有总结过，只是说一般使用0.01～0.1</p><p>● 迁移学习是什么？怎么迁移？怎么选择迁移的模型？</p><p>● softmax、多个logistic的各自的优势？</p><p>● 深度学习里面解决梯度消失的办法？</p><p>● CV中做数据增强的方法？</p><p>● 工程上如何对卷积操作进行优化？</p><p>● 样本不均衡怎么处理？一个batch类别均等采样，修改loss对不同样本的权重。     </p><p>● 学习率过大会出现什么问题，怎么解决？</p><h3 id="2-2-2-数学计算"><a href="#2-2-2-数学计算" class="headerlink" title="2.2.2 数学计算"></a><strong>2.2.2 数学计算</strong></h3><p>**<br>**</p><p>● 卷积时间复杂度？CNN的卷积计算，参数计算？</p><h2 id="2-3-深度学习：RNN递归神经网络方面"><a href="#2-3-深度学习：RNN递归神经网络方面" class="headerlink" title="2.3 深度学习：RNN递归神经网络方面"></a><strong>2.3</strong> <strong>深度学习：RNN递归神经网络方面</strong></h2><h3 id="2-3-1-讲解相关原理"><a href="#2-3-1-讲解相关原理" class="headerlink" title="2.3.1 讲解相关原理"></a><strong>2.3.1 讲解相关原理</strong></h3><p>**<br>**</p><p>● LSTM的原理大概说一下，解决RNN的哪些问题。</p><p>● 对于深度学习，例如LSTM等的理解，还有它的改进GRU，还有梯度消失、梯度爆炸等等</p><p>● BN细节，有什么好处，能不能在RNN中用?</p><p>● LSTM为什么可以缓解梯度消失</p><p>● 讲一下LSTM各个门？</p><p>● LSTM原理，其中的参数是否相同？</p><p>● LSTM的原理知道吗？LSTM与传统RNN相比有什么优势？</p><p>● 为什么LSTM可以解决梯度消失的问题？LSTM可以解决梯度爆炸吗？</p><p>● GRU原理（几乎是要手写公式了）</p><p>● GRU两个gate的作用分别是什么?</p><p>● GRU 和 transformer 各自的优势？</p><p>● LSTM/Bert的结构，优劣势？</p><p>● 序列标注时数据量太少的时候怎么办？</p><p>● LSTM相比RNN优点缺点？</p><p>●  讲一下LSTM，LSTM相对于RNN有哪些改进？LSTM为什么可以解决长期问题，相对与RNN改进在哪？梯度消失和梯度爆炸？LSTM如何解决梯度消失的问题？</p><h2 id="2-4-深度学习：CNN-amp-RNN常问通用知识点"><a href="#2-4-深度学习：CNN-amp-RNN常问通用知识点" class="headerlink" title="2.4 深度学习：CNN&amp;RNN常问通用知识点"></a><strong>2.4</strong> <strong>深度学习：CNN&amp;RNN常问通用知识点</strong></h2><h3 id="2-4-1-基础知识点"><a href="#2-4-1-基础知识点" class="headerlink" title="2.4.1 基础知识点"></a><strong>2.4.1 基础知识点</strong></h3><p>**<br>**</p><p>● 解释下CNN与RNN的区别？</p><p>● 对比讲了cnn,rnn和lstm,并讲了transformer相对于他们的优点，transformer有啥缺点，transformer里面的两种mask操作，反正问了很多transformer里面具体的实现细节？</p><p>● 为什么要用CNN，Bi-LSTM？如何用Attention？</p><p>● Attention原理？</p><p> 主要讲的是Transformer中Multi-Head Scaled Dot-ProductAttention。注意，这里有一个Mask Attention机制，它对于Transformer Decoder和XLNet的实现原理非常重要，同学们如果了解相关知识点，一定要对这个Mask Attention知识点进行深入的理解。</p><p>● Multi-Head Attention中如何优化Muti-Head的计算？</p><p>没有相关底层优化经验，所以回答：借助CNN底层计算原理，将多头变换展开为二维矩阵（填充大量0），将多头变换转为矩阵乘法运算。</p><h3 id="2-4-2-模型评价"><a href="#2-4-2-模型评价" class="headerlink" title="2.4.2 模型评价"></a><strong>2.4.2 模型评价</strong></h3><p>**<br>**</p><p>● logloss和auc的区别、为什么业务中喜欢用auc？</p><p>● 二分类的评价指标都有哪些？</p><p>● acc与auc的选择？</p><p>● 如果线下auc很高，线上各项指标都不好，可能是因为什么，怎么解决</p><p>● AUC的计算?</p><p>● 准确率和召回率的概念</p><p>● 了解统计学的指标吗？AUC,ROC,F1都是干嘛的</p><p>● 混淆矩阵角度解读召回率和准确率</p><h2 id="2-5-机器学习方面"><a href="#2-5-机器学习方面" class="headerlink" title="2.5 机器学习方面"></a><strong>2.5</strong> <strong>机器学习方面</strong></h2><h3 id="2-5-1-讲解相关原理"><a href="#2-5-1-讲解相关原理" class="headerlink" title="2.5.1 讲解相关原理"></a><strong>2.5.1 讲解相关原理</strong></h3><h4 id="2-5-1-1-数据准备"><a href="#2-5-1-1-数据准备" class="headerlink" title="2.5.1.1 数据准备"></a><strong>2.5.1.1 数据准备</strong></h4><p>**<br>**</p><p>● 常用采样的方法？</p><p>● MCMC采样？</p><h4 id="2-5-1-2-特征工程"><a href="#2-5-1-2-特征工程" class="headerlink" title="2.5.1.2 特征工程"></a><strong>2.5.1.2 特征工程</strong></h4><p>**<br>**</p><p><strong>①</strong> <strong>特征降维</strong></p><p>● PCA的主成分是怎么得到的？</p><p>● 特征变换做什么？特征处理？</p><p>● svd怎么实现图像降维，怎么确定不会影响训练效果？</p><p>● LDA知道不？说一下LDA的原理？</p><p>● PCA原理及涉及的公式?</p><p><strong>②</strong> <strong>特征选择</strong></p><p>● 特征相关怎么处理？好几个特征都相关怎么处理？</p><p>● 高维数据，其中有一维是时间，有缺失，如何处理？</p><p>● 如果选出好特征，去掉不好的特征？</p><p>● 特征工程中具体衍生出来的特征进行了详细的询问，为什么要生成这样的特征，依据是什么，为什么要使用PCA进行降维，如何存在多个特征高度共线会有什么问题？</p><p>● 机器学习中的特征，是选择重要的特征，还是特征的相互关联？</p><p>● 对相似度的理解？如何进行特征筛选？如何衡量特征之间的相关性？</p><p>● 如果要用树模型的话，可以做哪些特征工程？（n-gram，tf-idf，w2v）</p><p>● 假如说句子长度差别很大的话，tf-idf这个指标会有什么问题？one-hot encoding这个指标又会有什么问题？</p><p>● 树模型怎么分裂？怎么理解信息增益？</p><h4 id="2-5-1-3-有监督学习-分类和回归方面"><a href="#2-5-1-3-有监督学习-分类和回归方面" class="headerlink" title="2.5.1.3 有监督学习-分类和回归方面"></a><strong>2.5.1.3 有监督学习-分类和回归方面</strong></h4><p>**<br>**</p><p><strong>①</strong> <strong>分类回归树（集成学习）</strong></p><p>● 集成学习分为几大类，rf和bagging的区别？</p><p>● 从方差和偏差角度比较bagging和boosting？</p><p>● boosting和bagging的差异？两者的思想说一下？bagging里面树的深度和boosting里面的不同，为什么?</p><p>**A.**<strong>基于bagging：随机森林</strong></p><p>● bagging的思想是什么，本质是什么；</p><p>● 随机森林里面的两个随机，随机森林为什么是减小方差？和Adaboost的区别？</p><p>● 随机森林与决策树关系，防止过拟合的原理，随机性的体现。</p><p>● 随机森林如何选择feature？</p><p>● 从方差偏差的角度解释bagging？为什么随机森林泛化能力强？</p><p>● dropout是否了解？随机森林是否也可以用dropout?</p><p>● 随机森林的随机体现在哪里？样本/特征随机采样的目的是什么？</p><p>● 随机森林和GBDT的区别和联系</p><p>● 什么样的数据你会选择使用随机森林</p><p>● 随机森林相比决策树的优点有什么</p><p>● 随机森林怎么提高泛化能力的？</p><p>● 随机森林原理，和决策树有什么区别，追问投票是怎么投票的？</p><p>**B.**<strong>基于boosting：Adaboost、GDBT、XGBoost</strong></p><p>● 用到了哪些模型（LGB+XGB的bagging），为何？有什么好处？LGB里面你用到了哪些参数？你怎么调的参数？</p><p>● GBDT、XGBoost、LightGBM三个算法的原理和区别？XGBoost和GBDT哪个模型性能好，为什么？</p><p>● RF、GBDT、XGBoost、AdaBoost的区别？</p><p>● GBDT+LR原理</p><p>● 讲一下boosting算法？说一下adaboost是怎么更新的？</p><p>● 介绍XGB对GBDT的提升，LGB对XGB的提升，以及既然使用了LGB为什么还要使用XGB？</p><p>● XGB如何处理缺失值，LGB的差加速和直方图算法的底层代码是否有过了解</p><p>● XGBoost的优化点，与传统GDBT的区别？</p><p>● GBDT的特征组合原理?</p><p>● 问了gbdt在分类时节点输出是什么？怎么拟合残差？</p><p>● XGB为什么要做泰勒展开？正则项的内容？为什么要拟合二阶梯度？</p><p>● XGboost的底层算法是什么（CART树）</p><p>● Xgboost的应该着重调哪些参数？</p><p>● 介绍随机森林和GBDT的区别，为什么Bagging降方差，Boosting降偏差</p><p>● 讲一下GBDT，Gradient 代表什么？</p><p><strong>②</strong> <strong>线性回归</strong></p><p>● 线性回归和逻辑回归关系，区别？</p><p><strong>③ K****近邻（KNN）</strong></p><p>● KNN 复杂度高，怎么解决?</p><p><strong>④</strong> <strong>逻辑回归LR</strong></p><p>● LR的原理，LR参数的意义？LR的损失函数，怎么求解.</p><p>● 为什么LR的目标函数是最大化似然函数？</p><p>● LR加上正则化项后怎么求解？</p><p>● LR如何引入非线性？</p><p>● LR的损失函数是什么？lr为什么不用minsquare loss？它的导数是啥？加了正则化之后它的导数又是啥？</p><p>● LR里面，损失函数能不能把交叉熵换成MSE？</p><p>● LR和SVM的时间代价比较过没有。</p><p>● 问我逻辑回归中sigmoid函数的好处？以及为什么用极大似然？</p><p>● 逻辑回归的思想和过程，损失函数是什么，如何训练得到最优参数</p><p>● 说说逻辑回归吧，适用于什么场景呢，和knn区别？</p><p><strong>⑤ SVM****（支持向量机）</strong></p><p>● 详细说一下LR，SVM和DT的原理</p><p>● 讲一下LR、SVM、XGBoost模型的区别</p><p>● 讲一下SVM的原理，核函数，以及和LR的区别，哪个是参数模型？</p><p>● SVM目标函数，为什么转为对偶，SVM的核函数的本质是什么？</p><p>● SVM对偶问题介绍一下，从函数间隔，几何间隔开始介绍。</p><p>● SVM优化的目标是啥？问了SVM推导以及拉格朗日对偶法，从数学角度来说明</p><p>● SVM当线性不可分的时候怎么办？(楼主答用核函数升维) </p><p>● 说一下SVM适合什么场景？或者说有什么限制？</p><p>● SVM的KTT条件？</p><p>● 知道哪几种核函数？介绍一下高斯核函数？</p><p>● 核函数的作用，核函数为什么有用？从数学角度说明</p><p><strong>⑥</strong> <strong>朴素贝叶斯（Naive Bayes）</strong></p><p>● 朴素贝叶斯的原理？</p><p>● 最大似然估计和贝叶斯估计的联系和区别</p><p>● 贝叶斯估计和似然估计的区别</p><p><strong>⑦</strong> <strong>决策树（DT）</strong></p><p>● 决策树的原理，前后剪枝，评价指标。</p><p>● 决策树分裂节点的选择？</p><p>● 各种决策树:id3 c4.5 cart</p><p>● 朴素贝叶斯和决策树的差别，各有什么缺点？再加上SVM呢？</p><p>● rf和gbdt基分类器区别，里面的决策树分别长啥样，怎么剪枝</p><p>● 决策树的一下细节，GBDT，各种熵的计算，</p><p>● 决策树相比其他算法有什么优势？</p><p>● 决策树中有哪些参数，如何避免决策树的过拟合</p><p>● 决策树具体讲讲，原理，如何选取特征的，怎么进行分类预测的？</p><p><strong>⑧</strong> <strong>其他</strong></p><p>● 说一下生成式模型？生成式模型和判别式模型有什么特点？</p><p>● 生成式和判别式模型哪个使用极大似然估计？</p><h4 id="2-5-1-4-无监督学习-聚类方面"><a href="#2-5-1-4-无监督学习-聚类方面" class="headerlink" title="2.5.1.4 无监督学习-聚类方面"></a><strong>2.5.1.4 无监督学习-聚类方面</strong></h4><p>● 非监督学习有哪些？介绍了Kmeans，k-means的时间复杂度</p><p>● 怎么选取K值？手肘法  </p><p>● K-means的缺点？</p><p>● 如果没有先验知识，如何确定K-means的参数</p><p>● 衡量K-means效果好坏的方法</p><p>● kmeans原理，优化目标是什么?</p><p>● k-means和knn有什么关系，区别？</p><p>● k-means聚类效果会不会因为初始选取点不同，聚类效果完全不一样？k-means聚类效果和k的大小有什么关系？</p><p>● 简述kmeans算法，有什么缺点，如何改进，kmeans++是如何改进的？</p><h4 id="2-5-1-5-模型评价"><a href="#2-5-1-5-模型评价" class="headerlink" title="2.5.1.5 模型评价"></a><strong>2.5.1.5 模型评价</strong></h4><p>**<br>**</p><p>● 如何选择模型？从数据量，特征量方面分析了一遍</p><h3 id="2-5-2-手写算法及代码"><a href="#2-5-2-手写算法及代码" class="headerlink" title="2.5.2 手写算法及代码"></a><strong>2.5.2 手写算法及代码</strong></h3><h4 id="2-5-2-1-手写公式"><a href="#2-5-2-1-手写公式" class="headerlink" title="2.5.2.1 手写公式"></a><strong>2.5.2.1 手写公式</strong></h4><p>**<br>**</p><p>● 贝叶斯公式知道吗，什么含义？</p><p>● LR是否知道，讲一下数学原理（公式层面）</p><p>● 推导一下SVM？</p><h4 id="2-5-2-2-手写代码"><a href="#2-5-2-2-手写代码" class="headerlink" title="2.5.2.2 手写代码"></a><strong>2.5.2.2 手写代码</strong></h4><p>**<br>**</p><p>● 手写一个kmeans</p><p>● 求两个点的欧几里得距离？并实现一个kmeans？</p><h2 id="2-6-深度学习-amp-机器学习面经通用知识点"><a href="#2-6-深度学习-amp-机器学习面经通用知识点" class="headerlink" title="2.6 深度学习&amp;机器学习面经通用知识点"></a><strong>2.6</strong> <strong>深度学习&amp;机器学习面经通用知识点</strong></h2><h3 id="2-6-1-损失函数方面"><a href="#2-6-1-损失函数方面" class="headerlink" title="2.6.1 损失函数方面"></a><strong>2.6.1 损失函数方面</strong></h3><p>**<br>**</p><p>● 讲一下交叉熵的公式和意义</p><p>● 为什么分类问题的损失函数用交叉熵？</p><h3 id="2-6-2-激活函数方面"><a href="#2-6-2-激活函数方面" class="headerlink" title="2.6.2 激活函数方面"></a><strong>2.6.2 激活函数方面</strong></h3><p>**<br>**</p><p>● 激活函数作用，有哪些，都怎么改进？</p><p>● ReLu函数所有背景、原理、应用?</p><p>● RELU和Sigmoid相比，优点有哪些？ReLU 解决了什么问题？</p><h3 id="2-6-3-网络优化梯度下降方面"><a href="#2-6-3-网络优化梯度下降方面" class="headerlink" title="2.6.3 网络优化梯度下降方面"></a><strong>2.6.3 网络优化梯度下降方面</strong></h3><p>**<br>**</p><p>●  梯度下降、牛顿法优缺点？</p><p>● 梯度下降和随机梯度下降讲一下？</p><p>● SGD,ADAM区别</p><p>● SGD和BGD区别，还知道哪些优化算法？动量的作用是什么？</p><p>● 梯度下降过程中如果不按正确的方向进行怎么办？</p><p>● 用什么优化方法，梯度下降的种类，各有什么优点</p><p>● 梯度下降法和牛顿法是如何实现的。优化问题分哪些种，无约束的优化问题怎么处理，有约束的优化问题怎么处理。</p><p>● 凸优化了解吗？牛顿法、SGD、最小二乘法，各自的优势，牛顿法与SGD的区别？牛顿法能用于非凸函数吗？拟牛顿法能说说吗？</p><p>● 说一下牛顿法，为什么深度学习很少用牛顿法？牛顿法一般应用于什么场景，有什么好处？</p><h3 id="2-6-4-正则化方面"><a href="#2-6-4-正则化方面" class="headerlink" title="2.6.4 正则化方面"></a><strong>2.6.4 正则化方面</strong></h3><p>**<br>**</p><p>● L1，L2正则化的原理讲一下？</p><p>● L1和L2的区别，数学上解释（等高线）</p><p>● L1正则不是连续可导的，那么还能用梯度下降么，如果不能的话如何优化求解?</p><p>● 正则化有哪几种，分别有什么作用？</p><p>● L1和L2的区别？从贝叶斯估计的角度看？它们的先验分布是什么？</p><p>● 问了l1正则 产生稀疏解的原因（一画图准备扯拉普拉斯分布0点值就把我打住了，知道我想说什么），不可导点的处理（说了三种方法，问了不了解针对大规模数据的方法，这些实际都不常用）</p><p>● L1、L2正则化的区别和应用场景？你知道哪些激活函数？简单说说区别？为什么需要激活函数，它解决了一些什么问题？为什么ReLU比sigmoid更能解决梯度消失的问题？</p><h3 id="2-6-5-压缩-amp-剪枝-amp-量化-amp-加速"><a href="#2-6-5-压缩-amp-剪枝-amp-量化-amp-加速" class="headerlink" title="2.6.5 压缩&amp;剪枝&amp;量化&amp;加速"></a><strong>2.6.5 压缩&amp;剪枝&amp;量化&amp;加速</strong></h3><p>**<br>**</p><p>● 剪枝与正则化的联系，笔者从结构化剪枝与非结构化剪枝分别对应Lasso和Group Lasso的角度来回答</p><p>● 结构化剪枝和非结构化剪枝</p><p>● 三大角度：蒸馏，剪枝，量化。笔者分别介绍了三大角度的基本原理。</p><h3 id="2-6-6-过拟合-amp-欠拟合方面"><a href="#2-6-6-过拟合-amp-欠拟合方面" class="headerlink" title="2.6.6 过拟合&amp;欠拟合方面"></a><strong>2.6.6 过拟合&amp;欠拟合方面</strong></h3><p>**<br>**</p><p>● 如何检验过拟合，数据量很小怎么办？</p><p>就项目中数据处理方式做了详细的询问，生成的多张数据集如何使用，缺失值的处理需要考察到哪些问题，均值填充是否科学等</p><p>● 如何防止过拟合？为什么会过拟合?（从数据、模型、指标三个角度，提到了dropout、正则，后面正好顺着问。）</p><p>● dropout是否了解？讲一下dropout原理，为什么能防止过拟合？对训练数据和预测数据有什么区别？</p><p>● 神经网络怎么避免过拟合？</p><h3 id="2-6-7-其他方面"><a href="#2-6-7-其他方面" class="headerlink" title="2.6.7 其他方面"></a><strong>2.6.7 其他方面</strong></h3><p>**<br>**</p><p>● 监督学习，无监督学习区别，半监督是什么？</p><p>● 怎么做数据增强(结构化数据，图像，文本）</p><p>● 为什么会产生梯度震荡、学习率是干嘛的</p><p>● 怎么处理非平衡问题（除了我说的欠采样过采样，小哥哥说可以对损失函数进行改进，比如令正类的损失函数为1，负类的损失函数为9）</p><p>● 数据不平衡问题?</p><p>从欠采样过采样等经典解决办法的角度回答。另外回答了一些其他方法：GAN（ICCV 2019 best paper：SinGAN），特征空间增广，改进训练方式（源数据训练特征提取backbone，欠采样或过采样训练分类器），Loss加权，使用AdaGrad优化器等。</p><p>● 如何可视化和理解你的模型？（遮挡实验，attention score）</p><p>● 问平常训练模型有没有遇到什么问题，说了显存和batch-size,然后面试官一路问到底，就各种问题如何解决，有个地方我说到了静态rnn和dynamic rnn,例如如何提高显存利用率，一个模型如何发现性能瓶颈在哪</p><p>● 如何降低模型复杂度</p><p>● 如果数据不充足，或者说非常不平均，要怎么解决？从数据增强和建模来讲</p><p>● 偏差和方差的区别？</p><p>● GraphEmbedding和GNN的区别？</p><h1 id="3-阿里巴巴面经涉及项目知识点"><a href="#3-阿里巴巴面经涉及项目知识点" class="headerlink" title="3 阿里巴巴面经涉及项目知识点"></a><strong>3</strong> <strong>阿里巴巴面经涉及项目知识点</strong></h1><p><img src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" alt="图片"></p><h2 id="3-1-深度学习：CNN卷积神经网络方面"><a href="#3-1-深度学习：CNN卷积神经网络方面" class="headerlink" title="3.1****深度学习：CNN卷积神经网络方面"></a><strong>3.1****深度学习：CNN卷积神经网络方面</strong></h2><h3 id="3-1-1-目标检测方面"><a href="#3-1-1-目标检测方面" class="headerlink" title="3.1.1 目标检测方面"></a><strong>3.1.1 目标检测方面</strong></h3><h4 id="3-1-1-1-讲解原理"><a href="#3-1-1-1-讲解原理" class="headerlink" title="3.1.1.1 讲解原理"></a><strong>3.1.1.1 讲解原理</strong></h4><p>**<br>**</p><p>●  fasterrcnn的RPN和ROI pooling？</p><p>● 单阶段和两阶段的优缺点？</p><p>● 非极大值抑制？</p><p>● 交通标志检测和识别的关键？</p><p>● 基于SSD的，问训练阶段SSD的gt怎么和预测框发生联系。</p><p>● 在目标检测的基础上讲了讲跟踪方面的项目，深挖了几个点比如deformable CNN的具体实现，Siamese-RPN的具体实现</p><p>● 如何解决小目标检测的问题？</p><p>● 为什么要深层、浅层featureMap concat？提了点细节和我踩的坑，需要数量级上的调整，不然深层的feature可能会被压制。           </p><p>● Cascade的思想? 说了下我的摸索的一个过程。改变样本分布，困难样本挖掘，能达到比较好的效果。             </p><p>● 介绍网络：Faster-RCNN、YOLO、SSD、YOLOv1、YOLOv2、YOLOv3、Masker-RCNN、GAN</p><p>● CenterNet的实现细节（argmax）</p><p>● RCNN、Fast RCNN 和 FasterRCNN的区别</p><p>● 为什么选择RetinaNet</p><p>● 特征金字塔FPN的作用?</p><h4 id="3-1-1-2-损失函数"><a href="#3-1-1-2-损失函数" class="headerlink" title="3.1.1.2 损失函数"></a><strong>3.1.1.2 损失函数</strong></h4><p>**<br>**</p><p>● Focalloss原理?</p><h3 id="3-1-2-OCR"><a href="#3-1-2-OCR" class="headerlink" title="3.1.2 OCR"></a><strong>3.1.2 OCR</strong></h3><p>**<br>**</p><p>● 就实习阶段所做的超分辨率算法工作进行了详细的询问：数据如何生成，从概率的角度解释网络为何能够学到LR和SR的映射关系，如何搭建和训练网络，如何解决模型落地问题</p><p>● 了解到答主在做超分时遇到的问题后，对业界前沿的技术做了相关询问，用了哪些GAN模型，GAN模型的loss函数如何设计，为什么这么设计？</p><h3 id="3-1-3-图像分类"><a href="#3-1-3-图像分类" class="headerlink" title="3.1.3 图像分类"></a><strong>3.1.3 图像分类</strong></h3><p>**<br>**</p><p>● 分类器有了解吗？对哪些分类算法有研究？</p><h2 id="3-2-深度学习：RNN递归神经网络方面"><a href="#3-2-深度学习：RNN递归神经网络方面" class="headerlink" title="3.2 深度学习：RNN递归神经网络方面"></a><strong>3.2</strong> <strong>深度学习：RNN递归神经网络方面</strong></h2><h3 id="3-2-1-自然语言处理NLP"><a href="#3-2-1-自然语言处理NLP" class="headerlink" title="3.2.1 自然语言处理NLP"></a><strong>3.2.1 自然语言处理NLP</strong></h3><p>**<br>**</p><p><strong>① Bert</strong></p><p>● Bert详解？损失函数？bert的mask相对于CBOW有什么相同与不同</p><p>●  bert当时是用预训练模型还是自己重新训练的？用bert来干嘛了？</p><p>● 了解过BERT吗，里面的三种embedding分别是什么，为什么要这样做？</p><p>● Bert和Elmo在工程中存在的一些Trick？</p><p><strong>② Transformer</strong></p><p>● Transformer模型架构说一下？为什么用transformer，不用rnn和LSTM、transformer的优势是什么？</p><p>● Transformer和 BERT 的位置编码有什么区别？</p><p>● Transformer用的 Layer Normalize 还是Batch Normalize？Layer，有什么区别？…</p><p>● 用python写一个multi-head attention</p><p><strong>③ CRF</strong></p><p>● CRF的作用？维特比详细过程</p><p><strong>④ HMM****隐马尔科夫模型</strong></p><p>● 隐马原理？如何应用在分词当中的？</p><p><strong>⑤ Word2vec</strong></p><p>● 讲一下word2vec，word2vec网络模型？怎么训练？讲一下word2vec的霍夫曼树的原理</p><p><strong>⑥ CNN****方面</strong></p><p>● NLP哪个模型最熟悉？Text-CNN，讲一下？</p><p>● TextCNN模型背景、原理、应用；当时毕设/美团评论情感分析比赛的数据集、评价体系、任务要求？</p><p>● TextCNN当时用了哪些卷积核？数值、尺寸？为什么用这种？</p><p><strong>⑦</strong> <strong>其他</strong></p><p>● 介绍预训练语言模型</p><p>ELMo，BERT，Transforler-XL，XLNET，ERNIE，RoBERTa，ALBERT，ELECTRA。。。笔者从BERT的mask LM以及NSP任务出发讲解了BERT后续各大预训练的改进。</p><p>各大预训练语言模型可能不能从头到尾讲起，笔者线是介绍了BERT，然后从BERT的预训练任务出发，比如介绍了ERNIE中对mask LM的改进，ALBERT中将NSP任务替换为SOP任务等。</p><p>● CNN，RNN在处理文本上有什么区别?</p><p>● CNN在文本分类上的应用与什么比较像？（意思就是卷积核的作用与什么的作用很像，答案是n-gram，没答上来，我说了一下在文本分类中卷积核是怎么运作的）</p><p>● Dropout有什么作用？类似于 Bagging 。在Transformer 模型中 dropout 主要用在哪里？</p><p>● 如何衡量两个句子的相似度，sentence embedding的方法</p><p>● 从长文本到短文本的生成怎么做</p><p>● 生成文本或者说生成模型，最主要的因素是什么</p><p>● 对NLP的理解，讲了一下文本分类的发展史，主流分类方法的发展</p><p>● 说一说nlp的基本预训练模型（说了ELMO、GPT、BERT），然后这些预训练模型有什么特点？</p><p>● 说一个熟悉的文字识别模型（CRNN的结构、CTC-Loss）</p><p>● 怎么处理非平衡？欠采样的时候其实可以考虑文本相似度，了解怎么做文本相似度吗？</p><p>● 讲一下训练词向量的方法（w2v，skip-gram，CBOW，glove）</p><p>● 文本里面的“服务”、“食物好吃”等，如何抽取作为特征？（这问题当时理解有歧义，所以了有点久才知道问的，词性关联模板）</p><p>● 如何判断一段文字的时效性？</p><h2 id="3-3-强化学习方面"><a href="#3-3-强化学习方面" class="headerlink" title="3.3 强化学习方面"></a><strong>3.3</strong> <strong>强化学习方面</strong></h2><h3 id="3-3-1-讲解原理"><a href="#3-3-1-讲解原理" class="headerlink" title="3.3.1 讲解原理"></a><strong>3.3.1 讲解原理</strong></h3><p>**<br>**</p><p>● 你觉得强化学习和推荐有那些关联？</p><p>● 强化学习模型 和 CTR 预估模型的区别？</p><p>● 强化学习有哪些评价指标？</p><p>● gan中的转置卷积</p><p>● 论文相关，gan的对抗样本对判别器有什么效果， gan生成的样本只考虑了一部分特征，如何考虑业务数据下游的下游特征，他们如何增强？</p><p>● gan在淘宝场景中的应用（从数据，样本，模型，评估层面分析）</p><p>● 面试官在手淘那边是负责做推荐搜索的，而我的方向是做强化学习的，而用强化学习做推荐搜索很可能是近几年的一个趋势，让我用强化学习对推荐搜索进行建模，包括state、action、agent的选取，reward的设计，以及如何训练。以及和目前现有的推荐搜索技术相比，用强化学习做有什么优势呢。</p><p>● 介绍强化学习都有哪些方法？</p><p>● offpolicy 和on policy都有哪些应用场景？区别是什么？</p><h3 id="3-3-2-损失函数"><a href="#3-3-2-损失函数" class="headerlink" title="3.3.2 损失函数"></a><strong>3.3.2 损失函数</strong></h3><p>**<br>**</p><p>● 强化学习loss函数说一下？</p><h2 id="3-4-机器学习方面"><a href="#3-4-机器学习方面" class="headerlink" title="3.4 机器学习方面"></a><strong>3.4</strong> <strong>机器学习方面</strong></h2><h3 id="3-4-1-推荐系统"><a href="#3-4-1-推荐系统" class="headerlink" title="3.4.1 推荐系统"></a><strong>3.4.1 推荐系统</strong></h3><p>**<br>**</p><p>● 介绍下基本的推荐算法</p><p>● CTR比赛中如何做的特征？</p><p>● 说一下协同过滤公式，两种协同过滤额应用场景有啥不同？</p><p>● user向量和 item 向量，协同过滤，NeuralCF</p><p>● user的嵌入向量怎么得到的？</p><p>● 冷启动：用户很少交互怎么解决？商品数目很多训练嵌入向量怎么训练？</p><p>● CTR预估中，如果有两个模型 CTR/CVR, 怎么做最后的 item 排序。猜测是问 ensemble 方法，回答了 bagging，然后让我具体描述一下思路。</p><p>召回策略的方法：CF -&gt; Nerual CF -&gt; Youtube DNN</p><p>讲一下 CF 的思路，UserCF, ItemCF</p><p>● 假如说处理一个多级分类的问题，有没有什么办法只用一个模型？-multitask</p><p>如果说multitask的输出y之间互相有制约关系，要怎么处理？（之前还不知道有CRF这个东西，就说了如果输出的y之间冲突了就引入一个loss。面试官说看来你还不知道CRF，下去好好学习一下）</p><p>● 问了netflix的电影评分预测 讲了怎么基于矩阵分解求出评分 svd的k值代表什么？怎么确定？</p><p>● 问了deepfm，讲了FM的原理，怎么缩减计算量</p><p>● point-wise，pair-wise, list-wise的优缺点，对这些loss的常用设计形式了解吗？</p><p>● 对召回算法有了解吗？常用的召回算法的优缺点？</p><p>● CTR中为什么经常用LR？</p><p>● 介绍FM FFM原理</p><p>● FM推导</p><p>● 召回和精排的区别以及各自的特点？</p><p>● 召回和精排的负样本有何不同？</p><p>● 精排特征有哪些，点击序列是怎么作为特征放进去的？</p><h1 id="4-数据结构与算法分析相关知识点"><a href="#4-数据结构与算法分析相关知识点" class="headerlink" title="4 数据结构与算法分析相关知识点"></a><strong>4</strong> <strong>数据结构与算法分析相关知识点</strong></h1><p><img src="https://mmbiz.qpic.cn/mmbiz_png/Zibun5D7npIMPE0m2oQe9ExZ7Qgz2QbgicLHWfkWaeRt6DxegbibdpYFiblYOWblYJ9ZYLDiaSd4Nz3XxWaww3DEAOQ/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></p><h2 id="4-1-数据结构与算法分析"><a href="#4-1-数据结构与算法分析" class="headerlink" title="4.1 数据结构与算法分析"></a><strong>4.1</strong> <strong>数据结构与算法分析</strong></h2><h3 id="4-1-1-线性表"><a href="#4-1-1-线性表" class="headerlink" title="4.1.1 线性表"></a><strong>4.1.1 线性表</strong></h3><h4 id="4-1-1-1-数组"><a href="#4-1-1-1-数组" class="headerlink" title="4.1.1.1 数组"></a><strong>4.1.1.1 数组</strong></h4><p>**<br>**</p><p>● 2*N数组，将奇数放到奇数位置，偶数放到偶数位置</p><p>● 给定一个长度为n的数组，如【12123】，相邻值为1、-1，现在有一个值x=100，查找x的位置?</p><p>● 给一个数组，如何判断这是数组是不是一棵树的后序遍历？后序遍历有什么特点？</p><p>● 一个1000W的64bit整型数组，无需、可重复，找第100W的数字。【快排思想（非快排）O(2n) &gt; 堆排O(n logk) &gt; 快排O(n logn)】</p><p>● 一个数组按从大到小排列，但是有重复的元素，利用二分查找查找到指定的元素，如果有多个就返回最大的那个索引。我先写了递归算法，然后他让我非递归写一下。</p><p>● 如何在n个数组中找出它的中位数（n个数组无法完全放在内存中）</p><p>● 求一个数组的连续区间，使得和最大？</p><p>● 两个有序数组怎样最快的确定差集，复杂度是多少？</p><p>● 两个无序数组去重复</p><p>● 给定一个数组，求最大连乘子区间（可以包含小数、负数）</p><p>● 找到一个无序数组里面连续的最长整数数组长度。顺带考察了基数排序和快速排序</p><p>● 链表和数组的区别？</p><p>● 可重复排列，数组第k个数求时间复杂度？</p><h4 id="4-1-1-2-链表"><a href="#4-1-1-2-链表" class="headerlink" title="4.1.1.2 链表"></a><strong>4.1.1.2 链表</strong></h4><p>**<br>**</p><p>● 说一下链表反转？</p><p>● 把一个链表转化成平衡二叉树</p><p>● 说一下链表操作的时间复杂度：查找、查询、删除、增加。和顺序表的区别？</p><p>● 链表插入的复杂度什么时候是O(1)？</p><p>● 链表的插入和查找的复杂度</p><p>● 2*N数组，将奇数放到奇数位置，偶数放到偶数位置</p><h4 id="4-1-1-3-字符串"><a href="#4-1-1-3-字符串" class="headerlink" title="4.1.1.3 字符串"></a><strong>4.1.1.3 字符串</strong></h4><p>**<br>**</p><p>● 一个是单词级别的翻转字符串，比如“I love you”翻转成“you love I”？</p><p>● 反转字符串in place</p><p>● 把一个字符串的小写字母放到前面，大写放到后面，保持原有的顺序？</p><p>● 两个字符串的最小距离（插入，删除，改变一个字符）说一下思路和复杂度？</p><p>● 一个求最长不重复字符串长度的代码题？</p><p>● 给定一个字符串（例如abc），和一个文档，给出每个字符在文档中的倒排索引，在文档中找到一个最小窗口（adebdc），使给定字符串是其子串。（解法：使用归并思想，从第一个字符（a）的倒排索引开始，找仅比当前索引大的第二个字符（b）的索引，直到最后一个字符，计算窗口大小，保留最小值，时间复杂度：各字符倒排索引数组大小相加（线性），空间复杂度，O(1)？</p><p>● 从字符串中提取所有有效的ip地址？</p><p>● 给定一个字符串，和字符串列表，判断能否用字符串列表拼接生成字符串？</p><p>● 两个字符串的公共子串，动态规划？</p><h3 id="4-1-2-树"><a href="#4-1-2-树" class="headerlink" title="4.1.2 树"></a><strong>4.1.2 树</strong></h3><h4 id="4-1-2-1-二叉树"><a href="#4-1-2-1-二叉树" class="headerlink" title="4.1.2.1 二叉树"></a><strong>4.1.2.1 二叉树</strong></h4><p>**<br>**</p><p>● 求解二叉树的最大高度，用一个非递归的做法</p><p>● 二叉树最长叶子结点路径</p><p>● 如何判断一棵树是另一棵树的子树？</p><p>● 如何判断一棵树是不是平衡二叉树？</p><p>● 开发一颗字典树，实现建树和搜索功能</p><p>● 有一个词库, 如何像纸制字典一样建立索引？(B+树)</p><p>● 红黑树了解吗，介绍一下？</p><p>● 红黑树的查询复杂度？</p><p>● 二叉平衡查找树怎么实现平衡？</p><p>● 给出二叉树的前序和中序序列恢复二叉树的结构</p><p>● 不用递归遍历一棵树</p><p>● 二叉树层次遍历</p><p>● 给出二叉树的前序遍历和中序遍历结果，构建这棵树</p><p>● 描述一下二叉搜索树？</p><p>● 时间复杂度为O(n)、空间复杂度为O(k)的树的搜索方法</p><h4 id="4-1-2-2-堆"><a href="#4-1-2-2-堆" class="headerlink" title="4.1.2.2 堆"></a><strong>4.1.2.2 堆</strong></h4><p>**<br>**</p><p>● 什么是堆，构建堆的复杂度，堆找出第k大元素的复杂度？</p><p>● 描述一下堆排序、什么是大顶堆、什么是小顶堆</p><p>● 一个数组找 top-K，堆遍历数组。（k 最大）为啥用小顶堆。大顶堆小顶堆的区别？</p><h3 id="4-1-3-排序"><a href="#4-1-3-排序" class="headerlink" title="4.1.3 排序"></a><strong>4.1.3 排序</strong></h3><p>**<br>**</p><p>● 有哪些排序算法，他们的时间复杂度是多少</p><p>● 海量数据前k大</p><p>● 只给一分钟思考口述如何在时间复杂度最低的情况下找到无序数组中的第k个数。（快排+剪枝）</p><p>● 一亿个浮点数，大小不超过2^32，均匀分布在值域内，求最快的排序方法；分析排序方法的复杂度（面试官全程提示）</p><p>● 手写快排</p><h3 id="4-1-4-搜索"><a href="#4-1-4-搜索" class="headerlink" title="4.1.4 搜索"></a><strong>4.1.4 搜索</strong></h3><p>**<br>**</p><p>● 题目: 最少新建道路条数</p><p> 已知有N个城市，城市编号1…N</p><p> 已知M条路，每条路表示为(x_i, y_i), xy分别为城市编号</p><p> 现在需要新建道路，确保任意两个城市之间，是可以通过一条或者多条道路联通</p><p> 求解能够达到此目的的最小道路条数</p><p>思路：BFS或者DFS应该都可以找到城市簇即可</p><h2 id="4-2-算法思想实战及智力题"><a href="#4-2-算法思想实战及智力题" class="headerlink" title="4.2 算法思想实战及智力题"></a><strong>4.2</strong> <strong>算法思想实战及智力题</strong></h2><h3 id="4-2-1-算法思想实战"><a href="#4-2-1-算法思想实战" class="headerlink" title="4.2.1 算法思想实战"></a><strong>4.2.1 算法思想实战</strong></h3><p>**<br>**</p><p>● 一条无限长的x轴，假设你站在原点，可以向左走也可以向右走，第n次走n步或者-n步，给一个target，问走到target的最小次数</p><p>● 一个黑盒里有n个球，球分为三种颜色，RGB，乱序，每种颜色的球n/3个。这个黑盒有两个接口，一个接口可以获取第i个位置的球颜色，一个接口可以交换两个位置的球。通过这两个接口将球排序成RGBRGB这种的顺序。</p><p>面试官先上让我不考虑时间复杂度说出一个解法，我说了一个O(n^2)的解法。</p><p>然后他让我想想有没有更好的解法，我想了大概5分钟，恍然大悟，说了一个O(n)的解法</p><p>● 一个快递员送快递，有n个城市，怎么选择路线，使得走的路程最短。</p><p>● 给一个数，怎么快速求这个数的二进制中有多少个1？</p><p>● 实现一个数据结构，满足set(index,value),get(index),setall(value)的操作尽可能高效，使得get(index)返回正确的结果。</p><h3 id="4-2-2-智力题"><a href="#4-2-2-智力题" class="headerlink" title="4.2.2 智力题"></a><strong>4.2.2 智力题</strong></h3><p>**<br>**</p><p>● 有一座桥，A通过需要25分钟，B通过需要20分钟，C通过需要10分钟，D通过需要5分钟，一个桥同时只能走两人，且快的人需要等慢的人到达才能一起到达。走桥时必须要有手电筒才能经过，且手电筒只有一个，问如何在60分钟内使得四人均通过？</p><p>● 长m，宽n的长方形，每长度为1画一条线，问可以找到多少正方形（m*n+(m-1)(n-1)+(m-2)(n-2)+…+(m-n+1)*1）？多少长方形（C_{m+1}^2  * C_{n+1}^2，即在m+1个点中随机选2个，在n+1个点中随机选2个）？</p><p>● 两个人 A,B；A 红绿色盲，B 声称自己不是红绿色盲，现在 B 两只手上分别有红绿两颗球。</p><p>问 A 怎么分辨出 B 是不是红绿色盲。</p><p>● 你有两根均匀的绳子，和一个无限燃料的打火机。每根绳子若点燃一头可燃烧1分钟。问如何用这些东西准确测量出45秒的时间？</p><p>● 在面试官快要不耐烦的时候想出来了正解…绳A点燃两头，绳B点燃一头。在绳A燃尽时点燃绳B的另一头，两根绳子全部燃尽总耗时45秒。</p><p>● 8个小球称重；如果不知道次品轻重怎么办？</p><h2 id="4-3-其他方面"><a href="#4-3-其他方面" class="headerlink" title="4.3 其他方面"></a><strong>4.3</strong> <strong>其他方面</strong></h2><h3 id="4-3-1-数论"><a href="#4-3-1-数论" class="headerlink" title="4.3.1 数论"></a><strong>4.3.1 数论</strong></h3><p>**<br>**</p><p>● 在A地有两辆公交车，一辆间隔5分钟，一辆间隔7分钟，问等车时间的期望。</p><p>● 掷骰子，问掷到6个面全出现的期望？</p><h3 id="4-3-2-计算几何"><a href="#4-3-2-计算几何" class="headerlink" title="4.3.2 计算几何"></a>4.3.2 计算几何</h3><p>● 拉格朗日乘子法，hessian矩阵</p><h3 id="4-3-3-概率分析"><a href="#4-3-3-概率分析" class="headerlink" title="4.3.3 概率分析"></a>4.3.3 概率分析</h3><p>● 长度为1的线段，随机地取两点A和B，求AB长度的概率密度函数？</p><p>● 8个球有一个重一点，最少称几次能找出来，我说3次，后来鼓起勇气问他要几次，他说两次，让我自己想怎么称？</p><p>● 给定随机函数R，以p概率产生1,1-p产生0。生成随机函数R’，以1/2概率产生0,1？</p><p>● 54张扑克牌，分成三等份，大小王在同一组的概率？</p><p>● 求一根绳子被切两刀能组成一个三角形的概率。</p><p>● 有一苹果，两个人抛硬币来决定谁吃，先抛到正面的先吃，问先抛者吃到苹果的概率？</p><p>● 01 2 3 4 5 6 7 8 9 下面写一个数，使得下面这个数刚好是这个数字在下面一行出现的次数</p><p>● 投篮命中率10%，那么a.投10次中1次 b.投10000次中&lt;1000次，哪个概率大？</p><p>● 一段绳子分三段，组成三角形概率？</p><p>● 棋子在规定走法，规定大小的棋盘上，N步后还在棋盘上的概率，主要考察动态规划</p><p>● 一个NxN的棋盘，一个棋子可以等概率地跳八个方向（和象棋中马一样的跳法）。当这个棋子跳出棋盘范围的时候，就停止。问棋子跳了k步之后，棋子还留在棋盘的概率。</p><p>● 学校男生的概率2/3，女生的概率1/3，男生穿牛仔的概率2/3，女生穿牛仔的概率1/3，你看到一个穿牛仔的，问他是男生的概率是？</p><p>● 六边形，顶点上各有6只毛毛虫，可以沿着边走，两个毛毛虫相遇的概率，n边形呢</p><p>● 有A、B两枚不同的硬币，它们正面朝上的概率不一定是0.5，且两枚硬币正面朝上的概率不一定相同。现在做1000次这样的实验：从两枚硬币中随机抽一枚抛一下，记录下正面，重复100次/10次。问如何通过这1000次实验的结果求出A、B两枚硬币正面向上的概率？</p><p>● 一个总数很大的数据流，希望以等概率的方式进行样本数为一的抽样，怎么做？</p><p>● 将一根木棍分成三段，求这三段构成三角形的概率？</p><h3 id="4-3-4-矩阵运算"><a href="#4-3-4-矩阵运算" class="headerlink" title="4.3.4 矩阵运算"></a><strong>4.3.4 矩阵运算</strong></h3><p>**<br>**</p><p>● 一个m*n矩阵，从左往右逐步递增，下一行比上一行数都打，查找某一个值是否出现？</p><p>● 说一下矩阵的秩?再说一下矩阵的特征值和特征向量?(数学专业背景)</p><h3 id="4-3-5-其他"><a href="#4-3-5-其他" class="headerlink" title="4.3.5 其他"></a><strong>4.3.5 其他</strong></h3><p>**<br>**</p><p>● 贪心和DP介绍一下？</p><p>● DP的一般做法流程？</p><p>● 布隆过滤器知道吗？用在什么场景下？推导会么（加分项）</p><p>● 线性回归多变量求解的过程，为什么这样求解？这样求解为什么是最优解？</p><p>● 纳什均衡看过吗？</p><h2 id="4-4Leetcode-amp-剑指offer原题"><a href="#4-4Leetcode-amp-剑指offer原题" class="headerlink" title="4.4Leetcode&amp;剑指offer原题"></a>4.4Leetcode&amp;剑指offer原题</h2><p>● Leetcode原题：接雨水</p><p>● Leetcode 23：合并K个升序链表</p><p>● Leetcode236 ：判断围棋死活</p><p>● Leetcode139：单词拆分</p><p>● Leetcode382：链表随机节点，并口述蓄水池采样算法的推导</p><h1 id="5-编程高频问题：Python-amp-C-x2F-C-方面"><a href="#5-编程高频问题：Python-amp-C-x2F-C-方面" class="headerlink" title="5 编程高频问题：Python&amp;C/C++方面"></a><strong>5</strong> <strong>编程高频问题：Python&amp;C/C++方面</strong></h1><p><img src="https://mmbiz.qpic.cn/mmbiz_png/Zibun5D7npIMPE0m2oQe9ExZ7Qgz2QbgicW8MYiaceczBaetu8QztoYGh2C7hflzAqyjmInxrmEGBaBQn0Y69r5yQ/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></p><h2 id="5-1-python-方面"><a href="#5-1-python-方面" class="headerlink" title="5.1 python****方面"></a><strong>5.1 python****方面</strong></h2><h3 id="5-1-1-网络框架方面"><a href="#5-1-1-网络框架方面" class="headerlink" title="5.1.1 网络框架方面"></a><strong>5.1.1 网络框架方面</strong></h3><h4 id="5-1-1-1-Pytorch相关"><a href="#5-1-1-1-Pytorch相关" class="headerlink" title="5.1.1.1 Pytorch相关"></a><strong>5.1.1.1 Pytorch相关</strong></h4><p>**<br>**</p><p>● tensorflow和torch的区别？</p><h4 id="5-1-1-2-Tensorflow相关"><a href="#5-1-1-2-Tensorflow相关" class="headerlink" title="5.1.1.2 Tensorflow相关"></a><strong>5.1.1.2 Tensorflow相关</strong></h4><p>**<br>**</p><p>● Tensorflow如何读取数据？</p><p>● TensorFlow的参数初始化机制？</p><p>● Tensorflow写一个全连接层的代码？</p><h4 id="5-1-1-3-Caffe相关"><a href="#5-1-1-3-Caffe相关" class="headerlink" title="5.1.1.3 Caffe相关"></a><strong>5.1.1.3 Caffe相关</strong></h4><p>● 对caffe源码熟悉程度。（我扯了扯源码的底层设计模式，数据流怎么流的，如何添加新层、cuda代码的细节）</p><h3 id="5-1-2-基础知识方面"><a href="#5-1-2-基础知识方面" class="headerlink" title="5.1.2 基础知识方面"></a><strong>5.1.2 基础知识方面</strong></h3><h4 id="5-1-2-1-线程相关"><a href="#5-1-2-1-线程相关" class="headerlink" title="5.1.2.1 线程相关"></a><strong>5.1.2.1 线程相关</strong></h4><p>**<br>**</p><p>● Pyhton线程和协程的区别？</p><h4 id="5-1-2-2-内存相关"><a href="#5-1-2-2-内存相关" class="headerlink" title="5.1.2.2 内存相关"></a><strong>5.1.2.2 内存相关</strong></h4><p>**<br>**</p><p>● python语言怎么处理内存溢出的情况，怎么设计内存回收？</p><p>● Python的内存管理</p><p>● Python垃圾回收机制</p><h4 id="5-1-2-3-区别比较"><a href="#5-1-2-3-区别比较" class="headerlink" title="5.1.2.3 区别比较"></a><strong>5.1.2.3 区别比较</strong></h4><p>**<br>**</p><p>● 说说list和tuple的区别？  list和set的区别，装饰器</p><p>● 生成器、迭代器的区别？</p><p>● copy，deepcopy，赋值的区别？</p><h4 id="5-1-2-4-讲解原理"><a href="#5-1-2-4-讲解原理" class="headerlink" title="5.1.2.4 讲解原理"></a><strong>5.1.2.4 讲解原理</strong></h4><p>**<br>**</p><p>● Python里面的字典的key可以用list吗？可以用tuple吗？可以用set吗？为什么？从底层实现原理说一下？</p><p>● Python里面的循环很慢，为什么？</p><p>● Python怎么生成一个迭代器？</p><p>● 讲一下yield关键字？它的作用是啥？</p><p>● 什么是装饰器？</p><p>● 说一说python重载(面像对象)</p><h4 id="5-1-2-5-讲解应用"><a href="#5-1-2-5-讲解应用" class="headerlink" title="5.1.2.5 讲解应用"></a><strong>5.1.2.5 讲解应用</strong></h4><p>**<br>**</p><p>● Python、C++、Java 哪个用的多一点？值传递和引用传递区别。</p><h3 id="5-1-3-手写代码方面"><a href="#5-1-3-手写代码方面" class="headerlink" title="5.1.3 手写代码方面"></a><strong>5.1.3 手写代码方面</strong></h3><p>**<br>**</p><p>● Python字典的删除实现</p><h2 id="5-2-C-x2F-C-方面"><a href="#5-2-C-x2F-C-方面" class="headerlink" title="**5.2 C/C++**方面"></a>**5.2 C/C++**<strong>方面</strong></h2><h3 id="5-2-1-基础知识"><a href="#5-2-1-基础知识" class="headerlink" title="5.2.1 基础知识"></a><strong>5.2.1 基础知识</strong></h3><h4 id="5-2-1-1-线程相关"><a href="#5-2-1-1-线程相关" class="headerlink" title="5.2.1.1 线程相关"></a><strong>5.2.1.1 线程相关</strong></h4><p>**<br>**</p><p>● C++多线程熟不，有什么库？你使用过什么库？</p><h4 id="5-2-1-2-内存相关"><a href="#5-2-1-2-内存相关" class="headerlink" title="5.2.1.2 内存相关"></a><strong>5.2.1.2 内存相关</strong></h4><p>**<br>**</p><p>● C++中的内存泄漏是怎么发生的？</p><p>● 如何避免C++中发生内存泄漏？</p><h4 id="5-2-1-3-区别比较"><a href="#5-2-1-3-区别比较" class="headerlink" title="5.2.1.3 区别比较"></a><strong>5.2.1.3 区别比较</strong></h4><p>**<br>**</p><p>● C的动态库和静态库？</p><p>● C++中引用和指针的区别？</p><p>● C++operator new和new operator的区别</p><p>● C++的继承和Java继承的区别？</p><p>● 接口和类的区别？</p><p>● C++、Java、Python的主要区别(编译型语言和解释性语言)</p><h4 id="5-2-1-4-讲解原理"><a href="#5-2-1-4-讲解原理" class="headerlink" title="5.2.1.4 讲解原理"></a><strong>5.2.1.4 讲解原理</strong></h4><p>**<br>**</p><p>● C的相关特性，多态？</p><p>● C++中map、hash_map底层实现及增删改查的复杂度</p><p>● C++智能指针了解吗？</p><h1 id="6-操作系统高频问题：数据库-amp-线程-amp-常用命令等"><a href="#6-操作系统高频问题：数据库-amp-线程-amp-常用命令等" class="headerlink" title="6 操作系统高频问题：数据库&amp;线程&amp;常用命令等"></a><strong>6</strong> <strong>操作系统高频问题：数据库&amp;线程&amp;常用命令等</strong></h1><p><img src="https://mmbiz.qpic.cn/mmbiz_png/Zibun5D7npIMPE0m2oQe9ExZ7Qgz2QbgicZcD61PiazhOAbUanLAsXu3lzAyCvXwKaAEfcw2ia541BMFmESG9GZ5Yg/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="图片"></p><h2 id="6-1-数据库方面"><a href="#6-1-数据库方面" class="headerlink" title="6.1 数据库方面"></a><strong>6.1</strong> <strong>数据库方面</strong></h2><h3 id="6-1-1-基础知识"><a href="#6-1-1-基础知识" class="headerlink" title="6.1.1 基础知识"></a><strong>6.1.1 基础知识</strong></h3><p>**<br>**</p><p>● 数据库：什么是主键？主键的作用？</p><h2 id="6-2-操作系统方面"><a href="#6-2-操作系统方面" class="headerlink" title="6.2 操作系统方面"></a><strong>6.2</strong> <strong>操作系统方面</strong></h2><h3 id="6-2-1-TCP协议相关"><a href="#6-2-1-TCP协议相关" class="headerlink" title="6.2.1 TCP协议相关"></a><strong>6.2.1 TCP协议相关</strong></h3><p>**<br>**</p><p>● TCP与UDP的区别</p><p>● TCP/UDP简单介绍一下，如何用UDP来实现TCP</p><h3 id="6-2-2-线程和进程相关"><a href="#6-2-2-线程和进程相关" class="headerlink" title="6.2.2 线程和进程相关"></a><strong>6.2.2 线程和进程相关</strong></h3><h4 id="6-2-2-1-区别比较"><a href="#6-2-2-1-区别比较" class="headerlink" title="6.2.2.1 区别比较"></a><strong>6.2.2.1 区别比较</strong></h4><p>**<br>**</p><p>● 线程和进程的区别？</p><p>● 进程、线程的区别和联系，线程共享哪些资源</p><h4 id="6-2-2-2-讲解原理"><a href="#6-2-2-2-讲解原理" class="headerlink" title="6.2.2.2 讲解原理"></a><strong>6.2.2.2 讲解原理</strong></h4><p>**<br>**</p><p>● 线程进程是什么、什么关系、什么时候用线程，什么时候用进程？</p><p>● Linux多个进程如何通信的？</p><h4 id="6-2-2-3-讲解应用"><a href="#6-2-2-3-讲解应用" class="headerlink" title="6.2.2.3 讲解应用"></a><strong>6.2.2.3 讲解应用</strong></h4><p>**<br>**</p><p>● 详细讲讲线程进程的区别，还有一些具体场景下的变化</p><h3 id="6-2-3-常用命令"><a href="#6-2-3-常用命令" class="headerlink" title="6.2.3 常用命令"></a><strong>6.2.3 常用命令</strong></h3><p>**<br>**</p><p>● Linux如何查看进程</p><p>● Linux中查看进程状态和查看开放端口的命令</p><p>● 常见的操作指令，还有一些场景题：例如shell里面写查询一个服务器日志文件中访问最多的那个ip</p><h1 id="7-技术-amp-产品-amp-开放性问题"><a href="#7-技术-amp-产品-amp-开放性问题" class="headerlink" title="7 技术&amp;产品&amp;开放性问题"></a><strong>7</strong> <strong>技术&amp;产品&amp;开放性问题</strong></h1><h2 id="7-1-技术方面"><a href="#7-1-技术方面" class="headerlink" title="7.1 技术方面"></a><strong>7.1</strong> <strong>技术方面</strong></h2><p>**<br>**</p><p>● 问一个区域确定人口中心划区域用什么方法，开始我说用k-means，然后他问我具体怎么实现。我说先设定一个k值，他问k值怎么定，拍脑门吗？不过在没有先验信息的情况下，确实没法选，就多试几个不行吗。然后我说那用密度聚类。那密度聚类的密度函数用什么？</p><p>● 由于拍摄焦距的原因，有些图片的前景很清晰，但是背景很模糊。导致分类的之后分成了不清晰的图片。你觉得有什么解决的办法？</p><p>学一个mask的网络，把图像中清晰的部分扣出来。再去分类。</p><p>我说了用一些传统的手工特征如SIFT，HOG之类。去比较每帧之间的差异。</p><p>● 大量数据，亿为单位，找出与给定数据最相似的一个？</p><p>● 如何检测视频转场的时间，转场就是拍摄的场景变化了。</p><p>用光流算法？或者用LSTM（这里问了很多，比如光流的好处，LSTM的好处之类的，如何使用LSTM）</p><p>● 给饭店确定菜系，是鲁菜、川菜、西餐、混合菜系。。。你需要收集哪些数据，用什么方法？</p><p>● 对一个数据，比如点击率，应该做哪些处理？</p><p>● 现在的搜索技术很少上深度学习或者说很深的网络，你觉得是为什么？如果要用深度学习，你觉得应该往哪些方向思考？deep learning比较吃资源，不太适合业务规模比较大的系统，比如：双十一的压力，如果一定要用，可以考虑深度模型压缩，量化，矮胖网络的并行计算等方向。</p><p>● 在一个坐标系内，用户和商户都有自己的坐标（x,y），那么我想找到距离用户最近的k个商户，如何最快的得到？</p><p>● 比如淘宝搜索时的自动补全该怎么做，用什么模型或者算法（之后查了一下用模糊匹配）</p><p>● 原始数据被污染了怎么办，这时候怎么判断是模型的问题还是数据的问题。</p><p>● 通过一个单目固定相机，如何获得室内桌子等物体的3D信息（回答的是3D CNN + 卡尔曼滤波）</p><p>● 给了一个情景，如何训练模型、调优。（题目很空，主要考察你对深度学习的理解）</p><p>-根据需求（前向传播时间、模型大小），确定模型和基础网络，跑第一版模型。（举了个栗子）</p><p>-判断模型是否出现过拟合的情况，来决定下一步的优化方向。</p><p>-结果分析(confusionMatrix等)，分析问题，将论文中的方法套上去，如果没有自己创造。（又举了个栗子）</p><p>● 设计一个情景，倾斜字体检测，问我有什么好的想法？（我觉得应该是他现在遇到的问题）</p><p>数据增强，加入形变扰动。</p><p>非end-to-end版本：分别训练检测和分类，举了之前做过的一个文字识别的项目的实现。</p><p>end-to-end版本：加入仿射变换学习因子，学习字体倾斜的角度和形变。</p><p>● 如果让你设计一个推荐系统，你会设计一个什么样的架构？你设计的重点是什么？</p><p>● 在广告推荐的时候，我们常将展示出来但是用户没有点击的广告作为负样本，然而其实有的时候真实的情形是用户没有注意或者没有看见这个广告，</p><p>也就是说这条广告不是真正的负样本，用户对其可能是感兴趣的，这种情况该如何处理？</p><p>● 对抗学习有了解吗？你觉得该如何将NLP和推荐相互结合？</p><p>● 为什么人工智能在图像里应用落地更好，在nlp不行。谈谈你的看法。</p><p>● 双十一时向用户发放优惠券，希望在成本一定的前提下，使得盈利最大化，该如何建模发放给用户？用户无法做AB测试，该怎样划定正负样本？</p><p>● 模型上线时应该注意的事，如果请求过高模型服务挂了怎么办？</p><p>● 假设我们有很多数据给用户看到，但是我们只知道一部分的数据用户是否点击了，其他的我们都不知道用户是否点击了（相当于没有标签），如果是你的话，你怎么解决这个问题？</p><p>● 根据你的经验，你认为深度学习比传统机器学习好在哪里？（1. 特征工程：学习能力更强，同样多的数据，深度学习可以更准确地提取特征，而且深度学习可以自动提取特征，而传统机器学习很多需要手动提取特征 2. 比较灵活，我可以自己决定CNN有多少个卷积层，多少个池化层等等）</p><p>● 有没有非网络技术上的攻击，就是对模型本身的攻击？如果有，怎么应对？</p><p>● 怎么防止新来的错误数据对原有模型进行特别大的改变，怎么防数据污染，蓄意破坏？</p><p>● 双目相机识别目标深度的原理，以及PnP算法的原理</p><p>● 支付宝年末要出一个年终总结，那么我要对所有用户的交易额度进行全量的排序，那么内存肯定是不够用的，这种情况下应该怎么做？</p><p>● 输入是一个短文本，判断是否是服装相关内容</p><p>● 阿里有很多商品都是由一段话来描述的，现在让我来设计一个模型，输入是商品的描述，输出是描述商品的一句简短的话，要求用户看到这句话后尽可能保证不降低用户的体验（包括购买率、点击率、浏览量不要下降）</p><p>● 给定一堆商品C，每个商品c对应有标题、描述，给定标签：时尚风、复古风、百搭风。设计技术方案，把每个商品c的主题标签给分类/聚类 等弄出来。没有训练数据，有哪些方案？</p><p>● 做分类的时候如何评估好坏呢？追问比如:你分类总数为2，但是现在出现第3种类别，怎么办，例如做的分类是男女分类，现在有个中性的人，那你怎么办呢？如何应对这种情况？</p><p>● 场景题:如何从百万数据中找出最大的k个数？分治+堆排序</p><p>● 一个很大的日志文件里面存了各个ip访问的信息，几百G，如何统计里面某个ip地址访问的次数？</p><p>答：不一次性读取，分批读，缓存，然后分别统计，最后加起来。</p><p>● 开放式探讨：如何使用强化学习去实现个性化商品推荐？</p><p>● 一个超级大文件，每一行有一个 ip 地址，内存有限，如何找出其中重复次数最多的 ip 地址</p><h2 id="7-2-产品方面"><a href="#7-2-产品方面" class="headerlink" title="7.2 产品方面"></a><strong>7.2</strong> <strong>产品方面</strong></h2><p>**<br>**</p><p>● 怎么给新上架的商品做冷启动？</p><p>● 口碑要拉新客，我们的策略是发红包，怎么如何在预算有限的情况下发红包能让最多的用户来安装口碑呢？</p><p>● 给你一些用户每天的相对位置信息，怎么区分它们的职业？怎么判断上线的现金贷产品的盈利能力？</p><p>● 天猫有1000万条数据，如何找到使用人数最多的前1000个？</p><p>● 给你淘宝的商品总量，怎么预测拼多多的商品总量</p><p>● 给出用户的数据和交易记录，如何判断是否给他开通花呗？</p><p>● 在有各种用户的数据，上网状态，手机型号，用户照片等数据的情况下，怎么判断支付是否存在欺诈？</p><h2 id="7-3-开放性问题"><a href="#7-3-开放性问题" class="headerlink" title="7.3 开放性问题"></a><strong>7.3</strong> <strong>开放性问题</strong></h2><p>**<br>**</p><p>● 当碰到难题时，团队士气低落的时候，作为团队的一员，该怎么去做？</p><p>● 如何把AI技术用到素材生成和智能分发场景中？</p><p>● AI还有哪些能够应用到视频上的？</p><p>● 你觉得在公司或企业里，工程和算法的界限是什么？</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/04/21/untitled/"/>
      <url>/2022/04/21/untitled/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/04/15/zai-yin-he-qi-lin-gao-ji-fu-wu-qi-cao-zuo-xi-tong-v10-shang-an-zhuang-docker/"/>
      <url>/2022/04/15/zai-yin-he-qi-lin-gao-ji-fu-wu-qi-cao-zuo-xi-tong-v10-shang-an-zhuang-docker/</url>
      
        <content type="html"><![CDATA[<h1 id="在银河麒麟高级服务器操作系统V10上安装docker"><a href="#在银河麒麟高级服务器操作系统V10上安装docker" class="headerlink" title="在银河麒麟高级服务器操作系统V10上安装docker"></a>在银河麒麟高级服务器操作系统V10上安装docker</h1><p><a href="https://www.zhihu.com/people/neverland-60">整点bug</a></p><blockquote><p>银河麒麟高级服务器操作系统 V10 是针对企业级关键业务，适应虚拟化、 云计算、大数据、工业互联网时代对主机系统可靠性、安全性、性能、扩展性和 实时性的需求，依据 CMMI 5 级标准研制的提供内生安全、云原生支持、国产 平台深入优化、高性能、易管理的新一代自主服务器操作系统；同源支持飞腾、 龙芯、申威、兆芯、海光、鲲鹏等自主平台；可支撑构建大型数据中心服务器高 可用集群、负载均衡集群、分布式集群文件系统、虚拟化应用和容器云平台等， 可部署在物理服务器和虚拟化环境、私有云、公有云和混合云环境；应用于政府、 国防、金融、教育、财税、公安、审计、交通、医疗、制造等领域。</p></blockquote><p>公司有个项目需要将系统部署在 <strong>kylinos</strong>上，刚开始还有点头疼，害怕各种程序无法安装和使用，等安装好服务器进行使用的时候发现这不就是基于centos的嘛，虽然基于哪个版本不知道，但是可以测试的，于是我一顿操作，最后发现它是基于Centos8的，系统内核版本是 4.19，问题不大，既然是基于Centos8的，那Centos8上能跑的程序，在这肯定也能跑，然后我就开始了愉快（痛苦）的安装docker之旅了。</p><h3 id="配置阿里云Centos8镜像源"><a href="#配置阿里云Centos8镜像源" class="headerlink" title="配置阿里云Centos8镜像源"></a><strong>配置阿里云Centos8镜像源</strong></h3><p>之所以要配置 Centos8 的镜像源是因为在安装docker的时候需要额外的一些依赖，而这些依赖在麒麟官方的源里面是没有的。</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-8.repo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="配置阿里云-docker-镜像源"><a href="#配置阿里云-docker-镜像源" class="headerlink" title="配置阿里云 docker 镜像源"></a><strong>配置阿里云 docker 镜像源</strong></h3><pre class="line-numbers language-text" data-language="text"><code class="language-text">yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.reposed -i 's+download.docker.com+mirrors.aliyun.com/docker-ce+' /etc/yum.repos.d/docker-ce.repo<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="定义-yum-变量-amp-修改-repo"><a href="#定义-yum-变量-amp-修改-repo" class="headerlink" title="定义 yum 变量&amp;修改 repo"></a><strong>定义 yum 变量&amp;修改 repo</strong></h3><p>修改 centos 和 docker <code>repo</code>文件中的 <code>$releasever</code> 为 <code>centos_version</code> ，原因是在麒麟服务器操作系统V10中 <code>$releasever</code>被修改为了 10，而我们需要使用 centos 8的镜像源，如果你不替换，基本上仓库的每一个地址都是404。</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">echo "8" &gt; /etc/yum/vars/centos_versionsed -i 's/$releasever/$centos_version/g' /etc/yum.repos.d/docker-ce.reposed -i 's/$releasever/$centos_version/g' /etc/yum.repos.d/CentOS-Base.repo<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="建立yum缓存"><a href="#建立yum缓存" class="headerlink" title="建立yum缓存"></a><strong>建立yum缓存</strong></h3><p>没啥可说的</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">yum makecache<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="查看docker-ce-版本"><a href="#查看docker-ce-版本" class="headerlink" title="查看docker-ce 版本"></a><strong>查看docker-ce 版本</strong></h3><pre class="line-numbers language-text" data-language="text"><code class="language-text">yum list docker-ce --showduplicates | sort -rdocker-ce.x86_64               3:20.10.9-3.el8                 docker-ce-stabledocker-ce.x86_64               3:20.10.8-3.el8                 docker-ce-stabledocker-ce.x86_64               3:20.10.7-3.el8                 docker-ce-stabledocker-ce.x86_64               3:20.10.6-3.el8                 docker-ce-stabledocker-ce.x86_64               3:20.10.5-3.el8                 docker-ce-stabledocker-ce.x86_64               3:20.10.4-3.el8                 docker-ce-stabledocker-ce.x86_64               3:20.10.3-3.el8                 docker-ce-stabledocker-ce.x86_64               3:20.10.2-3.el8                 docker-ce-stabledocker-ce.x86_64               3:20.10.1-3.el8                 docker-ce-stabledocker-ce.x86_64               3:20.10.12-3.el8                docker-ce-stabledocker-ce.x86_64               3:20.10.11-3.el8                docker-ce-stabledocker-ce.x86_64               3:20.10.10-3.el8                docker-ce-stabledocker-ce.x86_64               3:20.10.0-3.el8                 docker-ce-stabledocker-ce.x86_64               3:19.03.15-3.el8                docker-ce-stabledocker-ce.x86_64               3:19.03.15-3.el8                @docker-ce-stabledocker-ce.x86_64               3:19.03.14-3.el8                docker-ce-stabledocker-ce.x86_64               3:19.03.13-3.el8                docker-ce-stable<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a><strong>安装docker</strong></h3><p>这里要安装 docker-ce 19.03 版本，因为我在使用最新版 20.10 启动容器时出现了未知的权限问题，而麒麟服务器操作系统资料相对较少，我未能找到相应的解决方案，只好退而求其次，换到上一个稳定版本。</p><p>20.10 版本错误信息如下：</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">docker: Error response from daemon: OCI runtime create failed: container_linux.go:318: starting container process caused "permission denied": unknown.ERRO[0000] error waiting for container: context canceled<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>还是安装 19.03 版本吧。</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">yum install docker-ce-19.03.15 docker-ce-cli-19.03.15 containerd.io -y<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="启动docker"><a href="#启动docker" class="headerlink" title="启动docker"></a><strong>启动docker</strong></h3><pre class="line-numbers language-text" data-language="text"><code class="language-text">systemctl start dockersystemctl enable docker<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="启动-hello-world-进行测试"><a href="#启动-hello-world-进行测试" class="headerlink" title="启动 hello-world 进行测试"></a><strong>启动 hello-world 进行测试</strong></h3><pre class="line-numbers language-text" data-language="text"><code class="language-text">root@localhost ~]# docker run hello-worldUnable to find image 'hello-world:latest' locallylatest: Pulling from library/hello-world2db29710123e: Pull completeDigest: sha256:2498fce14358aa50ead0cc6c19990fc6ff866ce72aeb5546e1d59caac3d0d60fStatus: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.    (amd64) 3. The Docker daemon created a new container from that image which runs the    executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it    to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>完美使用 -:)</p><p>发布于 2021-12-22 00:22</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>轻松掌握 MMDetection 中常用算法(二)：Faster R-CNN|Mask R-CNN</title>
      <link href="/2022/04/12/mmdetection-xue-xi-bi-ji-4-gou-jian-faster-r-cnn-ji-pei-zhi/"/>
      <url>/2022/04/12/mmdetection-xue-xi-bi-ji-4-gou-jian-faster-r-cnn-ji-pei-zhi/</url>
      
        <content type="html"><![CDATA[<h1 id="轻松掌握-MMDetection-中常用算法-二-：Faster-R-CNN-Mask-R-CNN"><a href="#轻松掌握-MMDetection-中常用算法-二-：Faster-R-CNN-Mask-R-CNN" class="headerlink" title="轻松掌握 MMDetection 中常用算法(二)：Faster R-CNN|Mask R-CNN"></a>轻松掌握 MMDetection 中常用算法(二)：Faster R-CNN|Mask R-CNN</h1><h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0 前言"></a>0 前言</h2><p>在<a href="https://zhuanlan.zhihu.com/p/346198300">轻松掌握 MMDetection 中常用算法(一)：RetinaNet 及配置详解</a>一文中，对经典 one-stage 目标检测算法 RetinaNet 以及相关配置参数进行了详细说明，本文解读经典 two-stage 算法 Faster R-CNN 以及改进版 Mask R-CNN。需要特别注意的是：如果涉及到和 RetinaNet 相同的配置，本文不再进一步描述，读者请查看 RetinaNet 一文解读。</p><p>项目地址：</p><p><a href="https://link.zhihu.com/?target=https://github.com/open-mmlab/mmdetection">https://github.com/open-mmlab/mmdetectiongithub.com/open-mmlab/mmdetection</a></p><p>欢迎 star</p><h2 id="1-Faster-R-CNN-和-Mask-R-CNN-简介"><a href="#1-Faster-R-CNN-和-Mask-R-CNN-简介" class="headerlink" title="1 Faster R-CNN 和 Mask R-CNN 简介"></a>1 Faster R-CNN 和 Mask R-CNN 简介</h2><p><strong>Faster R-CNN</strong> （Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks） 是目标检测领域最为经典的方法之一，<strong>通过 RPN(Region Proposal Networks) 区域提取网络</strong>和 <strong>R-CNN 网络联合训练</strong>实现高效目标检测。其简要发展历程为：</p><ol><li><strong>R-CNN。</strong>首先通过<strong>传统的 selective search</strong> 算法在图片上<strong>预取 2000 个左右 Region Proposal</strong>；接着将这些 Region Proposal 通过前处理<strong>统一尺寸输入到 CNN</strong> 中进行特征提取；然后把所提取的<strong>特征输入到 SVM</strong> 支持向量机中进行分类；最后对<strong>分类后的 Region Proposal 进行 bbox 回归</strong>。此时算法的整个过程较为繁琐，速度也较慢。</li><li><strong>Fast R-CNN。</strong>首先通过<strong>传统的 selective search</strong> 算法在图片上<strong>预取 2000 个左右 Region Proposal</strong>；接着对<strong>整张图片进行特征提取</strong>；然后利用 Region Proposal 坐标<strong>在 CNN 的最后一个特征图上进去 RoI 特征图提取</strong>；最后<strong>将所有 RoI 特征输入到分类和回归模块</strong>中。此时算法的整个过程相比 R-CNN 得到极大的简化，但依然无法联合训练。</li><li><strong>Faster R-CNN。</strong>首先通过可学习的 <strong>RPN 网络进行 Region Proposal 的预取</strong>；接着利用 Region Proposal 坐标在 <strong>CNN 的特征图上进行 RoI 特征图提取</strong>；然后<strong>利用 RoI Pooling 层进行空间池化</strong>使其所有特征图输出尺寸相同；最后将所有特征图输入到后续的 <strong>FC 层进行分类和回归</strong>。此时算法的整个过程一气呵成，实现了端到端训练。</li></ol><p>Faster R-CNN 的出现改变了整个目标检测算法的发展历程。之所以叫做 two-stage 检测器，原因是其<strong>包括一个区域提取网络 RPN 和 RoI Refine 网络 R-CNN，同时为了将 RPN 提取的不同大小的 RoI 特征图组成 batch 输入到后面的 R-CNN 中，在两者中间还插入了一个 RoI Pooling 层，可以保证任意大小特征图输入都可以变成指定大小输出</strong>。简要结构图如下所示：</p><p><img src="https://pic3.zhimg.com/80/v2-fc4a5e8c33187dbdcde632d1c7746792_720w.jpg" alt="img"></p><p>如果和 RetinaNet 进行类比的话，其过程相当于 <strong>二分类的 RetinaNet + RoI Pooling + 简单 FC 层多分类和回归网络</strong>，Faster R-CNN 也属于 Anchor-based 类算法。</p><p>Faster R-CNN 之后，考虑到多尺度预测问题，后续又提出了改进版本<strong>特征金字塔 FPN(Feature Pyramid Networks for Object Detection)<strong>。 通过分析目前目标检测中存在的图像金字塔、单层预测和多层预测问题，提出了一个简单的，通过从上到下路径和横向连接，结合高分辨率、弱语义信息的特征层和低分辨率、强语义信息的特征融合，实现类似图像金字塔效果，</strong>顶层特征通过上采样和低层特征做融合，而且每层都是独立预测的</strong>，效果显著，如下图所示：</p><p><img src="https://pic4.zhimg.com/80/v2-5a78ef8716761b468a1ae5f4d9810d13_720w.jpg" alt="img"></p><p><strong>由于其强大的性能，更加模块化现代化的设计，现在提到 Faster R-CNN, 一般默认是指的 FPN 网络。本文解读的 Faster R-CNN 网络实际上也是指的 FPN。</strong></p><p>在 FPN 提出后，Kaiming He 等进一步对其进行任务扩展，<strong>提出了 Mask R-CNN</strong>，通过<strong>新增 mask 掩码分支实现实例分割任务</strong>，其最大特点是<strong>任务扩展性强</strong>，通过新增不同分支就可以实现不同的扩展任务。<em>例如可以将 mask 分支替换为关键点分支即可实现多人姿态估计。</em>除此之外，为解决特征图与原始图像上的 RoI 不对准的问题，<strong>提出了 ROIAlign 模块</strong>。其简要示意图如下：</p><p><img src="https://pic3.zhimg.com/80/v2-1a0e112124c91b3c6c6df64359613b76_720w.jpg" alt="img"></p><p>带有 FPN 的 Faster R-CNN 和 Mask R-CNN 算法是目前的主流算法，应用非常广泛。并且由于 Faster R-CNN 与 Mask R-CNN 属于同一系列，因此本文将这两个核心算法同时解读。</p><h2 id="2-Faster-R-CNN-代码详解"><a href="#2-Faster-R-CNN-代码详解" class="headerlink" title="2 Faster R-CNN 代码详解"></a>2 Faster R-CNN 代码详解</h2><p>为方便算法与代码的解读，Faster R-CNN 模型整体流程如下所示：</p><p><img src="https://pic1.zhimg.com/80/v2-2ea42c8f593cb15b486025289f35e1b8_720w.jpg" alt="img"></p><ol><li>图片输入到 ResNet 中进行<strong>特征提取</strong>，输出 4 个特征图，按照特征图从大到小排列，分别是 C2 C3 C4 C5，stride = 4,8,16,32</li><li>4 个特征图输入到 FPN 模块中进行<strong>特征融合</strong>，输出 5 个通道数相同的特征图,分别是 p2 ~ p6，stride = 4,8,16,32,64</li><li>FPN 输出的 5 个特征图，输入到同一个 RPN 或者说 5 个相同的 RPN 中，每个分支都进行<strong>前后景分类和 bbox 回归</strong>，然后就可以和 label 计算 loss</li><li>在 5 个 RPN 分支输出的基础上，采用 RPN test 模块<strong>输出指定个数的 Region Proposal</strong>，将 Region Proposal 按照重映射规则，在对应的 p2 ~ p5 特征图上进行特征提取，注意并没有使用 p6 层特征图，从而得到指定个数例如 2k 个 Region Proposal 特征图</li><li>将 2k 个不同大小的 RoI 区域<strong>特征图</strong>输入到 RoIAlign 或者 RoIPool 层中进行<strong>统一采样</strong>，得到指定输出 shape 的 2k 个特征图</li><li>组成 batch <strong>输入到两层 FC 中进行多类别的分类和回归，</strong>其 loss 和 RPN 层 loss 相加进行联合训练</li></ol><p>下面将会对每个模块进行详细的分析。值得注意的是，除了 RoI Head 模块外，其他模块在前一篇 RetinaNet 都有介绍，大家也可以作为参考，方便辅助理解。</p><h3 id="2-1-Backbone"><a href="#2-1-Backbone" class="headerlink" title="2.1 Backbone"></a>2.1 Backbone</h3><p><img src="https://pic1.zhimg.com/80/v2-34e9618f1a3e91e31e70f2248255b508_720w.jpg" alt="img"></p><p>由于 Faster R-CNN 是后续各个算法的 baseline 且用途非常广泛，OpenMMLab 提供了非常多的模型配置供研究或者不同任务 fintune 用，几乎覆盖了所有常用配置，如下所示：</p><ul><li>1x、2x 和 3x 的模型配置和权重</li><li>多尺度训练配置和权重</li><li>不同骨架的配置和权重</li><li>PyTorch 和 Caffe style 的配置和权重</li><li>各种 loss 对比配置和权重</li><li>不包含 FPN 的 Faster R-CNN 配置和权重</li><li>常用类别例如 person 的配置和权重，可作为<strong>下游任务例如行人检测的预训练权重，性能极佳</strong></li></ul><p>以 ResNet50 为例，其配置和 RetinaNet 完全相同，此处不再描述。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 使用 pytorch 提供的在 imagenet 上面训练过的权重作为预训练权重</span>pretrained<span class="token operator">=</span><span class="token string">'torchvision://resnet50'</span><span class="token punctuation">,</span>backbone<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># 骨架网络类名</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'ResNet'</span><span class="token punctuation">,</span>    <span class="token comment"># 表示使用 ResNet50</span>    depth<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span>    <span class="token comment"># ResNet 系列包括 stem+ 4个 stage 输出</span>    num_stages<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    <span class="token comment"># 表示本模块输出的特征图索引，(0, 1, 2, 3),表示4个 stage 输出都需要，</span>    <span class="token comment"># 其 stride 为 (4,8,16,32)，channel 为 (256, 512, 1024, 2048)</span>    <span class="token comment"># stride表示模型的下采样率</span>    out_indices<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 表示固定 stem 加上第一个 stage 的权重，不进行训练</span>    frozen_stages<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>    <span class="token comment"># 所有的 BN 层的可学习参数都不需要梯度，也就不会进行参数更新</span>    norm_cfg<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'BN'</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># backbone 所有的 BN 层的均值和方差都直接采用全局预训练值，不进行更新</span>    norm_eval<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token comment"># 默认采用 pytorch 模式</span>    style<span class="token operator">=</span><span class="token string">'pytorch'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-2-Neck"><a href="#2-2-Neck" class="headerlink" title="2.2 Neck"></a>2.2 Neck</h3><p>虽然都采用了 FPN，但是 Faster R-CNN 的配置和 RetinaNet 不同，</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">neck<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'FPN'</span><span class="token punctuation">,</span>    <span class="token comment"># ResNet 模块输出的4个尺度特征图通道数</span>    in_channels<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">2048</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token comment"># FPN 输出的每个尺度输出特征图通道</span>    out_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>    <span class="token comment"># FPN 输出特征图个数</span>    num_outs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>和 RetinaNet 的区别是 ResNet 输出的 4 个特征图都会被利用。其详细流程是：</p><ul><li>将c2 c3 c4 c5 4 个特征图全部经过各自 1x1 卷积进行通道变换变成 m2~m5，输出通道统一为 256</li><li>从 m5 开始，先进行 2 倍最近邻上采样，然后和 m4 进行 add 操作，得到新的 m4</li><li>将新 m4 进行 2 倍最近邻上采样，然后和 m3 进行 add 操作，得到新的 m3</li><li>将新 m3 进行 2 倍最近邻上采样，然后和 m2 进行 add 操作，得到新的 m2</li><li>对 m5 和新的融合后的 m4 ~ m2，都进行各自的 3x3 卷积，得到 4 个尺度的最终输出 p5 ~ p2</li><li>将 c5 进行 3x3 且 stride=2 的卷积操作，得到 p6，目的是提供一个感受野非常大的特征图，有利于检测超大物体</li></ul><p>故 FPN 模块实现了c2 ~ c5 4 个特征图输入，p2 ~ p6 5个特征图输出，其 strides = (4,8,16,32,64)。</p><h3 id="2-3-RPN-Head"><a href="#2-3-RPN-Head" class="headerlink" title="2.3 RPN Head"></a>2.3 RPN Head</h3><p>其完整配置如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">rpn_head<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'RPNHead'</span><span class="token punctuation">,</span>    <span class="token comment"># FPN 层输出特征图通道数</span>    in_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>    <span class="token comment"># 中间特征图通道数</span>    feat_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>    <span class="token comment"># 后面分析</span>    anchor_generator<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'AnchorGenerator'</span><span class="token punctuation">,</span>        scales<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        ratios<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 后面分析</span>    bbox_coder<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'DeltaXYWHBBoxCoder'</span><span class="token punctuation">,</span>        target_means<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        target_stds<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 后面分析</span>    loss_cls<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'CrossEntropyLoss'</span><span class="token punctuation">,</span> use_sigmoid<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    loss_bbox<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'L1Loss'</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>相比 RetinaNet，RPN Head 网络比较简单，就一个卷积进行特征通道变换，加上两个输出分支即可，如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_init_layers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Initialize layers of the head."""</span>    <span class="token comment"># 特征通道变换</span>    self<span class="token punctuation">.</span>rpn_conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>        self<span class="token punctuation">.</span>in_channels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>feat_channels<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment"># 分类分支，类别固定是2，表示前后景分类</span>    <span class="token comment"># 并且由于 cls loss 是 bce，故实际上 self.cls_out_channels=1</span>    self<span class="token punctuation">.</span>rpn_cls <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>self<span class="token punctuation">.</span>feat_channels<span class="token punctuation">,</span>                             self<span class="token punctuation">.</span>num_anchors <span class="token operator">*</span> self<span class="token punctuation">.</span>cls_out_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment"># 回归分支，固定输出4个数值，表示基于 anchor 的变换值</span>    self<span class="token punctuation">.</span>rpn_reg <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>self<span class="token punctuation">.</span>feat_channels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_anchors <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>5 个 RPN Head 共享所有分类或者回归分支的卷积权重，经过 Head 模块的前向流程输出一共是 5*2 个特征图。</p><h3 id="2-4-BBox-Assigner"><a href="#2-4-BBox-Assigner" class="headerlink" title="2.4 BBox Assigner"></a>2.4 BBox Assigner</h3><p>RPN 这部分设计和 RetinaNet 原理完全相同，差别只在参数而已。</p><p><strong>(1) AnchorGenerator</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">anchor_generator<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'AnchorGenerator'</span><span class="token punctuation">,</span>    <span class="token comment"># 相当于 octave_base_scale，表示每个特征图的 base scales</span>    scales<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token comment"># 每个特征图有 3 个高宽比例</span>    ratios<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token comment"># 特征图对应的 stride，必须和特征图 stride 一致，不可以随意更改</span>    strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>相比不包括 FPN 的 Faster R-CNN 算法，由于其 RPN Head 是多尺度特征图，为了适应这种变化，anchor 设置进行了适当修改，FPN 输出的多尺度信息可以帮助区分不同大小物体识别问题，每一层就不再需要不包括 FPN 的 Faster R-CNN 算法那么多 anchor 了。</p><p>可以看出一共 5 个输出层，每个输出层包括 3 个高宽比例和 1 种尺度，也就是说每一层的每个特征图坐标处都包括 3 个 anchor，一共 15 个 anchor，相比 RetinaNet 少了很多，其具体实现看 RetinaNet 算法解读文章。</p><p><strong>(2) BBox Assigner</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">assigner<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># 最大 IoU 原则分配器</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'MaxIoUAssigner'</span><span class="token punctuation">,</span>    <span class="token comment"># 正样本阈值</span>    pos_iou_thr<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">,</span>    <span class="token comment"># 负样本阈值</span>    neg_iou_thr<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span>    <span class="token comment"># 正样本阈值下限</span>    min_pos_iou<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span>    <span class="token comment"># 忽略 bboxes 的阈值，-1 表示不忽略</span>    ignore_iof_thr<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>分配准则和 RetinaNet 完全相同，除了参数不同以外。简要总结为：</p><ul><li>如果 anchor 和所有 gt bbox 的最大 iou 值小于 0.3，那么该 anchor 就是背景样本</li><li>如果 anchor 和所有 gt bbox 的最大 iou 值大于等于 0.7，那么该 anchor 就是高质量正样本，该阈值比较高，这个阈值设置需要和后续的 R-CNN 模块匹配</li><li>如果 gt bbox 和所有 anchor 的最大 iou 值大于等于 0.3(可以看出可能有某些 gt bbox 没有和任意 anchor 匹配)，那么该 gt bbox 所对应的 anchor 也是正样本</li><li>其余样本全部为忽略样本，但是由于 <code>neg_iou_thr</code> 和 <code>min_pos_iou</code> 相等，故不存在忽略样本</li></ul><h3 id="2-5-BBox-Sampler"><a href="#2-5-BBox-Sampler" class="headerlink" title="2.5 BBox Sampler"></a>2.5 BBox Sampler</h3><p>和 RetinaNet 采用 Focal Loss 处理正负样本不平衡不同，Faster R-CNN 是通过正负样本采样模块来克服。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">sampler<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># 随机采样</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'RandomSampler'</span><span class="token punctuation">,</span>    <span class="token comment"># 采样后每张图片的训练样本总数，不包括忽略样本</span>    num<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>    <span class="token comment"># 正样本比例</span>    pos_fraction<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>    <span class="token comment"># 正负样本比例，用于确定负样本采样个数上界</span>    neg_pos_ub<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>    <span class="token comment"># 是否加入 gt 作为 proposals 以增加高质量正样本数</span>    add_gt_as_proposals<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>核心参数的具体含义是：</p><ul><li>num = 256 表示采样后每张图片的样本总数，<code>pos_fraction</code> 表示其中的正样本比例，具体是正样本采样 128 个，那么理论上负样本采样也是 128 个</li><li><code>neg_pos_ub</code> 表示负和正样本比例上限，用于确定负样本采样个数上界，例如打算采样 1000 个样本，正样本打算采样 500 个，但是可能正样本才 200 个，那么正样本实际上只能采样 200 个，如果设置 <code>neg_pos_ub=-1</code> 那么就会对负样本采样 800 个，用于凑足 1000 个，但是如果设置了 <code>neg_pos_ub</code> 比例，例如 1.5，那么负样本最多采样 200x1.5=300 个，最终返回的样本实际上不够 1000 个，默认情况 <code>neg_pos_ub=-1</code></li><li><code>add_gt_as_proposals=True</code> 是防止高质量正样本太少而加入的，可以保证前期收敛更快、更稳定，属于训练技巧，在 RPN 模块设置为 False，主要用于 R-CNN，因为前期 RPN 提供的正样本不够，可能会导致训练不稳定或者前期收敛慢的问题。</li></ul><p>其实现过程比较简单，如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> self<span class="token punctuation">.</span>add_gt_as_proposals <span class="token keyword">and</span> <span class="token builtin">len</span><span class="token punctuation">(</span>gt_bboxes<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>    <span class="token comment"># 增加 gt 作为 proposals</span>    bboxes <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>gt_bboxes<span class="token punctuation">,</span> bboxes<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>    assign_result<span class="token punctuation">.</span>add_gt_<span class="token punctuation">(</span>gt_labels<span class="token punctuation">)</span><span class="token comment"># 计算正样本个数</span>num_expected_pos <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num <span class="token operator">*</span> self<span class="token punctuation">.</span>pos_fraction<span class="token punctuation">)</span><span class="token comment"># 正样本随机采样</span>pos_inds <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_sampler<span class="token punctuation">.</span>_sample_pos<span class="token punctuation">(</span>    assign_result<span class="token punctuation">,</span> num_expected_pos<span class="token punctuation">,</span> bboxes<span class="token operator">=</span>bboxes<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token comment"># 去重</span>pos_inds <span class="token operator">=</span> pos_inds<span class="token punctuation">.</span>unique<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 计算负样本数</span>num_sampled_pos <span class="token operator">=</span> pos_inds<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span>num_expected_neg <span class="token operator">=</span> self<span class="token punctuation">.</span>num <span class="token operator">-</span> num_sampled_pos<span class="token keyword">if</span> self<span class="token punctuation">.</span>neg_pos_ub <span class="token operator">&gt;=</span> <span class="token number">0</span><span class="token punctuation">:</span>   <span class="token comment"># 计算负样本个数上限</span>    _pos <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_sampled_pos<span class="token punctuation">)</span>    neg_upper_bound <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>neg_pos_ub <span class="token operator">*</span> _pos<span class="token punctuation">)</span>    <span class="token keyword">if</span> num_expected_neg <span class="token operator">&gt;</span> neg_upper_bound<span class="token punctuation">:</span>        num_expected_neg <span class="token operator">=</span> neg_upper_bound<span class="token comment"># 负样本随机采样</span>neg_inds <span class="token operator">=</span> self<span class="token punctuation">.</span>neg_sampler<span class="token punctuation">.</span>_sample_neg<span class="token punctuation">(</span>    assign_result<span class="token punctuation">,</span> num_expected_neg<span class="token punctuation">,</span> bboxes<span class="token operator">=</span>bboxes<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token comment"># 去重  </span>neg_inds <span class="token operator">=</span> neg_inds<span class="token punctuation">.</span>unique<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>而具体的随机采样函数如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 随机采样正样本</span><span class="token keyword">def</span> <span class="token function">_sample_pos</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> assign_result<span class="token punctuation">,</span> num_expected<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Randomly sample some positive samples."""</span>    pos_inds <span class="token operator">=</span> torch<span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span>assign_result<span class="token punctuation">.</span>gt_inds <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">,</span> as_tuple<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> pos_inds<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>        pos_inds <span class="token operator">=</span> pos_inds<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> pos_inds<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;=</span> num_expected<span class="token punctuation">:</span>        <span class="token keyword">return</span> pos_inds    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>random_choice<span class="token punctuation">(</span>pos_inds<span class="token punctuation">,</span> num_expected<span class="token punctuation">)</span><span class="token comment"># 随机采样负样本</span><span class="token keyword">def</span> <span class="token function">_sample_neg</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> assign_result<span class="token punctuation">,</span> num_expected<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Randomly sample some negative samples."""</span>    neg_inds <span class="token operator">=</span> torch<span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span>assign_result<span class="token punctuation">.</span>gt_inds <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> as_tuple<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> neg_inds<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>        neg_inds <span class="token operator">=</span> neg_inds<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>neg_inds<span class="token punctuation">)</span> <span class="token operator">&lt;=</span> num_expected<span class="token punctuation">:</span>        <span class="token keyword">return</span> neg_inds    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>random_choice<span class="token punctuation">(</span>neg_inds<span class="token punctuation">,</span> num_expected<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>经过随机采样函数后，可以有效控制 RPN 网络计算 loss 时正负样本平衡问题。</p><h3 id="2-6-BBox-Encoder-Decoder"><a href="#2-6-BBox-Encoder-Decoder" class="headerlink" title="2.6 BBox Encoder Decoder"></a>2.6 BBox Encoder Decoder</h3><p>本部分参数和实现方式和 RetinaNet 中介绍的完全相同，请参照相关解读。</p><h3 id="2-7-Loss"><a href="#2-7-Loss" class="headerlink" title="2.7 Loss"></a>2.7 Loss</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">loss_cls<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'CrossEntropyLoss'</span><span class="token punctuation">,</span> use_sigmoid<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>loss_bbox<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'L1Loss'</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>RPN 采用的 loss 是常用的 bce loss 和 l1 loss，不需要详细描述。</p><h3 id="2-8-RPN-Test"><a href="#2-8-RPN-Test" class="headerlink" title="2.8 RPN Test"></a>2.8 RPN Test</h3><p>到目前为止， RPN 的整个训练流程就分析完后，但是实际上 RPN 是作为一个 RoI 提取模块，真正核心的是 R-CNN 部分，为了实现联合训练，RPN 不仅仅要自己进行训练，还要同时输出 RoI，然后利用这些 RoI 去 FPN 输出的特征图上进行截取，最后输入给 R-CNN 进行分类和回归。一个核心的问题是如何得到这些 RoI，实际上是调用了 RPN 的 test 过程。由于 RPN 也是和 RetinaNet 一样的 one-stage 算法，其大概过程和 RetinaNet 基本一致：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">rpn<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># 是否跨层进行 NMS 操作</span>    nms_across_levels<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>    <span class="token comment"># nms 前每个输出层最多保留 1000 个预测框</span>    nms_pre<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span>    <span class="token comment"># nms 后每张图片最多保留 1000 个预测框</span>    nms_post<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span>    <span class="token comment"># 每张图片最终输出检测结果最多保留 1000 个，RPN 层没有使用这个参数</span>    max_num<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span>    <span class="token comment"># nms 阈值</span>    nms_thr<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">,</span>    <span class="token comment"># 过滤掉的最小 bbox 尺寸</span>    min_bbox_size<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>1.对 batch 输入图片经过 Backbone+FPN+RPN Head 后输出 5 个特征图，每个图包括两个分支 <code>rpn_cls_score</code>，<code>rpn_bbox_pred</code>，首先遍历每张图，然后遍历每张图片中的每个输出层进行后续处理</p><p>2.对每层的分类 <code>rpn_cls_score</code> 进行 sigmoid 操作得到概率值</p><p>3.按照分类预测分值排序，保留前 <code>nms_pre</code> 个预测结果</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> cfg<span class="token punctuation">.</span>nms_pre <span class="token operator">&gt;</span> <span class="token number">0</span> <span class="token keyword">and</span> scores<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">&gt;</span> cfg<span class="token punctuation">.</span>nms_pre<span class="token punctuation">:</span>    <span class="token comment"># sort is faster than topk</span>    <span class="token comment"># _, topk_inds = scores.topk(cfg.nms_pre)</span>    ranked_scores<span class="token punctuation">,</span> rank_inds <span class="token operator">=</span> scores<span class="token punctuation">.</span>sort<span class="token punctuation">(</span>descending<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    topk_inds <span class="token operator">=</span> rank_inds<span class="token punctuation">[</span><span class="token punctuation">:</span>cfg<span class="token punctuation">.</span>nms_pre<span class="token punctuation">]</span>    scores <span class="token operator">=</span> ranked_scores<span class="token punctuation">[</span><span class="token punctuation">:</span>cfg<span class="token punctuation">.</span>nms_pre<span class="token punctuation">]</span>    rpn_bbox_pred <span class="token operator">=</span> rpn_bbox_pred<span class="token punctuation">[</span>topk_inds<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>    anchors <span class="token operator">=</span> anchors<span class="token punctuation">[</span>topk_inds<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>4.对每张图片的 5 个输出层都运行 2 ~ 3 步骤，将预测结果全部收集，然后进行解码</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>mlvl_scores<span class="token punctuation">)</span>anchors <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>mlvl_valid_anchors<span class="token punctuation">)</span>rpn_bbox_pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>mlvl_bbox_preds<span class="token punctuation">)</span>proposals <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_coder<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>    anchors<span class="token punctuation">,</span> rpn_bbox_pred<span class="token punctuation">,</span> max_shape<span class="token operator">=</span>img_shape<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>5.进行统一的 NMS 操作，每张图片最终保留 <code>cfg.nms_post</code> 个预测框</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">nms_cfg <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'nms'</span><span class="token punctuation">,</span> iou_threshold<span class="token operator">=</span>cfg<span class="token punctuation">.</span>nms_thr<span class="token punctuation">)</span>dets<span class="token punctuation">,</span> keep <span class="token operator">=</span> batched_nms<span class="token punctuation">(</span>proposals<span class="token punctuation">,</span> scores<span class="token punctuation">,</span> ids<span class="token punctuation">,</span> nms_cfg<span class="token punctuation">)</span><span class="token keyword">return</span> dets<span class="token punctuation">[</span><span class="token punctuation">:</span>cfg<span class="token punctuation">.</span>nms_post<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>经过 RPN test 计算后每张图片可以提供最多 <code>nms_post</code> 个候选框，一般该值为 2000。</p><h3 id="2-9-RoI-Head"><a href="#2-9-RoI-Head" class="headerlink" title="2.9 RoI Head"></a>2.9 RoI Head</h3><p><strong>R-CNN 模块接收 RPN 输出的每张图片共 <code>nms_post</code> 个候选框，然后对这些候选框进一步 refine，输出包括区分具体类别和 bbox 回归</strong>。该模块网络构建方面虽然简单，但是也包括了 RPN 中涉及到的组件，例如 BBox Assigner、BBox Sampler、BBox Encoder Decoder、Loss 等等，除此之外，还包括一个额外的 RPN 到 R-CNN 数据转换模块：RoIAlign 或者 RoIPool, 下面详细描述。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">roi_head<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># 一次 refine head，另外对应的是级联结构</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'StandardRoIHead'</span><span class="token punctuation">,</span>    bbox_roi_extractor<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'SingleRoIExtractor'</span><span class="token punctuation">,</span>        roi_layer<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'RoIAlign'</span><span class="token punctuation">,</span> output_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> sampling_ratio<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        out_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>        featmap_strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    bbox_head<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token comment"># 2 个共享 FC 模块</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'Shared2FCBBoxHead'</span><span class="token punctuation">,</span>        <span class="token comment"># 输入通道数，相等于 FPN 输出通道</span>        in_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>        <span class="token comment"># 中间 FC 层节点个数</span>        fc_out_channels<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span>        <span class="token comment"># RoIAlign 或 RoIPool 输出的特征图大小</span>        roi_feat_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span>        <span class="token comment"># 类别个数</span>        num_classes<span class="token operator">=</span><span class="token number">80</span><span class="token punctuation">,</span>        <span class="token comment"># bbox 编解码策略，除了参数外和 RPN 相同，</span>        bbox_coder<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>            <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'DeltaXYWHBBoxCoder'</span><span class="token punctuation">,</span>            target_means<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            target_stds<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token comment"># 影响 bbox 分支的通道数，True 表示 4 通道输出，False 表示 4×num_classes 通道输出</span>        reg_class_agnostic<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>        <span class="token comment"># CE Loss</span>        loss_cls<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>            <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'CrossEntropyLoss'</span><span class="token punctuation">,</span> use_sigmoid<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token comment"># L1 Loss</span>        loss_bbox<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'L1Loss'</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>从配置可以看出，和 RPN 相比，除了额外的 SingleRoIExtractor 外，基本都是相同的。其训练和测试流程简要概况如下：</p><p><strong>(1) 公共部分</strong></p><ol><li>RPN 层输出每张图片最多 <code>nms_post</code> 个候选框，故 R-CNN 输入 shape 为 <code>(batch, nms_post, 4)</code>，4 表示 RoI 坐标</li><li>利用 RoI 重映射规则，将 <code>nms_post</code> 个候选框映射到 FPN 输出的不同特征图上，提取对应的特征图，然后利用插值思想将其变成指定的固定大小输出，输出 shape 为 <code>(batch, nms_post, 256, roi_feat_size, roi_feat_size)</code>，其中 256 是 FPN 层输出特征图通道大小，<code>roi_feat_size</code> 一般取 7。上述步骤即为 RoIAlign 或者 RoIPool 计算过程</li><li>将 <code>(batch, nms_post, 256, roi_feat_size, roi_feat_size)</code> 数据拉伸为 <code>(batch*nms_post, 256*roi_feat_size*roi_feat_size)</code>，转化为 FC 可以支持的格式, 然后应用两次共享卷积，输出 shape 为 <code>(batch*nms_post, 1024)</code></li><li>将 <code>(batch*nms_post, 1024)</code> 分成分类和回归分支，分类分支输出 <code>(batch*nms_post, num_classes+1)</code>, 回归分支输出 <code>(batch*nms_post, 4*num_class)</code></li></ol><p>第二步的映射规则是在 FPN 论文中提出。不知大家是否有疑问：假设某个 proposal 是由第 4 个 特征图层检测出来的，为啥该 proposal 不是直接去对应特征图层切割就行，还需要重新映射？原因是这些 proposal 是 RPN 测试阶段检测出来的，大部分 proposal 可能符合前面设定，但是也有很多不符合的，也就是说测试阶段上述一致性不一定满足，需要重新映射，公式如下：</p><p>$$k=\lfloor k_0 + log_2(\sqrt{wh}/224)\rfloor$$</p><p>上述公式中 k_0=4，通过公式可以算出 pk，具体是：</p><ul><li>wh&gt;=448x448，则分配给 p5</li><li>wh&lt;448x448 并且 wh&gt;=224x224，则分配给 p4</li><li>wh&lt;224x224 并且 wh&gt;=112x112，则分配给 p3</li><li>其余分配给 p2</li></ul><p>在 R-CNN 部分没有采用感受野最大的 p6 层。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">map_roi_levels</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> rois<span class="token punctuation">,</span> num_levels<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Map rois to corresponding feature levels by scales.    - scale &lt; finest_scale * 2: level 0    - finest_scale * 2 &lt;= scale &lt; finest_scale * 4: level 1    - finest_scale * 4 &lt;= scale &lt; finest_scale * 8: level 2     - scale &gt;= finest_scale * 8: level 3    """</span>    scale <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>        <span class="token punctuation">(</span>rois<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">-</span> rois<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>rois<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span> <span class="token operator">-</span> rois<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    target_lvls <span class="token operator">=</span> torch<span class="token punctuation">.</span>floor<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>log2<span class="token punctuation">(</span>scale <span class="token operator">/</span> self<span class="token punctuation">.</span>finest_scale <span class="token operator">+</span> <span class="token number">1e-6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    target_lvls <span class="token operator">=</span> target_lvls<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token builtin">min</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">max</span><span class="token operator">=</span>num_levels <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> target_lvls<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中 <code>finest_scale=56，num_level=5</code>。</p><p>在基于候选框提取出对应的特征图后，再利用 RoIAlign 或者 RoIPool 进行统一输出大小，其计算过程在 Mask R-CNN 部分分析。</p><p>经过 RoIAlign 或者 RoIPool 后，所有候选框特征图的 shape 为 <code>(batch, nms_post, 256, roi_feat_size, roi_feat_size)</code>，将其拉伸后输入到 R-CNN 的 Head 模块中，具体来说主要是包括两层分类和回归共享全连接层 FC，最后是各自的输出头，其 forward 逻辑如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> self<span class="token punctuation">.</span>num_shared_fcs <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>    x <span class="token operator">=</span> x<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment"># 两层共享 FC</span>    <span class="token keyword">for</span> fc <span class="token keyword">in</span> self<span class="token punctuation">.</span>shared_fcs<span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>x_cls <span class="token operator">=</span> xx_reg <span class="token operator">=</span> x<span class="token comment"># 不共享的分类和回归分支输出</span>cls_score <span class="token operator">=</span> self<span class="token punctuation">.</span>fc_cls<span class="token punctuation">(</span>x_cls<span class="token punctuation">)</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>with_cls <span class="token keyword">else</span> <span class="token boolean">None</span>bbox_pred <span class="token operator">=</span> self<span class="token punctuation">.</span>fc_reg<span class="token punctuation">(</span>x_reg<span class="token punctuation">)</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>with_reg <span class="token keyword">else</span> <span class="token boolean">None</span><span class="token keyword">return</span> cls_score<span class="token punctuation">,</span> bbox_pred<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最终输出分类和回归预测结果。相比于目前主流的全卷积模型，Faster R-CNN 的 R-CNN 模块依然采用的是全连接模式。</p><p><strong>(2) 训练逻辑</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">rcnn<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    assigner<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token comment"># 和 RPN 一样，正负样本定义参数不同</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'MaxIoUAssigner'</span><span class="token punctuation">,</span>        pos_iou_thr<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>        neg_iou_thr<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>        min_pos_iou<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>        match_low_quality<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>        ignore_iof_thr<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    sampler<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token comment"># 和 RPN 一样，随机采样参数不同</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'RandomSampler'</span><span class="token punctuation">,</span>        num<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>        pos_fraction<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">,</span>        neg_pos_ub<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>        <span class="token comment"># True，RPN 中为 False</span>        add_gt_as_proposals<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>理论上，BBox Assigner 和 BBox Sampler 逻辑可以放置在 <em>(1) 公共部分</em> 后面，因为其任务是输入每张图片的 <code>nms_post</code> 个候选框以及标注的 gt bbox 信息，然后计算每个候选框样本的正负样本属性，最后再进行随机采样尽量保证样本平衡。<strong>R-CNN的候选框对应了 RPN 阶段的 anchor，只不过 RPN 中的 anchor 是预设密集的，而 R-CNN 面对的 anchor 是动态稀疏的，RPN 阶段基于 anchor 进行分类回归对应于 R-CNN 阶段基于候选框进行分类回归，思想是完全一致的，故 Faster R-CNN 类算法叫做 two-stage，因此可以简化为 one-stage + RoI 区域特征提取 + one-stage。</strong></p><p>实际上为了方便理解，BBox Assigner 和 BBox Sampler 逻辑是在 <em>(1) 公共部分</em> 的步骤 1 后运行的。需要特别注意的是配置参数和 RPN 不同：</p><ul><li><code>match_low_quality=False</code>。为了避免出现低质量匹配情况(因为 two-stage 算法性能核心在于 R-CNN，RPN 主要保证高召回率，R-CNN 保证高精度)，R-CNN 阶段禁用了允许低质量匹配设置</li><li>3 个 <code>iou_thr</code> 设置都是 0.5，不存在忽略样本，这个参数在 Cascade R-CNN 论文中有详细说明，影响较大</li><li><code>add_gt_as_proposals=True</code>。主要是克服刚开始 R-CNN 训练不稳定情况</li></ul><p>R-CNN 整体训练逻辑如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> self<span class="token punctuation">.</span>with_bbox <span class="token keyword">or</span> self<span class="token punctuation">.</span>with_mask<span class="token punctuation">:</span>    num_imgs <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>img_metas<span class="token punctuation">)</span>    sampling_results <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token comment"># 遍历每张图片，单独计算 BBox Assigner 和 BBox Sampler </span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_imgs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># proposal_list 是 RPN test 输出的候选框</span>        assign_result <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_assigner<span class="token punctuation">.</span>assign<span class="token punctuation">(</span>            proposal_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> gt_bboxes<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> gt_bboxes_ignore<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>            gt_labels<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment"># 随机采样</span>        sampling_result <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_sampler<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>            assign_result<span class="token punctuation">,</span>            proposal_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>            gt_bboxes<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>            gt_labels<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>            feats<span class="token operator">=</span><span class="token punctuation">[</span>lvl_feat<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token keyword">for</span> lvl_feat <span class="token keyword">in</span> x<span class="token punctuation">]</span><span class="token punctuation">)</span>        sampling_results<span class="token punctuation">.</span>append<span class="token punctuation">(</span>sampling_result<span class="token punctuation">)</span><span class="token comment"># 特征重映射+ RoI 区域特征提取+ 网络 forward + Loss 计算</span>losses <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># bbox head forward and loss</span><span class="token keyword">if</span> self<span class="token punctuation">.</span>with_bbox<span class="token punctuation">:</span>    bbox_results <span class="token operator">=</span> self<span class="token punctuation">.</span>_bbox_forward_train<span class="token punctuation">(</span>x<span class="token punctuation">,</span> sampling_results<span class="token punctuation">,</span>                                            gt_bboxes<span class="token punctuation">,</span> gt_labels<span class="token punctuation">,</span>                                            img_metas<span class="token punctuation">)</span>    losses<span class="token punctuation">.</span>update<span class="token punctuation">(</span>bbox_results<span class="token punctuation">[</span><span class="token string">'loss_bbox'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># mask head forward and loss</span><span class="token keyword">if</span> self<span class="token punctuation">.</span>with_mask<span class="token punctuation">:</span>    mask_results <span class="token operator">=</span> self<span class="token punctuation">.</span>_mask_forward_train<span class="token punctuation">(</span>x<span class="token punctuation">,</span> sampling_results<span class="token punctuation">,</span>                                            bbox_results<span class="token punctuation">[</span><span class="token string">'bbox_feats'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                            gt_masks<span class="token punctuation">,</span> img_metas<span class="token punctuation">)</span>    losses<span class="token punctuation">.</span>update<span class="token punctuation">(</span>mask_results<span class="token punctuation">[</span><span class="token string">'loss_mask'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">return</span> losses<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>_bbox_forward_train</code> 逻辑和 RPN 非常类似，只不过多了额外的 RoI 区域特征提取步骤：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_bbox_forward_train</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> sampling_results<span class="token punctuation">,</span> gt_bboxes<span class="token punctuation">,</span> gt_labels<span class="token punctuation">,</span>                        img_metas<span class="token punctuation">)</span><span class="token punctuation">:</span>    rois <span class="token operator">=</span> bbox2roi<span class="token punctuation">(</span><span class="token punctuation">[</span>res<span class="token punctuation">.</span>bboxes <span class="token keyword">for</span> res <span class="token keyword">in</span> sampling_results<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment"># 特征重映射+ RoI 特征提取+ 网络 forward</span>    bbox_results <span class="token operator">=</span> self<span class="token punctuation">.</span>_bbox_forward<span class="token punctuation">(</span>x<span class="token punctuation">,</span> rois<span class="token punctuation">)</span>    <span class="token comment"># 计算每个样本对应的 target, bbox encoder 在内部进行</span>    bbox_targets <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">.</span>get_targets<span class="token punctuation">(</span>sampling_results<span class="token punctuation">,</span> gt_bboxes<span class="token punctuation">,</span>                                              gt_labels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>train_cfg<span class="token punctuation">)</span>    <span class="token comment"># 计算 loss</span>    loss_bbox <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>bbox_results<span class="token punctuation">[</span><span class="token string">'cls_score'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                    bbox_results<span class="token punctuation">[</span><span class="token string">'bbox_pred'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> rois<span class="token punctuation">,</span>                                    <span class="token operator">*</span>bbox_targets<span class="token punctuation">)</span>    bbox_results<span class="token punctuation">.</span>update<span class="token punctuation">(</span>loss_bbox<span class="token operator">=</span>loss_bbox<span class="token punctuation">)</span>    <span class="token keyword">return</span> bbox_results<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>_bbox_forward</code> 逻辑是 R-CNN 的重点：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_bbox_forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> rois<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 特征重映射+ RoI 区域特征提取，仅仅考虑前 num_inputs 个特征图</span>    bbox_feats <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_roi_extractor<span class="token punctuation">(</span>        x<span class="token punctuation">[</span><span class="token punctuation">:</span>self<span class="token punctuation">.</span>bbox_roi_extractor<span class="token punctuation">.</span>num_inputs<span class="token punctuation">]</span><span class="token punctuation">,</span> rois<span class="token punctuation">)</span>    <span class="token comment"># 共享模块</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>with_shared_head<span class="token punctuation">:</span>        bbox_feats <span class="token operator">=</span> self<span class="token punctuation">.</span>shared_head<span class="token punctuation">(</span>bbox_feats<span class="token punctuation">)</span>    <span class="token comment"># 独立分类和回归 head</span>    cls_score<span class="token punctuation">,</span> bbox_pred <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">(</span>bbox_feats<span class="token punctuation">)</span>    bbox_results <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>        cls_score<span class="token operator">=</span>cls_score<span class="token punctuation">,</span> bbox_pred<span class="token operator">=</span>bbox_pred<span class="token punctuation">,</span> bbox_feats<span class="token operator">=</span>bbox_feats<span class="token punctuation">)</span>    <span class="token keyword">return</span> bbox_results<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>(3) 测试逻辑</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">rcnn<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    score_thr<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">,</span>    nms<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'nms'</span><span class="token punctuation">,</span> iou_threshold<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    max_per_img<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>测试逻辑核心逻辑如下：</p><ul><li>公共逻辑部分输出 batch * nms_post 个候选框的分类和回归预测结果</li><li>将所有预测结果按照 batch 维度进行切分，然后依据单张图片进行后处理，后处理逻辑为：先解码并还原为原图尺度；然后利用 <code>score_thr</code> 去除低分值预测；然后进行 NMS；最后保留最多 <code>max_per_img</code> 个结果</li></ul><h2 id="3-Mask-R-CNN-代码详解"><a href="#3-Mask-R-CNN-代码详解" class="headerlink" title="3 Mask R-CNN 代码详解"></a>3 Mask R-CNN 代码详解</h2><p>Mask R-CNN 和 Faster R-CNN 的区别主要包括两个方面：</p><ul><li>R-CNN 中额外引入 Mask Head，从而可以实现实例分割任务</li><li>针对特征图与原始图像上的 RoI 不对准问题，提出了 RoIPool 的改进版本 RoIAlign</li></ul><h3 id="3-1-Mask-Head"><a href="#3-1-Mask-Head" class="headerlink" title="3.1 Mask Head"></a>3.1 Mask Head</h3><p><img src="https://pic2.zhimg.com/80/v2-5d406aac4497e77d37ae7c3fbfb35349_720w.jpg" alt="img"></p><p>上图为在 Faster R-CNN 和 FPN 基础上扩展得到 Mask R-CNN 模型，本文解读的是 FPN 扩展的 Mask R-CNN 模型，其简要描述为：</p><ul><li>bbox 分支和 mask 分支都有自己的 RoI 区域特征提取算子，其中由于 mask 任务要求更加精细的结果，所以 RoI 特征区域提取算子输出特征图比 bbox 分支大，一般设置为 14</li><li>bbox 分支是全连接结构，而 mask 分支是全卷积结构，输出 shape 为 <code>(batch* num_post,80,28,28)</code>，其中 80 表示类别，即输出的每个通道代表一个类别的 mask</li><li>由于 mask 分支目的是对前景物体进行分割，故该分支的输入仅仅包括 Bbox Assigner 加上 Bbox Sampler 后的正样本而已</li><li>在测试阶段，为了避免 bbox 和 mask 结果没有对齐，做法是<strong>先对 bbox 分支进行后处理，得到预测的 bbox，然后将 bbox 作为 proposal 输入到 mask 分支中进行 RoI 特征区域提取和 forward，输出和类别相关的 mask</strong>。为了得到二值分割图，还需要在 <code>test_cfg</code> 指定 <code>mask_thr_binary</code> 参数，一般设置为 0.5</li></ul><p>结合上面图示和配置就可以知道 Mask R-CNN 所有细节：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">roi_head<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># 和 Faster R-CNN 完全相同</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'StandardRoIHead'</span><span class="token punctuation">,</span>    bbox_roi_extractor<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'SingleRoIExtractor'</span><span class="token punctuation">,</span>        roi_layer<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'RoIAlign'</span><span class="token punctuation">,</span> output_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> sampling_ratio<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        out_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>        featmap_strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    bbox_head<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'Shared2FCBBoxHead'</span><span class="token punctuation">,</span>        in_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>        fc_out_channels<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span>        roi_feat_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span>        num_classes<span class="token operator">=</span><span class="token number">80</span><span class="token punctuation">,</span>        bbox_coder<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>            <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'DeltaXYWHBBoxCoder'</span><span class="token punctuation">,</span>            target_means<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            target_stds<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        reg_class_agnostic<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>        loss_cls<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>            <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'CrossEntropyLoss'</span><span class="token punctuation">,</span> use_sigmoid<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        loss_bbox<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'L1Loss'</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># mask 分支也有自己的 RoIAlign     </span>    mask_roi_extractor<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'SingleRoIExtractor'</span><span class="token punctuation">,</span>        <span class="token comment"># 除了 output_size 不同外，其他都相同</span>        roi_layer<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'RoIAlign'</span><span class="token punctuation">,</span> output_size<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">,</span> sampling_ratio<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        out_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>        featmap_strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 全卷积 head</span>    mask_head<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'FCNMaskHead'</span><span class="token punctuation">,</span>        <span class="token comment"># 总共 4 层卷积</span>        num_convs<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>        in_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>        <span class="token comment"># 中间卷积通道数</span>        conv_out_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>        <span class="token comment"># 输出类别</span>        num_classes<span class="token operator">=</span><span class="token number">80</span><span class="token punctuation">,</span>        <span class="token comment"># loss 采用 bce loss</span>        loss_mask<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>            <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'CrossEntropyLoss'</span><span class="token punctuation">,</span> use_mask<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-2-RoIAlign-和-RoIPool"><a href="#3-2-RoIAlign-和-RoIPool" class="headerlink" title="3.2 RoIAlign 和 RoIPool"></a>3.2 RoIAlign 和 RoIPool</h3><p>Mask R-CNN 中一个比较大的创新点就是提出了 RoIAlign 层，本小节先描述 RoIPool 层原理以及缺点，然后再分析 RoIAlign。</p><p><strong>(1) RoIPool</strong></p><p>首先需要明确其作用： <strong>将任意大小的特征图都池化为指定输出大小</strong>。示意图如下：</p><p><img src="https://pic1.zhimg.com/80/v2-f0b90b9532a88513dcf7cef347c10a00_720w.jpg" alt="img"></p><p>假设左上图为输入特征图，bbox 分支预测的坐标可能是浮点数，设置 RoIPool 输出 size 是 (2,2)</p><ul><li>首先将 bbox 预测值转化为整数，得到右上蓝框</li><li>将蓝框内 5x7 的特征图均匀切割为 2x2 的块，但是由于取整操作，实际上第一个块是 wh=(7//2,5//2)，第二个块是 wh=(7-7//2,5-5//2), 后面类似，从而得到左下图示</li><li>然后在每个小块内采用 MaxPool 提取最大值，从而得到右下角的 2x2 输出</li></ul><p>可以发现 RoIPool 存在两次取整操作，第一次是将 proposal 值变成整数，第二次是均匀切割时候。对于小物体的特征图而言，两次取整操作特征图会产生比较大的偏差，从而对 分割和定位精度有比较大的影响，在论文里作者把它总结为“不匹配问题”(mis-alignment)。</p><p><strong>(2) RoIAlign</strong> 为了解决这个问题，RoIAlign 取消两次整数化操作，保留了小数，每个小数位置都采用双线性插值方法获得坐标为浮点数的特征图上数值, 其可视化如下所示：</p><p><img src="https://pic2.zhimg.com/80/v2-fa489fcd45ed2027402ddb92505d3b19_720w.jpg" alt="img"></p><p>其对应的论文图示如下：</p><p><img src="https://pic3.zhimg.com/80/v2-8d5013acfb3e16270b29ef82ebf15e96_720w.jpg" alt="img"></p><p>假设黑色大框是要切割的 bbox，打算输出 size 为 2x2 输出，则先把黑色大 bbox 均匀切割为 4 个小 bbox，然后在每个小 bbox 内部均匀采样 4 个点(相当于每个小 bbox 内部再次均匀切割为 2x2 共 4 个小块，取每个小块的中心点即可)，首先对每个采样点利用双线性插值函数得到该浮点值处的值(插值的 4 个整数点是上下左右最近的 4 个点)，然后对 4 个采样点采样值取 max 操作得到该小 bbox 的最终值。采样个数 4 是超参，实验发现设置为 4 的时候速度和精度都是最合适的。</p><p>关于 RoIAlign 和 RoIPool 的源码实现，由于是采用 CUDA 编程，在后续系列中统一进行分析。</p><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h2><p>本文重点分析了主流的带有 FPN 模块的 Faster R-CNN 算法，对每个组件都进行了详细分析，在此基础上，对其扩展版本 Mask R-CNN 进行了详细描述。通过本系列第一篇 RetinaNet 和 本文第二篇 Faster R-CNN 和 Mask R-CNN的解读，希望大家可以了解：</p><ul><li><p>掌握 MMDetection 中涉及到的常用配置参数含义</p></li><li><p>掌握 MMDetection 框架中两个经典算法 RetinaNet 和 Faster R-CNN</p></li><li><p>对 one-stage 和 two-stage 算法有清晰的理解，例如能够理解 two-stage 实际上可以认为是 one-stage + RoI 区域特征提取 + one-stage，对于 Cascade R-CNN 这类算法可以认为是 one-stage + RoI 区域特征提取 + one-stage + RoI 区域特征提取 + one-stage</p></li><li><p>RetinaNet 和 Faster R-CNN 解读完后，大家阅读 MMDetection 中大部分算法都会相对容易2.2 MMDetection<br>  使用Pytorch构建一个新算法时，通常包含如下几步：</p><p>注册数据集：CustomDataset是MMDetection在原始的Dataset基础上的再次封装，其__getitem__()方法会根据训练和测试模式分别重定向到prepare_train_img()和prepare_test_img()函数。用户以继承CustomDataset类的方式构建自己的数据集时，需要重写load_annotations()和get_ann_info()函数，定义数据和标签的加载及遍历方式。完成数据集类的定义后，还需要使用DATASETS.register_module()进行模块注册。<br>注册模型：模型构建的方式和Pytorch类似，都是新建一个Module的子类然后重写forward()函数。唯一的区别在于MMDetection中需要继承BaseModule而不是Module，BaseModule是Module的子类，MMLab中的任何模型都必须继承此类。另外，MMDetection将一个完整的模型拆分为backbone、neck和head三部分进行管理，所以用户需要按照这种方式，将算法模型拆解成3个类，分别使用BACKBONES.register_module()、NECKS.register_module()和HEADS.register_module()完成模块注册。<br>构建配置文件：配置文件用于配置算法各个组件的运行参数，大体上可以包含四个部分：datasets、models、schedules和runtime。完成相应模块的定义和注册后，在配置文件中配置好相应的运行参数，然后MMDetection就会通过Registry类读取并解析配置文件，完成模块的实例化。另外，配置文件可以通过_base_字段实现继承功能，以提高代码复用率。<br>训练和验证：在完成各模块的代码实现、模块的注册、配置文件的编写后，就可以使用./tools/train.py和./tools/test.py对模型进行训练和验证，不需要用户编写额外的代码。<br>————————————————<br>版权声明：本文为CSDN博主「Maples丶丶」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/qq_16137569/article/details/121316235">https://blog.csdn.net/qq_16137569/article/details/121316235</a></p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>轻松掌握 MMDetection 中常用算法(一)：RetinaNet 及配置详解</title>
      <link href="/2022/04/05/mmdetection-xue-xi-bi-ji-3-gou-jian-retinanet-ji-pei-zhi/"/>
      <url>/2022/04/05/mmdetection-xue-xi-bi-ji-3-gou-jian-retinanet-ji-pei-zhi/</url>
      
        <content type="html"><![CDATA[<h1 id="轻松掌握-MMDetection-中常用算法-一-：RetinaNet-及配置详解"><a href="#轻松掌握-MMDetection-中常用算法-一-：RetinaNet-及配置详解" class="headerlink" title="轻松掌握 MMDetection 中常用算法(一)：RetinaNet 及配置详解"></a>轻松掌握 MMDetection 中常用算法(一)：RetinaNet 及配置详解</h1><h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0 前言"></a>0 前言</h2><p>在解读完<a href="https://zhuanlan.zhihu.com/p/337375549">轻松掌握 MMDetection 训练和测试流程</a>的相关系列文章后，相信大家对 MMDetection 框架训练和测试流程以及各个组件的内部抽象实现有了一定了解。本系列文章则从框架中已经实现的一些常用算法入手，通过对这些算法进行深度解析，使读者能够对 MMDetection 有进一步深入理解。本系列文章希望达到的目的是：</p><ul><li>通过<strong>对常用算法进行深度解析</strong>，使读者能够对该系列算法及其改进算法的实现有非常透彻的理解</li><li>通过对算法相<strong>关的配置文件解读和扩展</strong>，使读者能够更加熟练的自定义配置</li><li>通过对常用算法进行解读，使读者能够更加便捷快速的<strong>使用 MMDetection</strong></li></ul><p>作为开篇系列，第一个解读算法是 RetinaNet。或许大家会有所疑问：为什么第一个解读算法不是经典的 Faster R-CNN? 这是由于考虑到 Faster R-CNN 包括两个阶段，复杂度比较高，并且第一阶段可以认为是 one-stage 检测器。为了<strong>降低理解 Faster R-CNN 难度</strong>，我们<strong>先分析经典的 one-stage 算法 RetinaNet</strong>。在理解该算法基础上再去理解 Faster R-CNN，应该会更加容易，思路也会更加清晰。</p><p>本系列文章的重点是解读 MMDetection 中相关算法实现，对于其原理描述的比较简单，并且一些配置和参数都是以 MMDetection 默认参数为准，某些参数和设置不完全与论文相同。</p><p>在阅读本文前，请先阅读<a href="https://zhuanlan.zhihu.com/p/337375549">前置系列文章</a>。MMDetection 依然在快速发展，本文解读的版本是 V2.8。</p><p><strong>需要特别注意的是：</strong>由于本文是系列文章开篇，所涉及的内容不仅仅是 RetinaNet，还<strong>包括了配置文件里面每个参数的详细解读</strong>(这个非常关键)，在后续文章中如果出现重复配置就不再描述，故<strong>不管你对 RetinaNet 有多了解，如果你想进一步熟悉 MMDetection 参数配置及其含义，那么本文可能对你有帮助</strong>。</p><h2 id="1-RetinaNet-简要介绍"><a href="#1-RetinaNet-简要介绍" class="headerlink" title="1 RetinaNet 简要介绍"></a>1 RetinaNet 简要介绍</h2><p><strong>RetinaNet 来自 FAIR 论文：Focal Loss for Dense Object Detection</strong>，其简要概述为：深入分析了<strong>极度不平衡的正负（前景背景）样本比例</strong>导致 one-stage 检测器精度低于 two-stage 检测器，基于上述分析，提出了一种简单但是非常实用的 <strong>Focal Loss 焦点损失函数</strong>，并且 Loss 设计思想可以推广到其他领域，同时针对目标检测领域特定问题，设计了 <strong>RetinaNet 网络</strong>，结合 Focal Loss 使得 one-stage 检测器在精度上能够达到乃至超过 two-stage 检测器。</p><p>其简要网络结构图如下所示：</p><p><img src="https://pic3.zhimg.com/80/v2-35848a98ce3803dde4d7c533edbb1972_720w.jpg" alt="img"></p><p>总的来说，RetinaNet 有<strong>两个大创新</strong>：</p><ul><li>Focal Loss</li><li>RetinaNet 网络</li></ul><p><strong>Focal Loss 几乎已经成为 one-stage 算法的标配</strong>，而 RetinaNet 网络结构也是目前<strong>主流的目标检测网络结构</strong>，其变体不计其数。</p><h2 id="2-RetinaNet-代码详解"><a href="#2-RetinaNet-代码详解" class="headerlink" title="2 RetinaNet 代码详解"></a>2 RetinaNet 代码详解</h2><p>在<a href="https://zhuanlan.zhihu.com/p/337375549">轻松掌握 MMDetection 整体构建流程(一)</a>一文中分析了 MMDetection 中模型构建的基本组件，RetinaNet 涉及的组件包括： Backbone、Neck、Head、BBox Assigner、BBox Encoder Decoder、Loss 和 BBox PostProcess。下面按照顺序结合代码详细分析。</p><h3 id="2-1-Backbone"><a href="#2-1-Backbone" class="headerlink" title="2.1 Backbone"></a>2.1 Backbone</h3><p><img src="https://pic4.zhimg.com/80/v2-7face90299b486de63263311f200c58f_720w.jpg" alt="img"></p><p>标准的 RetinaNet 骨架网络<strong>采用的是 ResNet 系列</strong>。由于骨架本身没有限制，MMDetection 中目前提供的预训练权重所涉及的骨架网络包括：ResNet50-Caffe、ResNet50-Pytorch、ResNet101-Caffe、ResNet101-Pytorch、ResNeXt101，非常丰富。</p><p>为了读者好理解，先解释<strong>下配置文件名含义</strong>：</p><ul><li>retinanet 表示算法名称</li><li>r50 等表示骨架网络名</li><li>caffe 和 PyTorch 是指 Bottleneck 模块的区别，省略情况下表示是 PyTorch，后面会详细说明</li><li>fpn 表示 Neck 模块采用了 FPN 结构</li><li>mstrain 表示多尺度训练，一般对应的是 pipeline 中 <code>Resize</code> 类</li><li>1x 表示 1 倍数的 epoch 训练即 12 个 epoch，2x 则表示 24 个 epcoh</li><li>coco 表示在 COCO 数据集上训练</li></ul><p><strong>以 ResNet50 为例进行具体分析，骨架网络配置如下：</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 使用 pytorch 提供的在 imagenet 上面训练过的权重作为预训练权重</span>pretrained<span class="token operator">=</span><span class="token string">'torchvision://resnet50'</span><span class="token punctuation">,</span>backbone<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># 骨架网络类名</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'ResNet'</span><span class="token punctuation">,</span>    <span class="token comment"># 表示使用 ResNet50</span>    depth<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span>    <span class="token comment"># ResNet 系列包括 stem+ 4个 stage 输出</span>    num_stages<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    <span class="token comment"># 表示本模块输出的特征图索引，(0, 1, 2, 3),表示4个 stage 输出都需要，</span>    <span class="token comment"># 其 stride 为 (4,8,16,32)，channel 为 (256, 512, 1024, 2048)</span>    out_indices<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 表示固定 stem 加上第一个 stage 的权重，不进行训练</span>    frozen_stages<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>    <span class="token comment"># 所有的 BN 层的可学习参数都不需要梯度，也就不会进行参数更新</span>    norm_cfg<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'BN'</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># backbone 所有的 BN 层的均值和方差都直接采用全局预训练值，不进行更新</span>    norm_eval<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token comment"># 默认采用 pytorch 模式</span>    style<span class="token operator">=</span><span class="token string">'pytorch'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看出，RetinaNet 算法采用了 ResNet50 作为 Backbone, 并且考虑到整个目标检测网络比较大，<strong>前面部分网络没有进行训练，BN 也不会进行参数更新</strong>。需要说明的是上述默认配置是经过前人工作和 OpenMMLab 在 COCO 数据集上<strong>不断实践的结果</strong>。推荐大家直接使用该配置模式，效果相对比较稳定。</p><h4 id="1-out-indices"><a href="#1-out-indices" class="headerlink" title="(1) out_indices"></a><strong>(1) out_indices</strong></h4><p><strong>ResNet 提出了骨架网络设计范式即 <code>stem+n stage+ cls head</code><strong>，对于 ResNet 而言，其实际 forward 流程是 <code>stem -&gt; 4 个 stage -&gt; 分类 head</code>，stem 的输出 stride 是 4，而 4 个 stage 的输出 stride 是 4,8,16,32，</strong>这 4 个输出就对应 <code>out_indices</code> 索引</strong>。例如如果你想要输出 stride=4 的特征图，那么你可以设置 <code>out_indices=(0,)</code>，如果你想要输出 stride=4 和 8 的特征图，那么你可以设置 <code>out_indices=(0, 1)</code>。</p><p>因为 RetinaNet 后面需要接 FPN，故需要输出 4 个尺度特征图，简要代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> i<span class="token punctuation">,</span> layer_name <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>res_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>    res_layer <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> layer_name<span class="token punctuation">)</span>    x <span class="token operator">=</span> res_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token comment"># 如果 i 在 self.out_indices 中才保留</span>    <span class="token keyword">if</span> i <span class="token keyword">in</span> self<span class="token punctuation">.</span>out_indices<span class="token punctuation">:</span>        outs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-frozen-stages"><a href="#2-frozen-stages" class="headerlink" title="(2) frozen_stages"></a><strong>(2) frozen_stages</strong></h4><p>该参数表示你想冻结前几个 stages 的权重，ResNet 结构包括 stem+4 stage</p><ul><li>frozen_stages=-1，表示全部可学习</li><li>frozen_stage=0，表示stem权重固定</li><li>frozen_stages=1，表示 stem 和第一个 stage 权重固定</li><li>frozen_stages=2，表示 stem 和前两个 stage 权重固定</li></ul><p>依次类推，具体代码为：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 固定权重，需要两个步骤：1. 设置 eval 模式；2. requires_grad=False</span><span class="token keyword">def</span> <span class="token function">_freeze_stages</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>frozen_stages <span class="token operator">&gt;=</span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token comment"># 固定 stem 权重</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>deep_stem<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>stem<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">for</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>stem<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>norm1<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">for</span> m <span class="token keyword">in</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">,</span> self<span class="token punctuation">.</span>norm1<span class="token punctuation">]</span><span class="token punctuation">:</span>                <span class="token keyword">for</span> param <span class="token keyword">in</span> m<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                    param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>    <span class="token comment"># 固定 stage 权重</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>frozen_stages <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        m <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f'layer</span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>        m<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> param <span class="token keyword">in</span> m<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>需要特别注意的是</strong>：上述函数不能仅仅在类初始化时候调用，因为在训练模式下，运行时候会调用 <code>model.train()</code> 导致 BN 层又进入 train 模式，最终 BN 没有被固定，故需要在 ResNet 中重写 train 方法，如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 这行代码会导致 BN 进入 train 模式</span>    <span class="token builtin">super</span><span class="token punctuation">(</span>ResNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>train<span class="token punctuation">(</span>mode<span class="token punctuation">)</span>    <span class="token comment"># 再次调用，固定 stem 和 前 n 个 stage 的 BN</span>    self<span class="token punctuation">.</span>_freeze_stages<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># 如果所有 BN 都采用全局均值和方差，则需要对整个网络的 BN 都开启 eval 模式</span>    <span class="token keyword">if</span> mode <span class="token keyword">and</span> self<span class="token punctuation">.</span>norm_eval<span class="token punctuation">:</span>        <span class="token keyword">for</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment"># trick: eval have effect on BatchNorm only</span>            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> _BatchNorm<span class="token punctuation">)</span><span class="token punctuation">:</span>                m<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果你自定义了骨架网络，想实现固定某一部分权重功能，你可以参考上述做法。</p><p><strong>(3) norm_cfg 和 norm_eval</strong></p><p><code>norm_cfg</code> 表示所采用的归一化算子，一般是 BN 或者 GN，而 <code>requires_grad</code> 表示该算子是否需要梯度，也就是是否进行参数更新，而布尔参数 <code>norm_eval</code> 是用于控制整个骨架网络的归一化算子是否需要变成 eval 模式。</p><p>RetinaNet 中用法是 <code>norm_cfg=dict(type='BN', requires_grad=True)</code>，表示通过 Registry 模式实例化 BN 类，并且设置为参数可学习。在 MMDetection 中会常看到通过字典配置方式来实例化某个类的做法， 底层是采用了装饰器模式进行构建，最大好处是扩展性极强，类和类之间的耦合度降低。</p><p><strong>(4) style</strong></p><p><code>style='caffe'</code> 和 <code>style='pytorch'</code> 的差别就在 <code>Bottleneck</code> 模块中</p><p><img src="https://pic3.zhimg.com/80/v2-d62671f250295573295414341010cb3e_720w.jpg" alt="img"></p><p><code>Bottleneck</code> 是标准的 1x1-3x3-1x1 结构，考虑 stride=2 下采样的场景，caffe 模式下，stride 参数放置在第一个 1x1 卷积上，而 Pyorch 模式下，stride 放在第二个 3x3 卷积上：</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">if self.style == 'pytorch':        self.conv1_stride = 1        self.conv2_stride = stride    else:        self.conv1_stride = stride        self.conv2_stride = 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>出现两种模式的原因是因为 ResNet 本身就有不同的实现，torchvision 的 resnet 和早期 release 的 resnet 版本不一样，使得目标检测框架在使用 Backbone 的时候有两种不同的配置，不过目前新网络都是采用 PyTorch 模式。</p><h3 id="2-2-Neck"><a href="#2-2-Neck" class="headerlink" title="2.2 Neck"></a>2.2 Neck</h3><p>Neck 模块即为 FPN，其简要结构如下所示：</p><p><img src="https://pic3.zhimg.com/80/v2-63d4cede659613ffd0886ddff49591ee_720w.jpg" alt="img"></p><p>MMDetection 中对应配置为：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">neck<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'FPN'</span><span class="token punctuation">,</span>    <span class="token comment"># ResNet 模块输出的4个尺度特征图通道数</span>    in_channels<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">2048</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token comment"># FPN 输出的每个尺度输出特征图通道</span>    out_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>    <span class="token comment"># 从输入多尺度特征图的第几个开始计算</span>    start_level<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>    <span class="token comment"># 额外输出层的特征图来源</span>    add_extra_convs<span class="token operator">=</span><span class="token string">'on_input'</span><span class="token punctuation">,</span>    <span class="token comment"># FPN 输出特征图个数</span>    num_outs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>前面说过 ResNet 输出 4 个不同尺度特征图 (c2,c3,c4,c5)，stride 分别是 (4,8,16,32)，通道数为 (256,512,1024,2048),通过配置文件我们可以知道：</p><ul><li><code>start_level=1</code> 说明虽然输入是 4 个特征图，但是实际上 FPN 中仅仅用了后面三个</li><li><code>num_outs=5</code> 说明 FPN 模块虽然是接收 3 个特征图，但是输出 5 个特征图</li><li><code>add_extra_convs='on_input'</code> 说明额外输出的 2 个特征图的来源是骨架网络输出，而不是 FPN 层本身输出又作为后面层的输入</li><li><code>out_channels=256</code> 说明了 5 个输出特征图的通道数都是 256</li></ul><p>下面对代码运行流程进行描述：</p><ol><li>将 c3、c4 和 c5 三个特征图全部经过各自 1x1 卷积进行通道变换得到 m3~m5，输出通道统一为 256</li><li>从 m5(特征图最小)开始，先进行 2 倍最近邻上采样，然后和 m4 进行 add 操作，得到新的 m4</li><li>将新 m4 进行 2 倍最近邻上采样，然后和 m3 进行 add 操作，得到新的 m3</li><li>对 m5 和新融合后的 m4、m3，都进行各自的 3x3 卷积，得到 3 个尺度的最终输出 P5～P3</li><li>将 c5 进行 3x3 且 stride=2 的卷积操作，得到 P6</li><li>将 P6 再一次进行 3x3 且 stride=2 的卷积操作，得到 P7</li></ol><p>P6 和 P7 目的是提供一个大感受野强语义的特征图，有利于大物体和超大物体检测。 在 RetinaNet 的 FPN 模块中只包括卷积，不包括 BN 和 ReLU。</p><p>总结：FPN 模块接收 c3, c4, c5 三个特征图，输出 P3-P7 五个特征图，通道数都是 256, stride 为 (8,16,32,64,128)，其中大 stride (特征图小)用于检测大物体，小 stride (特征图大)用于检测小物体。</p><h3 id="2-3-Head"><a href="#2-3-Head" class="headerlink" title="2.3 Head"></a>2.3 Head</h3><p>论文中作者认为 one-stage 算法 <strong>head 设计比较关键</strong>，对最终性能影响较大，相比于其余 one-stage 算法，RetinaNet 的 Head 模块比较重量级，<strong>输出头包括分类和检测两个分支</strong>，且每个分支都包括 4 个卷积层，不进行参数共享，<strong>分类 Head 输出通道是 <code>num_class*K</code><strong>，</strong>检测 head 输出通道是<code>4*K</code>, K 是 anchor 个数</strong>, 虽然每个 Head 的分类和回归分支权重不共享，但是 <strong>5 个输出特征图的 Head 模块权重是共享的</strong>。</p><p>其完整配置如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">bbox_head<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'RetinaHead'</span><span class="token punctuation">,</span>    <span class="token comment"># COCO 数据集类别个数</span>    num_classes<span class="token operator">=</span><span class="token number">80</span><span class="token punctuation">,</span>    <span class="token comment"># FPN 层输出特征图通道数</span>    in_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>    <span class="token comment"># 每个分支堆叠4层卷积</span>    stacked_convs<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    <span class="token comment"># 中间特征图通道数</span>    feat_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>    <span class="token comment"># 后面分析</span>    anchor_generator<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'AnchorGenerator'</span><span class="token punctuation">,</span>        octave_base_scale<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>        scales_per_octave<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>        ratios<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 后面分析</span>    bbox_coder<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'DeltaXYWHBBoxCoder'</span><span class="token punctuation">,</span>        target_means<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        target_stds<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 后面分析</span>    loss_cls<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'FocalLoss'</span><span class="token punctuation">,</span>        use_sigmoid<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        gamma<span class="token operator">=</span><span class="token number">2.0</span><span class="token punctuation">,</span>        alpha<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">,</span>        loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 后面分析</span>    loss_bbox<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'L1Loss'</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Head 模块比较简单，网络构建代码如下所示:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_init_layers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># stacked_convs=4</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>stacked_convs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 构建4个中间卷积层，分类和回归分支不共享权重</span>        chn <span class="token operator">=</span> self<span class="token punctuation">.</span>in_channels <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">else</span> self<span class="token punctuation">.</span>feat_channels        self<span class="token punctuation">.</span>cls_convs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>            ConvModule<span class="token punctuation">(</span>                chn<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>feat_channels<span class="token punctuation">,</span>                <span class="token number">3</span><span class="token punctuation">,</span>                <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>reg_convs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>            ConvModule<span class="token punctuation">(</span>                chn<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>feat_channels<span class="token punctuation">,</span>                <span class="token number">3</span><span class="token punctuation">,</span>                <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment"># 构建最终输出层</span>    self<span class="token punctuation">.</span>retina_cls <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>        self<span class="token punctuation">.</span>feat_channels<span class="token punctuation">,</span>        self<span class="token punctuation">.</span>num_anchors <span class="token operator">*</span> self<span class="token punctuation">.</span>cls_out_channels<span class="token punctuation">,</span>        <span class="token number">3</span><span class="token punctuation">,</span>        padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>retina_reg <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>        self<span class="token punctuation">.</span>feat_channels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_anchors <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>单张特征图的 forward 流程为:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># x是 p3-p7 中的某个特征图</span>cls_feat <span class="token operator">=</span> xreg_feat <span class="token operator">=</span> x<span class="token comment"># 4层不共享参数卷积</span><span class="token keyword">for</span> cls_conv <span class="token keyword">in</span> self<span class="token punctuation">.</span>cls_convs<span class="token punctuation">:</span>     cls_feat <span class="token operator">=</span> cls_conv<span class="token punctuation">(</span>cls_feat<span class="token punctuation">)</span><span class="token keyword">for</span> reg_conv <span class="token keyword">in</span> self<span class="token punctuation">.</span>reg_convs<span class="token punctuation">:</span>     reg_feat <span class="token operator">=</span> reg_conv<span class="token punctuation">(</span>reg_feat<span class="token punctuation">)</span><span class="token comment"># 输出特征图</span>cls_score <span class="token operator">=</span> self<span class="token punctuation">.</span>retina_cls<span class="token punctuation">(</span>cls_feat<span class="token punctuation">)</span>bbox_pred <span class="token operator">=</span> self<span class="token punctuation">.</span>retina_reg<span class="token punctuation">(</span>reg_feat<span class="token punctuation">)</span><span class="token keyword">return</span> cls_score<span class="token punctuation">,</span> bbox_pred<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>5 个输出 Head 共享所有分类或者回归分支的卷积权重，经过 Head 模块的前向流程输出一共是 5*2 个特征图。</p><h3 id="2-4-BBox-Assigner"><a href="#2-4-BBox-Assigner" class="headerlink" title="2.4 BBox Assigner"></a>2.4 BBox Assigner</h3><h3 id="2-4-1-AnchorGenerator"><a href="#2-4-1-AnchorGenerator" class="headerlink" title="2.4.1 AnchorGenerator"></a>2.4.1 AnchorGenerator</h3><p>RetinaNet 属于 Anchor-based 算法，在运行 bbox 属性分配前<strong>需要得到每个输出特征图位置的 anchor 列表</strong>，故在分析 BBox Assigner 前，需要先详细说明下 anchor 生成过程，其对应配置如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">anchor_generator<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'AnchorGenerator'</span><span class="token punctuation">,</span>    <span class="token comment"># 特征图 anchor 的 base scale, 值越大，所有 anchor 的尺度都会变大</span>    octave_base_scale<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    <span class="token comment"># 每个特征图有3个尺度，2**0, 2**(1/3), 2**(2/3)</span>    scales_per_octave<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>    <span class="token comment"># 每个特征图有3个高宽比例</span>    ratios<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token comment"># 特征图对应的 stride，必须特征图 stride 一致，不可以随意更改</span>    strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>从上面配置可以看出：RetinaNet 一共 5 个输出特征图，每个特征图上有 3 种尺度和 3 种宽高比，每个位置一共 9 个 anchor，并且通过 <code>octave_base_scale</code> 参数来控制全局 anchor 的 base scales ，如果自定义数据集中普遍都是大物体或者小物体，则可能修改更改 <code>octave_base_scale</code> 参数。</p><p><strong>为了方便理解，可以写个简单脚本可视化下指定特征图位置的 anchor 情况。</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> mmcv<span class="token punctuation">.</span>visualization <span class="token keyword">import</span> imshow_bboxes<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">from</span> mmdet<span class="token punctuation">.</span>core <span class="token keyword">import</span> build_anchor_generator<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    anchor_generator_cfg <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'AnchorGenerator'</span><span class="token punctuation">,</span>        octave_base_scale<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>        scales_per_octave<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>        ratios<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    anchor_generator <span class="token operator">=</span> build_anchor_generator<span class="token punctuation">(</span>anchor_generator_cfg<span class="token punctuation">)</span>    <span class="token comment"># 输出原图尺度上 anchor 坐标 xyxy 左上角格式</span>    <span class="token comment"># base_anchors 长度为5，表示5个输出特征图，不同的特征图尺度相差的只是 strides</span>    <span class="token comment"># 故我们取 strides=8 的位置 anchor 可视化即可</span>    base_anchors <span class="token operator">=</span> anchor_generator<span class="token punctuation">.</span>base_anchors<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    h <span class="token operator">=</span> <span class="token number">100</span>    w <span class="token operator">=</span> <span class="token number">160</span>    img <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span>h<span class="token punctuation">,</span> w<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">255</span>    base_anchors<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+=</span> w <span class="token operator">//</span> <span class="token number">2</span>    base_anchors<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+=</span> h <span class="token operator">//</span> <span class="token number">2</span>    colors <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'green'</span><span class="token punctuation">,</span> <span class="token string">'red'</span><span class="token punctuation">,</span> <span class="token string">'blue'</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        base_anchor <span class="token operator">=</span> base_anchors<span class="token punctuation">[</span>i<span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>        imshow_bboxes<span class="token punctuation">(</span>img<span class="token punctuation">,</span> base_anchor<span class="token punctuation">,</span> show<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> colors<span class="token operator">=</span>colors<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>结果如下所示：</p><p><img src="https://pic3.zhimg.com/80/v2-6d3d2c417ced4976933b3c4c7ec7e64a_720w.jpg" alt="img"></p><p>相同颜色表示在该特征图中基本尺度是相同的，只是宽高比不一样而已。</p><p>在对 AnchorGenerator 有基本认识后，下面对其实现源码进行分析：</p><p><strong>(1) 先对单个位置 (0,0) 生成 base anchors</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">w <span class="token operator">=</span> base_sizeh <span class="token operator">=</span> base_size<span class="token comment"># 计算高宽比例</span>h_ratios <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>ratios<span class="token punctuation">)</span>w_ratios <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> h_ratios<span class="token comment"># base_size 乘上宽高比例乘上尺度，就可以得到 n 个 anchor 的原图尺度wh值</span>ws <span class="token operator">=</span> <span class="token punctuation">(</span>w <span class="token operator">*</span> w_ratios<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token operator">*</span> scales<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>hs <span class="token operator">=</span> <span class="token punctuation">(</span>h <span class="token operator">*</span> h_ratios<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token operator">*</span> scales<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment"># 得到 x1y1x2y2 格式的 base_anchor 坐标值</span>base_anchors <span class="token operator">=</span> <span class="token punctuation">[</span>    x_center <span class="token operator">-</span> <span class="token number">0.5</span> <span class="token operator">*</span> ws<span class="token punctuation">,</span> y_center <span class="token operator">-</span> <span class="token number">0.5</span> <span class="token operator">*</span> hs<span class="token punctuation">,</span> x_center <span class="token operator">+</span> <span class="token number">0.5</span> <span class="token operator">*</span> ws<span class="token punctuation">,</span>    y_center <span class="token operator">+</span> <span class="token number">0.5</span> <span class="token operator">*</span> hs<span class="token punctuation">]</span><span class="token comment"># 堆叠起来即可</span>base_anchors <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>base_anchors<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>(2) 利用输入特征图尺寸加上 base anchors，得到每个特征图位置的对于原图的 anchors</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">feat_h<span class="token punctuation">,</span> feat_w <span class="token operator">=</span> featmap_size<span class="token comment"># 遍历特征图上所有位置，并且乘上 stride，从而变成原图坐标</span>shift_x <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> feat_w<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span> <span class="token operator">*</span> stride<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>shift_y <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> feat_h<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span> <span class="token operator">*</span> stride<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>shift_xx<span class="token punctuation">,</span> shift_yy <span class="token operator">=</span> self<span class="token punctuation">.</span>_meshgrid<span class="token punctuation">(</span>shift_x<span class="token punctuation">,</span> shift_y<span class="token punctuation">)</span>shifts <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>shift_xx<span class="token punctuation">,</span> shift_yy<span class="token punctuation">,</span> shift_xx<span class="token punctuation">,</span> shift_yy<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>shifts <span class="token operator">=</span> shifts<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>base_anchors<span class="token punctuation">)</span><span class="token comment"># (0,0) 位置的 base_anchor，假设原图上坐标 shifts，即可得到特征图上面每个点映射到原图坐标上的 anchor</span>all_anchors <span class="token operator">=</span> base_anchors<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">+</span> shifts<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>all_anchors <span class="token operator">=</span> all_anchors<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token keyword">return</span> all_anchors<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>简单来说就是：假设一共 m 个输出特征图</p><ul><li>遍历 m 个输出特征图，在每个特征图的 (0,0) 或者说原图的 (0,0) 坐标位置生成 <code>base_anchors</code>，注意 <code>base_anchors</code> 不是特征图尺度，而是原图尺度</li><li>遍历 m 个输出特征图中每个特征图上每个坐标点，将其映射到原图坐标上</li><li>原图坐标点加上 <code>base_anchors</code>，就可以得到特征图每个位置的对应到原图尺度的 anchor 列表，anchor 列表长度为 m</li></ul><h3 id="2-4-2-BBox-Assigner"><a href="#2-4-2-BBox-Assigner" class="headerlink" title="2.4.2 BBox Assigner"></a>2.4.2 BBox Assigner</h3><p>计算得到输出特征图上面每个点对应的原图 anchor 坐标后，就可以和 gt 信息计算每个 anchor 的正负样本属性，对应配置如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">assigner<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># 最大 IoU 原则分配器</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'MaxIoUAssigner'</span><span class="token punctuation">,</span>    <span class="token comment"># 正样本阈值</span>    pos_iou_thr<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>    <span class="token comment"># 负样本阈值</span>    neg_iou_thr<span class="token operator">=</span><span class="token number">0.4</span><span class="token punctuation">,</span>    <span class="token comment"># 正样本阈值下限</span>    min_pos_iou<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>    <span class="token comment"># 忽略 bboes 的阈值，-1表示不忽略</span>    ignore_iof_thr<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>仅从上面的描述可能比较难理解参数含义，通过下面的流程分析就比较容易理解每个参数含义了。MaxIoUAssigner 操作包括 4 个步骤：</p><p><strong>(1) 初始化所有 anchor 为忽略样本</strong></p><p>假设所有输出特征的所有 anchor 总数一共 n 个，对应某张图片中 gt bbox 个数为 m，首先初始化长度为 n 的 <code>assigned_gt_inds</code>，全部赋值为 -1，表示当前全部设置为忽略样本</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 1. assign -1 by default</span>assigned_gt_inds <span class="token operator">=</span> overlaps<span class="token punctuation">.</span>new_full<span class="token punctuation">(</span><span class="token punctuation">(</span>num_bboxes<span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">,</span>                                     <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>                                     dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><strong>(2) 计算背景样本</strong></p><p>将每个 anchor 和所有 gt bbox 计算 iou，找出最大 iou，如果该 iou 小于 <code>neg_iou_thr</code> 或者在背景样本阈值范围内，则该 anchor 对应索引位置的 <code>assigned_gt_inds</code> 设置为 0，表示是负样本(背景样本)</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">max_overlaps<span class="token punctuation">,</span> argmax_overlaps <span class="token operator">=</span> overlaps<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>gt_max_overlaps<span class="token punctuation">,</span> gt_argmax_overlaps <span class="token operator">=</span> overlaps<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment"># 2. assign negative: below</span><span class="token comment"># the negative inds are set to be 0</span><span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>neg_iou_thr<span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    assigned_gt_inds<span class="token punctuation">[</span><span class="token punctuation">(</span>max_overlaps <span class="token operator">&gt;=</span> <span class="token number">0</span><span class="token punctuation">)</span>                     <span class="token operator">&amp;</span> <span class="token punctuation">(</span>max_overlaps <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>neg_iou_thr<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>neg_iou_thr<span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>neg_iou_thr<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span>    <span class="token comment"># 可以设置一个范围</span>    assigned_gt_inds<span class="token punctuation">[</span><span class="token punctuation">(</span>max_overlaps <span class="token operator">&gt;=</span> self<span class="token punctuation">.</span>neg_iou_thr<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                     <span class="token operator">&amp;</span> <span class="token punctuation">(</span>max_overlaps <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>neg_iou_thr<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>(3) 计算高质量正样本</strong></p><p>将每个 anchor 和所有 gt bbox 计算 iou，找出最大 iou，如果其最大 iou 大于等于 <code>pos_iou_thr</code>，则设置该 anchor 对应所有的 <code>assigned_gt_inds</code> 设置为当前匹配 gt bbox 的编号 +1(后面会减掉 1)，表示该 anchor 负责预测该 gt bbox，且是高质量 anchor。之所以要加 1，是为了区分背景样本(背景样本的 <code>assigned_gt_inds</code> 值为 0)</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 3. assign positive: above positive IoU threshold</span>pos_inds <span class="token operator">=</span> max_overlaps <span class="token operator">&gt;=</span> self<span class="token punctuation">.</span>pos_iou_thrassigned_gt_inds<span class="token punctuation">[</span>pos_inds<span class="token punctuation">]</span> <span class="token operator">=</span> argmax_overlaps<span class="token punctuation">[</span>pos_inds<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>(4) 适当增加更多正样本</strong></p><p>在第三步计算高质量正样本中可能会出现某些 gt bbox 没有分配给任何一个 anchor (由于 iou 低于 <code>pos_iou_thr</code>)，导致该 gt bbox 不被认为是前景物体，此时可以通过 <code>self.match_low_quality=True</code> 配置进行补充正样本。</p><p>对于每个 gt bbox 需要找出和其最大 iou 的 anchor 索引，如果其 iou 大于 <code>min_pos_iou</code>，则将该 anchor 对应索引的 <code>assigned_gt_inds</code> 设置为正样本，表示该 anchor 负责预测对应的 gt bbox。通过本步骤，可以最大程度保证每个 gt bbox 都有相应的 anchor 负责预测，<strong>但是如果其最大 iou 值还是小于 <code>min_pos_iou</code>，则依然不被认为是前景物体</strong>。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> self<span class="token punctuation">.</span>match_low_quality<span class="token punctuation">:</span>    <span class="token comment"># Low-quality matching will overwirte the assigned_gt_inds assigned</span>    <span class="token comment"># in Step 3. Thus, the assigned gt might not be the best one for</span>    <span class="token comment"># prediction.</span>    <span class="token comment"># For example, if bbox A has 0.9 and 0.8 iou with GT bbox 1 &amp; 2,</span>    <span class="token comment"># bbox 1 will be assigned as the best target for bbox A in step 3.</span>    <span class="token comment"># However, if GT bbox 2's gt_argmax_overlaps = A, bbox A's</span>    <span class="token comment"># assigned_gt_inds will be overwritten to be bbox B.</span>    <span class="token comment"># This might be the reason that it is not used in ROI Heads.</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_gts<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> gt_max_overlaps<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&gt;=</span> self<span class="token punctuation">.</span>min_pos_iou<span class="token punctuation">:</span>            <span class="token keyword">if</span> self<span class="token punctuation">.</span>gt_max_assign_all<span class="token punctuation">:</span>                <span class="token comment">#如果有多个相同最高 iou 的 anchor 和该 gt bbox 对应，则一并赋值</span>                max_iou_inds <span class="token operator">=</span> overlaps<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">==</span> gt_max_overlaps<span class="token punctuation">[</span>i<span class="token punctuation">]</span>                <span class="token comment"># 同样需要加1</span>                assigned_gt_inds<span class="token punctuation">[</span>max_iou_inds<span class="token punctuation">]</span> <span class="token operator">=</span> i <span class="token operator">+</span> <span class="token number">1</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                assigned_gt_inds<span class="token punctuation">[</span>gt_argmax_overlaps<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> i <span class="token operator">+</span> <span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>从这一步可以看出，3 和 4 有部分 anchor 重复分配了，即当某个 gt bbox 和 anchor 的最大 iou 大于等于 <code>pos_iou_thr</code>，那肯定大于 <code>min_pos_iou</code>，此时 3 和 4 步骤分配的同一个 anchor，并且从上面注释可以看出本步骤可能会引入低质量 anchor，是否需要开启本步骤需要根据不同算法来确定。</p><p>再次回到 RetinaNet 的 bbox assigner 配置：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">assigner<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># 最大 IoU 原则分配器</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'MaxIoUAssigner'</span><span class="token punctuation">,</span>    <span class="token comment"># 正样本阈值</span>    pos_iou_thr<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>    <span class="token comment"># 负样本阈值</span>    neg_iou_thr<span class="token operator">=</span><span class="token number">0.4</span><span class="token punctuation">,</span>    <span class="token comment"># 正样本阈值下限</span>    min_pos_iou<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>    <span class="token comment"># 忽略 bbox 的阈值，-1表示不忽略</span>    ignore_iof_thr<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>此时可以可以得到如下总结：</p><ul><li>如果 anchor 和所有 gt bbox 的最大 iou 值小于 0.4，那么该 anchor 就是背景样本</li><li>如果 anchor 和所有 gt bbox 的最大 iou 值大于等于 0.5，那么该 anchor 就是高质量正样本</li><li>如果 gt bbox 和所有 anchor 的最大 iou 值大于等于 0(可以看出每个 gt bbox 都一定有至少一个 anchor 匹配)，那么该 gt bbox 所对应的 anchor 也是正样本</li><li>其余样本全部为忽略样本即 anchor 和所有 gt bbox 的最大 iou 值处于 [0.4,0.5) 区间的 anchor 为忽略样本，不计算 loss</li></ul><h3 id="2-5-BBox-Encoder-Decoder"><a href="#2-5-BBox-Encoder-Decoder" class="headerlink" title="2.5 BBox Encoder Decoder"></a>2.5 BBox Encoder Decoder</h3><p>在 anchor-based 算法中，为了利用 anchor 信息进行更快更好的收敛，一般会对 head 输出的 bbox 分支 4 个值进行编解码操作，作用有两个：</p><ol><li>更好的平衡分类和回归分支 loss，以及平衡 bbox 四个预测值的 loss</li><li>训练过程中引入 anchor 信息，加快收敛</li></ol><p>RetinaNet 采用的编解码函数是主流的 DeltaXYWHBBoxCoder，其配置如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">bbox_coder<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'DeltaXYWHBBoxCoder'</span><span class="token punctuation">,</span>    target_means<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    target_stds<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><code>target_means</code> 和 <code>target_stds</code> 相当于对 bbox 回归的 4 个 <code>txtytwth</code> 进行变换。在不考虑 <code>target_means</code> 和 <code>target_stds</code> 情况下，其编码公式如下：</p><p><img src="https://www.zhihu.com/equation?tex=t%5E*_x=(x%5E*-x_a)/w_a,t%5E*_y=(y%5E*-x_a)/h_a" alt="[公式]"></p><p><img src="https://www.zhihu.com/equation?tex=t%5E*_w=log(w%5E*/w_a),t%5E*_h=log(h%5E*/h_a)" alt="[公式]"></p><p><img src="https://www.zhihu.com/equation?tex=x%5E*,y%5E*" alt="[公式]"> 是 gt bbox 的中心 xy 坐标， <img src="https://www.zhihu.com/equation?tex=w%5E*,h%5E*" alt="[公式]"> 是 gt bbox 的 wh 值， <img src="https://www.zhihu.com/equation?tex=x%5Ea,y%5Ea" alt="[公式]"> 是 anchor 的中心 xy 坐标， <img src="https://www.zhihu.com/equation?tex=w%5Ea,h%5Ea" alt="[公式]"> 是 anchor 的 wh 值， <img src="https://www.zhihu.com/equation?tex=t%5E*" alt="[公式]"> 是 bbox 分支输出的 4 个值对应 targets。可以看出 <img src="https://www.zhihu.com/equation?tex=t_x,t_y" alt="[公式]"> 预测值表示 gt bbox 中心相对于 anchor 中心点的偏移，并且通过除以 anchor 的 wh 进行归一化；而 <img src="https://www.zhihu.com/equation?tex=t_w,t_h" alt="[公式]"> 预测值表示 gt bbox 的 wh 除以 anchor 的 wh，然后取 log 非线性变换即可。</p><p>考虑编码过程 <code>target_means</code> 和 <code>target_stds</code> 情况下，核心代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">dx <span class="token operator">=</span> <span class="token punctuation">(</span>gx <span class="token operator">-</span> px<span class="token punctuation">)</span> <span class="token operator">/</span> pwdy <span class="token operator">=</span> <span class="token punctuation">(</span>gy <span class="token operator">-</span> py<span class="token punctuation">)</span> <span class="token operator">/</span> phdw <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>gw <span class="token operator">/</span> pw<span class="token punctuation">)</span>dh <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>gh <span class="token operator">/</span> ph<span class="token punctuation">)</span>deltas <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>dx<span class="token punctuation">,</span> dy<span class="token punctuation">,</span> dw<span class="token punctuation">,</span> dh<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment"># 最后减掉均值，处于标准差</span>means <span class="token operator">=</span> deltas<span class="token punctuation">.</span>new_tensor<span class="token punctuation">(</span>means<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>stds <span class="token operator">=</span> deltas<span class="token punctuation">.</span>new_tensor<span class="token punctuation">(</span>stds<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>deltas <span class="token operator">=</span> deltas<span class="token punctuation">.</span>sub_<span class="token punctuation">(</span>means<span class="token punctuation">)</span><span class="token punctuation">.</span>div_<span class="token punctuation">(</span>stds<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>解码过程是编码过程的反向，比较容易理解，其核心代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 先乘上 std，加上 mean</span>means <span class="token operator">=</span> deltas<span class="token punctuation">.</span>new_tensor<span class="token punctuation">(</span>means<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> deltas<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">)</span>stds <span class="token operator">=</span> deltas<span class="token punctuation">.</span>new_tensor<span class="token punctuation">(</span>stds<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> deltas<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">)</span>denorm_deltas <span class="token operator">=</span> deltas <span class="token operator">*</span> stds <span class="token operator">+</span> meansdx <span class="token operator">=</span> denorm_deltas<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span>dy <span class="token operator">=</span> denorm_deltas<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span>dw <span class="token operator">=</span> denorm_deltas<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span>dh <span class="token operator">=</span> denorm_deltas<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token comment"># wh 解码</span>gw <span class="token operator">=</span> pw <span class="token operator">*</span> dw<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">)</span>gh <span class="token operator">=</span> ph <span class="token operator">*</span> dh<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 中心点 xy 解码</span>gx <span class="token operator">=</span> px <span class="token operator">+</span> pw <span class="token operator">*</span> dxgy <span class="token operator">=</span> py <span class="token operator">+</span> ph <span class="token operator">*</span> dy<span class="token comment"># 得到 x1y1x2y2 的 gt bbox 预测坐标</span>x1 <span class="token operator">=</span> gx <span class="token operator">-</span> gw <span class="token operator">*</span> <span class="token number">0.5</span>y1 <span class="token operator">=</span> gy <span class="token operator">-</span> gh <span class="token operator">*</span> <span class="token number">0.5</span>x2 <span class="token operator">=</span> gx <span class="token operator">+</span> gw <span class="token operator">*</span> <span class="token number">0.5</span>y2 <span class="token operator">=</span> gy <span class="token operator">+</span> gh <span class="token operator">*</span> <span class="token number">0.5</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-6-Loss"><a href="#2-6-Loss" class="headerlink" title="2.6 Loss"></a>2.6 Loss</h3><p>前面说过 RetinaNet 一个非常大的亮点就是提出了 Focal Loss。</p><p><img src="https://pic4.zhimg.com/80/v2-7bf35b65e528d7ea226281211beef7ef_720w.jpg" alt="img"></p><p>Focal Loss 属于 CE Loss 的动态加权版本，其可以根据样本的难易程度(预测值和 label 的差距可以反映)对每个样本单独加权，易学习样本权重比较低，难样本权重比较高。因为在前面的 bbox assigner 环节，大部分样本都是背景易学习样本，虽然其本身 loss 比较小，但是由于数目众多最终会主导梯度，从而得到次优模型，而 Focal Loss 通过指数效应把大量易学习样本的权重大大降低，从而避免上述问题。</p><p>完整的 Focal Loss 为： <img src="https://www.zhihu.com/equation?tex=loss_%7Bfocal%7D+=-%5Calpha_t(1-p_t)%5E%5Cgamma+log(p_t)" alt="[公式]"></p><p><img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> 属于正负样本的加权参数，值越大，正样本的权重越大， <img src="https://www.zhihu.com/equation?tex=%5Cbeta" alt="[公式]"> 有focal效应，可以控制难易样本权重，值越大，对分类错误样本梯度越大(难样本权重大)，focal 效应越大，这个参数非常关键。</p><p>代码实现方面也比较简单，MMDetection 提供了 py 和 cuda 版本，py 版本如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">pred_sigmoid <span class="token operator">=</span> pred<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># one-hot 格式</span>target <span class="token operator">=</span> target<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>pred<span class="token punctuation">)</span>pt <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> pred_sigmoid<span class="token punctuation">)</span> <span class="token operator">*</span> target <span class="token operator">+</span> pred_sigmoid <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> target<span class="token punctuation">)</span>focal_weight <span class="token operator">=</span> <span class="token punctuation">(</span>alpha <span class="token operator">*</span> target <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> alpha<span class="token punctuation">)</span> <span class="token operator">*</span>            <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> target<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> pt<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>gamma<span class="token punctuation">)</span>loss <span class="token operator">=</span> F<span class="token punctuation">.</span>binary_cross_entropy_with_logits<span class="token punctuation">(</span>        pred<span class="token punctuation">,</span> target<span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span> <span class="token operator">*</span> focal_weightloss <span class="token operator">=</span> weight_reduce_loss<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> weight<span class="token punctuation">,</span> reduction<span class="token punctuation">,</span> avg_factor<span class="token punctuation">)</span><span class="token keyword">return</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>RetinaNet 的完整 loss 配置如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 分类 loss</span>loss_cls<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'FocalLoss'</span><span class="token punctuation">,</span>    use_sigmoid<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    gamma<span class="token operator">=</span><span class="token number">2.0</span><span class="token punctuation">,</span>    alpha<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">,</span>    loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token comment"># 回归 loss</span>loss_bbox<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'L1Loss'</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-7-测试流程"><a href="#2-7-测试流程" class="headerlink" title="2.7 测试流程"></a>2.7 测试流程</h3><p>对应配置如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">test_cfg <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># nms 前每个输出层最多保留1000个预测框</span>    nms_pre<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span>    <span class="token comment"># 过滤掉的最小 bbox 尺寸</span>    min_bbox_size<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>    <span class="token comment"># 分值阈值</span>    score_thr<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">,</span>    <span class="token comment"># nms 方法和 nms 阈值</span>    nms<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'nms'</span><span class="token punctuation">,</span> iou_threshold<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 最终输出的每张图片最多 bbox 个数</span>    max_per_img<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>测试阶段流程如下：</p><p>(1) 对 5 个 head 输出特征图结果进行遍历，先按照预测分值排序，保留前 nms_pre 个预测结果</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> nms_pre <span class="token operator">&gt;</span> <span class="token number">0</span> <span class="token keyword">and</span> scores<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">&gt;</span> nms_pre<span class="token punctuation">:</span>    <span class="token comment"># Get maximum scores for foreground classes.</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_sigmoid_cls<span class="token punctuation">:</span>        max_scores<span class="token punctuation">,</span> _ <span class="token operator">=</span> scores<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token comment"># remind that we set FG labels to [0, num_class-1]</span>        <span class="token comment"># since mmdet v2.0</span>        <span class="token comment"># BG cat_id: num_class</span>        max_scores<span class="token punctuation">,</span> _ <span class="token operator">=</span> scores<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    _<span class="token punctuation">,</span> topk_inds <span class="token operator">=</span> max_scores<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>nms_pre<span class="token punctuation">)</span>    anchors <span class="token operator">=</span> anchors<span class="token punctuation">[</span>topk_inds<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>    bbox_pred <span class="token operator">=</span> bbox_pred<span class="token punctuation">[</span>topk_inds<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>    scores <span class="token operator">=</span> scores<span class="token punctuation">[</span>topk_inds<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>(2) 对剩下的 bbox 进行解码</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">bboxes <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_coder<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>    anchors<span class="token punctuation">,</span> bbox_pred<span class="token punctuation">,</span> max_shape<span class="token operator">=</span>img_shape<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>(3) 还原到原图尺度</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">mlvl_bboxes <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>mlvl_bboxes<span class="token punctuation">)</span><span class="token keyword">if</span> rescale<span class="token punctuation">:</span>    mlvl_bboxes <span class="token operator">/=</span> mlvl_bboxes<span class="token punctuation">.</span>new_tensor<span class="token punctuation">(</span>scale_factor<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>(4) 用 <code>score_thr</code> 阈值对所有结果进行过滤，然后将保留框进行 nms，最终输出框最大为 <code>max_per_img</code> 个</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">valid_mask <span class="token operator">=</span> scores <span class="token operator">&gt;</span> score_thrinds <span class="token operator">=</span> valid_mask<span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span>as_tuple<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>bboxes<span class="token punctuation">,</span> scores<span class="token punctuation">,</span> labels <span class="token operator">=</span> bboxes<span class="token punctuation">[</span>inds<span class="token punctuation">]</span><span class="token punctuation">,</span> scores<span class="token punctuation">[</span>inds<span class="token punctuation">]</span><span class="token punctuation">,</span> labels<span class="token punctuation">[</span>inds<span class="token punctuation">]</span>dets<span class="token punctuation">,</span> keep <span class="token operator">=</span> batched_nms<span class="token punctuation">(</span>bboxes<span class="token punctuation">,</span> scores<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> nms_cfg<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h2><p>本文结合源码深入详细的分析了 MMDetection 中的 RetinaNet 模型，不仅如此，本文还将所涉及到的所有配置参数都进行了仔细分析，希望读者通过阅读本文可以了解到：</p><ul><li>RetinaNet 算法的整个实现过程和细节</li><li>MMDetection 中算法的配置参数具体含义和用法</li><li>对 MMDetection 有更加清晰深入的理解</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>轻松掌握 MMDetection 整体构建流程(二)</title>
      <link href="/2022/04/05/mmdetection-xue-xi-bi-ji-2-zheng-ti-gou-jian-liu-cheng-dai-ma/"/>
      <url>/2022/04/05/mmdetection-xue-xi-bi-ji-2-zheng-ti-gou-jian-liu-cheng-dai-ma/</url>
      
        <content type="html"><![CDATA[<h1 id="轻松掌握-MMDetection-整体构建流程-二"><a href="#轻松掌握-MMDetection-整体构建流程-二" class="headerlink" title="轻松掌握 MMDetection 整体构建流程(二)"></a>轻松掌握 MMDetection 整体构建流程(二)</h1><h2 id="0-内容概览"><a href="#0-内容概览" class="headerlink" title="0 内容概览"></a>0 内容概览</h2><p>上一节中，重点分析了 MMDetection 框架中 Model 整体构建流程，但仅对 Model 算法组件方面进行深入分析，并未涉及整个框架训练和测试流程。</p><p>本文核心内容是<strong>按照抽象到具体方式，从多个层次进行训练和测试流程深入解析</strong>，<strong>从最抽象层讲起，到最后核心代码实现</strong>，帮助理解 MMDetection 开源框架整体构建细节。</p><h2 id="1-第一层整体抽象"><a href="#1-第一层整体抽象" class="headerlink" title="1 第一层整体抽象"></a>1 第一层整体抽象</h2><p><img src="https://pic2.zhimg.com/80/v2-2463639f7e39afd273fdeccbfa530d49_720w.jpg" alt="img"></p><ul><li>上图为 MMDetection 框架整体训练和测试抽象流程图。按照数据流过程，<strong>训练流程可以简单总结为</strong>：</li></ul><ol><li>给定任何一个数据集，首先需要<strong>构建 Dataset 类</strong>，用于迭代输出数据。</li><li>在迭代输出数据的时候需要通过数据 Pipeline <strong>对数据进行各种处理</strong>，最典型的处理流是训练中的<strong>数据增强</strong>操作，测试中的数据预处理等等。</li><li><strong>通过 Sampler 采样器可以控制 Dataset 输出的数据顺序</strong>，最常用的是随机采样器 RandomSampler。由于 Dataset 中输出的图片大小不一样，<strong>为了尽可能减少后续组成 batch 时 pad 的像素个数，MMDetection 引入了分组采样器 GroupSampler</strong> 和 DistributedGroupSampler，相当于在 RandomSampler 基础上额外新增了根据图片宽高比进行 group 功能。</li><li><strong>将 Sampler 和 Dataset 都输入给 DataLoader</strong>，然后通过 DataLoader 输出已组成 batch 的数据，作为 Model 的输入。</li><li>对于任何一个 Model，<strong>为了方便处理数据流以及分布式需求</strong>，MMDetection 引入了两个 Model 的上层封装：<strong>单机版本 MMDataParallel</strong>、<strong>分布式（单机多卡或多机多卡）版本 MMDistributedDataParallel</strong>。</li><li>Model 运行后会输出 <strong>loss 以及其他一些信息，会通过 logger 进行保存或者可视化</strong>。</li><li>为了更好地解耦， 方便地获取各个组件之间依赖和灵活扩展，MMDetection <strong>引入了 Runner 类进行全生命周期管理</strong>，并且通过 <strong>Hook 方便的获取</strong>、修改和拦截任何生命周期数据流，扩展非常便捷。</li></ol><ul><li>而<strong>测试流程</strong>就比较简单了，直接对 DataLoader 输出的数据进行前向推理即可，还原到最终原图尺度过程也是在 Model 中完成。</li></ul><p>以上就是 MMDetection 框架整体训练和测试抽象流程，上图不仅仅反映了训练和测试数据流，而且还包括了模块和模块之间的调用关系。<strong>对于训练而言，最核心部分应该是 Runner，理解了 Runner 的运行流程，也就理解了整个 MMDetection 数据流。</strong></p><h2 id="2-第二层模块抽象"><a href="#2-第二层模块抽象" class="headerlink" title="2 第二层模块抽象"></a>2 第二层模块抽象</h2><p>在总体把握了整个 MMDetection 框架训练和测试流程后，下个层次是每个模块内部抽象流程，主要包括 <strong>Pipeline、DataParallel、Model、Runner 和 Hooks。</strong></p><h3 id="2-1-Pipeline"><a href="#2-1-Pipeline" class="headerlink" title="2.1 Pipeline"></a>2.1 Pipeline</h3><p>Pipeline 实际上<strong>由一系列按照插入顺序运行的数据处理模块组成</strong>，每个模块完成某个特定功能，例如 Resize，因为其流式顺序运行特性，故叫做 Pipeline。</p><p><img src="https://pic3.zhimg.com/80/v2-d7eb7e24335613da3da22da4ea93e132_720w.jpg" alt="img"></p><p>上图是一个非常典型的训练流程 Pipeline，<strong>每个类都接收字典输入，输出也是字典，顺序执行</strong>，其中绿色表示该类运行后新增字段，橙色表示对该字段可能会进行修改。如果进一步细分的话，不同算法的 Pipeline 都可以划分为如下部分：</p><ul><li><strong>图片和标签加载</strong>，通常用的类是 LoadImageFromFile 和 LoadAnnotations</li><li><strong>数据前处理</strong>，例如统一 Resize</li><li><strong>数据增强</strong>，典型的例如各种图片几何变换等，这部分是训练流程特有，测试阶段一般不采用(多尺度测试采用其他实现方式)</li><li><strong>数据收集</strong>，例如 Collect</li></ul><p>在 MMDetection 框架中，图片和标签加载和数据后处理流程一般是固定的，<strong>用户主要可能修改的是数据增强步骤</strong>，目前已经接入了第三方增强库 Albumentations，可以按照示例代码轻松构建属于你自己的数据增强 Pipeline。</p><p><strong>在构建自己的 Pipeline 时候一定要仔细检查你修改或者新增的字典 key 和 value，因为一旦你错误地覆盖或者修改原先字典里面的内容，代码也可能不会报错，如果出现 bug，则比较难排查</strong>。</p><h3 id="2-2-DataParallel-和-Model"><a href="#2-2-DataParallel-和-Model" class="headerlink" title="2.2 DataParallel 和 Model"></a>2.2 DataParallel 和 Model</h3><p><strong>在 MMDetection 中 DataLoader 输出的内容不是 pytorch 能处理的标准格式</strong>，还包括了 DataContainer 对象，该对象的作用是包装不同类型的对象使之能按需组成 batch。在目标检测中，每张图片 gt bbox 个数是不一样的，如果想组成 batch tensor，要么你设置最大长度，要么你自己想办法组成 batch。而考虑到内存和效率，<strong>MMDetection 通过引入 DataContainer</strong> 模块来解决上述问题，但是随之带来的问题是 pytorch 无法解析 DataContainer 对象，故需要在 MMDetection 中自行处理。</p><p>解决办法其实非常多，<strong>MMDetection 选择了一种比较优雅的实现方式：MMDataParallel 和 MMDistributedDataParallel。</strong>具体来说，这两个类相比 PyTorch 自带的 DataParallel 和 DistributedDataParallel 区别是：</p><ul><li>可以处理 DataContainer 对象</li><li>额外实现了 <code>train_step()</code> 和 <code>val_step()</code> 两个函数，可以被 Runner 调用</li></ul><p>关于这两个类的具体实现后面会描述。</p><p>而 Model 部分内容就是第一篇解读文章所讲的，具体如下：</p><p><img src="https://pic4.zhimg.com/80/v2-0c8f69636320fb40d8a8cd994296bf87_720w.jpg" alt="img"></p><h3 id="2-3-Runner-和-Hooks"><a href="#2-3-Runner-和-Hooks" class="headerlink" title="2.3 Runner 和 Hooks"></a>2.3 Runner 和 Hooks</h3><p>对于任何一个目标检测算法，都需要包括优化器、学习率设置、权重保存等等组件才能构成完整训练流程，而这些组件是通用的。为了方便 OpenMMLab 体系下的所有框架复用，<strong>在 MMCV 框架中引入了 Runner 类来统一管理训练和验证流程</strong>，并且<strong>通过 Hooks 机制以一种非常灵活、解耦的方式来实现丰富扩展功能</strong>。</p><p>关于 Runner 和 Hooks 详细解读会发布在 MMCV 系列解读文章中，简单来说 <strong>Runner 封装了 OpenMMLab 体系下各个框架的训练和验证详细流程</strong>，其负责管理训练和验证过程中的整个生命周期，<strong>通过预定义回调函数，用户可以插入定制化 Hook ，从而实现各种各样的需求</strong>。下面列出了在 MMDetection 几个非常重要的 hook 以及其作用的生命周期：</p><p><img src="https://pic4.zhimg.com/80/v2-5d614997aa85e1b841457094b7bc0cbb_720w.jpg" alt="img"></p><p>例如 CheckpointHook 在每个训练 epoch 完成后会被调用，从而实现保存权重功能。用户也可以将自己定制实现的 Hook 采用上述方式绘制，对理解整个流程或许有帮助。</p><h2 id="3-第三层代码抽象"><a href="#3-第三层代码抽象" class="headerlink" title="3 第三层代码抽象"></a>3 第三层代码抽象</h2><p>前面两层抽象分析流程，基本上把整个 MMDetection 的训练和测试流程分析完了，下面从具体代码层面进行抽象分析。</p><h3 id="3-1-训练和测试整体代码抽象流程"><a href="#3-1-训练和测试整体代码抽象流程" class="headerlink" title="3.1 训练和测试整体代码抽象流程"></a>3.1 训练和测试整体代码抽象流程</h3><p><img src="https://pic4.zhimg.com/80/v2-b03d43ed4b3dc4c02e68712e57023cff_720w.jpg" alt="img"></p><p>上图为训练和验证的和具体代码相关的整体抽象流程，对应到代码上，其核心代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#=================== tools/train.py ==================</span><span class="token comment"># 1.初始化配置</span>cfg <span class="token operator">=</span> Config<span class="token punctuation">.</span>fromfile<span class="token punctuation">(</span>args<span class="token punctuation">.</span>config<span class="token punctuation">)</span><span class="token comment"># 2.判断是否为分布式训练模式</span><span class="token comment"># 3.初始化 logger</span>logger <span class="token operator">=</span> get_root_logger<span class="token punctuation">(</span>log_file<span class="token operator">=</span>log_file<span class="token punctuation">,</span> log_level<span class="token operator">=</span>cfg<span class="token punctuation">.</span>log_level<span class="token punctuation">)</span><span class="token comment"># 4.收集运行环境并且打印，方便排查硬件和软件相关问题</span>env_info_dict <span class="token operator">=</span> collect_env<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 5.初始化 model</span>model <span class="token operator">=</span> build_detector<span class="token punctuation">(</span>cfg<span class="token punctuation">.</span>model<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token comment"># 6.初始化 datasets</span><span class="token comment">#=================== mmdet/apis/train.py ==================</span><span class="token comment"># 1.初始化 data_loaders ，内部会初始化 GroupSampler</span>data_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token comment"># 2.基于是否使用分布式训练，初始化对应的 DataParallel</span><span class="token keyword">if</span> distributed<span class="token punctuation">:</span>  model <span class="token operator">=</span> MMDistributedDataParallel<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>  model <span class="token operator">=</span> MMDataParallel<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token comment"># 3.初始化 runner</span>runner <span class="token operator">=</span> EpochBasedRunner<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token comment"># 4.注册必备 hook</span>runner<span class="token punctuation">.</span>register_training_hooks<span class="token punctuation">(</span>cfg<span class="token punctuation">.</span>lr_config<span class="token punctuation">,</span> optimizer_config<span class="token punctuation">,</span>                               cfg<span class="token punctuation">.</span>checkpoint_config<span class="token punctuation">,</span> cfg<span class="token punctuation">.</span>log_config<span class="token punctuation">,</span>                               cfg<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'momentum_config'</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 5.如果需要 val，则还需要注册 EvalHook           </span>runner<span class="token punctuation">.</span>register_hook<span class="token punctuation">(</span>eval_hook<span class="token punctuation">(</span>val_dataloader<span class="token punctuation">,</span> <span class="token operator">**</span>eval_cfg<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 6.注册用户自定义 hook</span>runner<span class="token punctuation">.</span>register_hook<span class="token punctuation">(</span>hook<span class="token punctuation">,</span> priority<span class="token operator">=</span>priority<span class="token punctuation">)</span><span class="token comment"># 7.权重恢复和加载</span><span class="token keyword">if</span> cfg<span class="token punctuation">.</span>resume_from<span class="token punctuation">:</span>    runner<span class="token punctuation">.</span>resume<span class="token punctuation">(</span>cfg<span class="token punctuation">.</span>resume_from<span class="token punctuation">)</span><span class="token keyword">elif</span> cfg<span class="token punctuation">.</span>load_from<span class="token punctuation">:</span>    runner<span class="token punctuation">.</span>load_checkpoint<span class="token punctuation">(</span>cfg<span class="token punctuation">.</span>load_from<span class="token punctuation">)</span><span class="token comment"># 8.运行，开始训练</span>runner<span class="token punctuation">.</span>run<span class="token punctuation">(</span>data_loaders<span class="token punctuation">,</span> cfg<span class="token punctuation">.</span>workflow<span class="token punctuation">,</span> cfg<span class="token punctuation">.</span>total_epochs<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面的流程比较简单，<strong>一般大家比较难以理解的是 <code>runner.run</code> 内部逻辑</strong>，下小节进行详细分析，而对于测试逻辑由于比较简单，就不详细描述了，<strong>简单来说测试流程下不需要 runner，直接加载训练好的权重，然后进行 model 推理</strong>即可。</p><h2 id="3-2-Runner-训练和验证代码抽象"><a href="#3-2-Runner-训练和验证代码抽象" class="headerlink" title="3.2 Runner 训练和验证代码抽象"></a>3.2 Runner 训练和验证代码抽象</h2><p>runner 对象内部的 run 方式是一个通用方法，可以运行任何 workflow，目前常用的主要是 train 和 val。</p><ul><li>当配置为：workflow = [(‘train’, 1)]，表示仅仅进行 train workflow，也就是迭代训练</li><li>当配置为：workflow = [(‘train’, n),(‘val’, 1)]，表示先进行 n 个 epoch 的训练，然后再进行1个 epoch 的验证，然后循环往复,如果写成 [(‘val’, 1),(‘train’, n)] 表示先进行验证，然后才开始训练</li></ul><p>当进入对应的 workflow，则会调用 runner 里面的 train() 或者 val()，表示进行一次 epoch 迭代。其代码也非常简单，如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data_loader<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>mode <span class="token operator">=</span> <span class="token string">'train'</span>    self<span class="token punctuation">.</span>data_loader <span class="token operator">=</span> data_loader    self<span class="token punctuation">.</span>call_hook<span class="token punctuation">(</span><span class="token string">'before_train_epoch'</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> data_batch <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>call_hook<span class="token punctuation">(</span><span class="token string">'before_train_iter'</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>run_iter<span class="token punctuation">(</span>data_batch<span class="token punctuation">,</span> train_mode<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>call_hook<span class="token punctuation">(</span><span class="token string">'after_train_iter'</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>call_hook<span class="token punctuation">(</span><span class="token string">'after_train_epoch'</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">val</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data_loader<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>mode <span class="token operator">=</span> <span class="token string">'val'</span>    self<span class="token punctuation">.</span>data_loader <span class="token operator">=</span> data_loader    self<span class="token punctuation">.</span>call_hook<span class="token punctuation">(</span><span class="token string">'before_val_epoch'</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> data_batch <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>call_hook<span class="token punctuation">(</span><span class="token string">'before_val_iter'</span><span class="token punctuation">)</span>        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>run_iter<span class="token punctuation">(</span>data_batch<span class="token punctuation">,</span> train_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>call_hook<span class="token punctuation">(</span><span class="token string">'after_val_iter'</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>call_hook<span class="token punctuation">(</span><span class="token string">'after_val_epoch'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>核心函数实际上是 self.run_iter()，如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">run_iter</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data_batch<span class="token punctuation">,</span> train_mode<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> train_mode<span class="token punctuation">:</span>        <span class="token comment"># 对于每次迭代，最终是调用如下函数</span>        outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>train_step<span class="token punctuation">(</span>data_batch<span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token comment"># 对于每次迭代，最终是调用如下函数</span>        outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>val_step<span class="token punctuation">(</span>data_batch<span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token string">'log_vars'</span> <span class="token keyword">in</span> outputs<span class="token punctuation">:</span>        self<span class="token punctuation">.</span>log_buffer<span class="token punctuation">.</span>update<span class="token punctuation">(</span>outputs<span class="token punctuation">[</span><span class="token string">'log_vars'</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>outputs <span class="token operator">=</span> outputs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上述 self.call_hook() 表示在不同生命周期调用所有已经注册进去的 hook，而字符串参数表示对应的生命周期。以 OptimizerHook 为例，其执行反向传播、梯度裁剪和参数更新等核心训练功能：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token decorator annotation punctuation">@HOOKS<span class="token punctuation">.</span>register_module</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">OptimizerHook</span><span class="token punctuation">(</span>Hook<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> grad_clip<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>grad_clip <span class="token operator">=</span> grad_clip    <span class="token keyword">def</span> <span class="token function">after_train_iter</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> runner<span class="token punctuation">)</span><span class="token punctuation">:</span>        runner<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        runner<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>grad_clip <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            grad_norm <span class="token operator">=</span> self<span class="token punctuation">.</span>clip_grads<span class="token punctuation">(</span>runner<span class="token punctuation">.</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        runner<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以发现 OptimizerHook 注册到的生命周期是 after_train_iter，故在每次 train() 里面运行到 <code>self.call_hook('after_val_iter')</code> 时候就会被调用，其他 hook 也是同样运行逻辑。</p><h3 id="3-3-Model-训练和测试代码抽象"><a href="#3-3-Model-训练和测试代码抽象" class="headerlink" title="3.3 Model 训练和测试代码抽象"></a>3.3 Model 训练和测试代码抽象</h3><p>前面说个，训练和验证的时候实际上调用了 model 内部的 <code>train_step</code> 和 <code>val_step</code> 函数，<strong>理解了两个函数调用流程就理解了 MMDetection 训练和测试流程</strong>。</p><p>注意，由于 model 对象会被 DataParallel 类包裹，故实际上上此时的 model，是指的 MMDataParallel 或者 MMDistributedDataParallel。以非分布式 train_step 流程为例，其内部完成调用流程图示如下：</p><p><img src="https://pic4.zhimg.com/80/v2-0d17b53f68286931803bf9d1dca10467_720w.jpg" alt="img"></p><h3 id="3-3-1-train-或者-val-流程"><a href="#3-3-1-train-或者-val-流程" class="headerlink" title="3.3.1 train 或者 val 流程"></a>3.3.1 train 或者 val 流程</h3><h4 id="1-调用-runner-中的-train-step-或者-val-step"><a href="#1-调用-runner-中的-train-step-或者-val-step" class="headerlink" title="(1) 调用 runner 中的 train_step 或者 val_step"></a><strong>(1) 调用 runner 中的 <code>train_step</code> 或者 <code>val_step</code></strong></h4><p>在 runner 中调用 <code>train_step</code> 或者 <code>val_step</code>，代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#=================== mmcv/runner/epoch_based_runner.py ==================</span><span class="token keyword">if</span> train_mode<span class="token punctuation">:</span>    outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>train_step<span class="token punctuation">(</span>data_batch<span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>    outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>val_step<span class="token punctuation">(</span>data_batch<span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>实际上，首先会调用 DataParallel 中的 <code>train_step</code> 或者 <code>val_step</code> ，其具体调用流程为：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 非分布式训练</span><span class="token comment">#=================== mmcv/parallel/data_parallel.py/MMDataParallel ==================</span><span class="token keyword">def</span> <span class="token function">train_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>inputs<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>device_ids<span class="token punctuation">:</span>        inputs<span class="token punctuation">,</span> kwargs <span class="token operator">=</span> self<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> kwargs<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment"># 此时才是调用 model 本身的 train_step</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>train_step<span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>    <span class="token comment"># 单 gpu 模式</span>    inputs<span class="token punctuation">,</span> kwargs <span class="token operator">=</span> self<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> kwargs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>device_ids<span class="token punctuation">)</span>    <span class="token comment"># 此时才是调用 model 本身的 train_step</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>train_step<span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># val_step 也是的一样逻辑</span><span class="token keyword">def</span> <span class="token function">val_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>inputs<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    inputs<span class="token punctuation">,</span> kwargs <span class="token operator">=</span> self<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> kwargs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>device_ids<span class="token punctuation">)</span>    <span class="token comment"># 此时才是调用 model 本身的 val_step</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>val_step<span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以发现，在调用 model 本身的 train_step 前，需要额外调用 <strong>scatter 函数</strong>，前面说过该函数的作用是<strong>处理 DataContainer 格式数据，使其能够组成 batch</strong>，否则程序会报错。</p><p>如果是分布式训练，则调用的实际上是 <code>mmcv/parallel/distributed.py/MMDistributedDataParallel</code>，最终调用的依然是 model 本身的 <code>train_step</code> 或者 <code>val_step</code>。</p><h4 id="2-调用-model-中的-train-step-或者-val-step"><a href="#2-调用-model-中的-train-step-或者-val-step" class="headerlink" title="(2) 调用 model 中的 train_step 或者 val_step"></a><strong>(2) 调用 model 中的 <code>train_step</code> 或者 <code>val_step</code></strong></h4><p>其核心代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#=================== mmdet/models/detectors/base.py/BaseDetector ==================</span><span class="token keyword">def</span> <span class="token function">train_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data<span class="token punctuation">,</span> optimizer<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 调用本类自身的 forward 方法</span>    losses <span class="token operator">=</span> self<span class="token punctuation">(</span><span class="token operator">**</span>data<span class="token punctuation">)</span>    <span class="token comment"># 解析 loss</span>    loss<span class="token punctuation">,</span> log_vars <span class="token operator">=</span> self<span class="token punctuation">.</span>_parse_losses<span class="token punctuation">(</span>losses<span class="token punctuation">)</span>    <span class="token comment"># 返回字典对象</span>    outputs <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>        loss<span class="token operator">=</span>loss<span class="token punctuation">,</span> log_vars<span class="token operator">=</span>log_vars<span class="token punctuation">,</span> num_samples<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token string">'img_metas'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> outputs<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">,</span> img_metas<span class="token punctuation">,</span> return_loss<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> return_loss<span class="token punctuation">:</span>        <span class="token comment"># 训练模式</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>forward_train<span class="token punctuation">(</span>img<span class="token punctuation">,</span> img_metas<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token comment"># 测试模式</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>forward_test<span class="token punctuation">(</span>img<span class="token punctuation">,</span> img_metas<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>forward_train</code> 和 <code>forward_test</code> 需要在不同的算法子类中实现，输出是 Loss 或者 预测结果。</p><p><strong>(3) 调用子类中的 <code>forward_train</code> 方法</strong></p><p>目前提供了两个具体子类，<code>TwoStageDetector</code> 和 <code>SingleStageDetector</code> ，用于实现 two-stage 和 single-stage 算法。</p><p>对于 <code>TwoStageDetector</code> 而言，其核心逻辑是：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#============= mmdet/models/detectors/two_stage.py/TwoStageDetector ============</span><span class="token keyword">def</span> <span class="token function">forward_train</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 先进行 backbone+neck 的特征提取</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>extract_feat<span class="token punctuation">(</span>img<span class="token punctuation">)</span>    losses <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># RPN forward and loss</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>with_rpn<span class="token punctuation">:</span>        <span class="token comment"># 训练 RPN</span>        proposal_cfg <span class="token operator">=</span> self<span class="token punctuation">.</span>train_cfg<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'rpn_proposal'</span><span class="token punctuation">,</span>                                          self<span class="token punctuation">.</span>test_cfg<span class="token punctuation">.</span>rpn<span class="token punctuation">)</span>        <span class="token comment"># 主要是调用 rpn_head 内部的 forward_train 方法</span>        rpn_losses<span class="token punctuation">,</span> proposal_list <span class="token operator">=</span> self<span class="token punctuation">.</span>rpn_head<span class="token punctuation">.</span>forward_train<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>        losses<span class="token punctuation">.</span>update<span class="token punctuation">(</span>rpn_losses<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        proposal_list <span class="token operator">=</span> proposals    <span class="token comment"># 第二阶段，主要是调用 roi_head 内部的 forward_train 方法</span>    roi_losses <span class="token operator">=</span> self<span class="token punctuation">.</span>roi_head<span class="token punctuation">.</span>forward_train<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    losses<span class="token punctuation">.</span>update<span class="token punctuation">(</span>roi_losses<span class="token punctuation">)</span>    <span class="token keyword">return</span> losses<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对于 <code>SingleStageDetector</code> 而言，其核心逻辑是：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#============= mmdet/models/detectors/single_stage.py/SingleStageDetector ============</span><span class="token keyword">def</span> <span class="token function">forward_train</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token builtin">super</span><span class="token punctuation">(</span>SingleStageDetector<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>forward_train<span class="token punctuation">(</span>img<span class="token punctuation">,</span> img_metas<span class="token punctuation">)</span>    <span class="token comment"># 先进行 backbone+neck 的特征提取</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>extract_feat<span class="token punctuation">(</span>img<span class="token punctuation">)</span>    <span class="token comment"># 主要是调用 bbox_head 内部的 forward_train 方法</span>    losses <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">.</span>forward_train<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> losses<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果再往里分析，那就到各个 Head 模块的训练环节了，这部分内容请读者自行分析，应该不难。</p><h3 id="3-3-2-test-流程"><a href="#3-3-2-test-流程" class="headerlink" title="3.3.2 test 流程"></a>3.3.2 test 流程</h3><p>由于没有 runner 对象，测试流程简单很多，下面简要概述：</p><ol><li>调用 MMDataParallel 或 MMDistributedDataParallel 中的 <code>forward</code> 方法</li><li>调用 base.py 中的 <code>forward</code> 方法</li><li>调用 base.py 中的 <code>self.forward_test</code> 方法</li><li>如果是单尺度测试，则会调用 TwoStageDetector 或 SingleStageDetector 中的 <code>simple_test</code> 方法，如果是多尺度测试，则调用 <code>aug_test</code> 方法</li><li>最终调用的是每个具体算法 Head 模块的 <code>simple_test</code> 或者 <code>aug_test</code> 方法</li></ol><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h2><p>本文基于第一篇解读文章，详细地从三个层面全面解读了 MMDetection 框架，希望读者读完本文，能够对 MMDetection 框架设计思想、组件间关系和整体代码实现流程了然于心。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>轻松掌握 MMDetection 整体构建流程(一)</title>
      <link href="/2022/04/02/mmdetection-xue-xi-bi-ji-1-zheng-ti-gou-jian-liu-cheng/"/>
      <url>/2022/04/02/mmdetection-xue-xi-bi-ji-1-zheng-ti-gou-jian-liu-cheng/</url>
      
        <content type="html"><![CDATA[<h1 id="轻松掌握-MMDetection-整体构建流程-一"><a href="#轻松掌握-MMDetection-整体构建流程-一" class="headerlink" title="轻松掌握 MMDetection 整体构建流程(一)"></a>轻松掌握 MMDetection 整体构建流程(一)</h1><h2 id="0-涉及内容"><a href="#0-涉及内容" class="headerlink" title="0 涉及内容"></a>0 涉及内容</h2><ul><li>MMDetection整体构建流程与思想</li><li>目标检测算法核心组件划分</li><li>目标检测核心组件功能</li></ul><h2 id="1-目标检测算法抽象流程"><a href="#1-目标检测算法抽象流程" class="headerlink" title="1 目标检测算法抽象流程"></a>1 目标检测算法抽象流程</h2><p>目前目标检测算法大致归纳如下：</p><p><img src="https://pic1.zhimg.com/80/v2-23f3f33d5ed5792e7ad55e559a6798fc_720w.jpg" alt="img"></p><p>目标检测算法可以按照 3 个维度划分：</p><ul><li><p><strong>按照 stage 个数划分</strong>，常规是 one-stage 和 two-stage，但是实际上界限不是特别清晰，例如带 refine 阶段的算法 RepPoints，实际上可以认为是1.5 stage 算法，而 Cascade R-CNN 可以认为是多阶段算法，为了简单，上面图示没有划分如此细致。</p></li><li><p><strong>按照是否需要预定义 anchor 划分</strong>，常规是 anchor-based 和 anchor-free，当然也有些算法是两者混合的。</p></li><li><p><strong>按照是否采用了 transformer 结构划分</strong>，目前基于 transformer 结构的目标检测算法发展迅速，也引起了极大的关注，所以这里特意增加了这个类别的划分</p></li></ul><h2 id="2-MMDetection整体构建流程与思想"><a href="#2-MMDetection整体构建流程与思想" class="headerlink" title="2 MMDetection整体构建流程与思想"></a>2 MMDetection整体构建流程与思想</h2><p>基于目前代码实现，所有目标检测算法都按照以下流程进行划分：<img src="https://pic1.zhimg.com/80/v2-7ecc8e5e19c59a3e6682c5e3cdc34918_720w.jpg" alt="img"></p><p>上述流程对应 MMDetection 代码构建流程，理解每个组件的作用不仅仅对阅读算法源码有帮助，而且还能够快速理解新提出算法对应的改进部分。下面对每个模块进行详细解读。</p><h3 id="2-1-训练核心组件"><a href="#2-1-训练核心组件" class="headerlink" title="2.1 训练核心组件"></a>2.1 训练核心组件</h3><p>训练部分一般包括 9 个核心组件，总体流程是：</p><ol><li>任何一个 batch 的图片先输入到 <strong>backbone 中进行特征提取</strong>，典型的骨干网络是 <strong>ResNet</strong>。</li><li>输出的单尺度或者多尺度特征图输入到 <strong>neck 模块中进行特征融合或者增强</strong>，典型的 neck 是 <strong>FPN</strong>。</li><li>上述多尺度特征最终输入到 <strong>head 部分</strong>，一般都会包括<strong>分类和回归</strong>分支输出。</li><li>在整个<strong>网络构建阶段</strong>都可以引入一些即插即用<strong>增强算子</strong>来增加提取提取能力，典型的例如 <strong>SPP、DCN</strong> 等等。</li><li>目标检测 <strong>head 输出一般是特征图</strong>，对于分类任务存在严重的正负样本不平衡，可以通过正负样本<strong>属性分配和采样</strong>控制。</li><li>为了方便收敛和平衡多分支，一般都会<strong>对 gt bbox 进行编码</strong>。</li><li>最后一步是<strong>计算分类和回归 loss</strong>，进行训练。</li><li>在训练过程中也包括非常多的 <strong>trick</strong>，例如优化器选择等，参数调节也非常关键</li></ol><p>注意上述 9 个组件不是每个算法都需要的，下面详细分析。</p><h4 id="2-1-1-Backbone"><a href="#2-1-1-Backbone" class="headerlink" title="2.1.1 Backbone"></a>2.1.1 Backbone</h4><p><strong>backbone 作用主要是特征提取</strong>，最常用的是 ResNet 系列、ResNetV1d 系列和 Res2Net 系列。</p><p><img src="https://pic2.zhimg.com/80/v2-cdee2bd9f289d650ddbcbd748c4be0f9_720w.jpg" alt="img"></p><p>目前 MMDetection 中已经集成了大部分骨架网络，具体见文件：<code>mmdet/models/backbones</code>，V2.7 已经实现的骨架如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'RegNet'</span><span class="token punctuation">,</span> <span class="token string">'ResNet'</span><span class="token punctuation">,</span> <span class="token string">'ResNetV1d'</span><span class="token punctuation">,</span> <span class="token string">'ResNeXt'</span><span class="token punctuation">,</span> <span class="token string">'SSDVGG'</span><span class="token punctuation">,</span> <span class="token string">'HRNet'</span><span class="token punctuation">,</span> <span class="token string">'Res2Net'</span><span class="token punctuation">,</span>    <span class="token string">'HourglassNet'</span><span class="token punctuation">,</span> <span class="token string">'DetectoRS_ResNet'</span><span class="token punctuation">,</span> <span class="token string">'DetectoRS_ResNeXt'</span><span class="token punctuation">,</span> <span class="token string">'Darknet'</span><span class="token punctuation">,</span>    <span class="token string">'ResNeSt'</span><span class="token punctuation">,</span> <span class="token string">'TridentResNet'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果需要对骨架进行扩展，可以继承上述网络，然后通过<strong>注册器机制</strong>注册使用。一个典型用法为：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 骨架的预训练权重路径</span>pretrained<span class="token operator">=</span><span class="token string">'torchvision://resnet50'</span><span class="token punctuation">,</span>backbone<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'ResNet'</span><span class="token punctuation">,</span> <span class="token comment"># 骨架类名，后面的参数都是该类的初始化参数</span>    depth<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span>    num_stages<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    out_indices<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    frozen_stages<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>    norm_cfg<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'BN'</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>     norm_eval<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    style<span class="token operator">=</span><span class="token string">'pytorch'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>通过 MMCV 中的注册器机制，你可以通过 dict 形式的配置来实例化任何已经注册的类</strong>，非常方便和灵活。</p><h4 id="2-1-2-Neck"><a href="#2-1-2-Neck" class="headerlink" title="2.1.2 Neck"></a>2.1.2 Neck</h4><p>neck 可以认为<strong>是 backbone 和 head 的连接层</strong>，主要负责对 backbone 的特征进行高效融合和增强，能够对输入的单尺度或者多尺度<strong>特征进行融合、增强输出</strong>等。</p><p><img src="https://pic1.zhimg.com/80/v2-f0975c00a32fa03a80860f9c09234bbc_720w.jpg" alt="img"></p><p>具体见文件：<code>mmdet/models/necks</code>，V2.7 已经实现的 neck 如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'FPN'</span><span class="token punctuation">,</span> <span class="token string">'BFP'</span><span class="token punctuation">,</span> <span class="token string">'ChannelMapper'</span><span class="token punctuation">,</span> <span class="token string">'HRFPN'</span><span class="token punctuation">,</span> <span class="token string">'NASFPN'</span><span class="token punctuation">,</span> <span class="token string">'FPN_CARAFE'</span><span class="token punctuation">,</span> <span class="token string">'PAFPN'</span><span class="token punctuation">,</span>    <span class="token string">'NASFCOS_FPN'</span><span class="token punctuation">,</span> <span class="token string">'RFP'</span><span class="token punctuation">,</span> <span class="token string">'YOLOV3Neck'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>最常用的应该是 FPN，一个典型用法是：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">neck<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'FPN'</span><span class="token punctuation">,</span>    in_channels<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">2048</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token comment"># 骨架多尺度特征图输出通道</span>    out_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token comment"># 增强后通道输出</span>    num_outs<span class="token operator">=</span><span class="token number">5</span><span class="token comment"># 输出num_outs个多尺度特征图</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-1-3-Head"><a href="#2-1-3-Head" class="headerlink" title="2.1.3 Head"></a>2.1.3 Head</h4><p>目标检测算法<strong>输出一般包括分类和框坐标回归</strong>两个分支，不同算法 head 模块复杂程度不一样，灵活度比较高。在网络构建方面，<strong>理解目标检测算法主要是要理解 head 模块</strong>。几乎<strong>每个算法都包括一个独立的 head</strong>。</p><p><img src="https://pic2.zhimg.com/80/v2-fdd9a6232e62c75b143153dab8ba9bc1_720w.jpg" alt="img"></p><p>MMDetection 中 head 模块又划分为 <strong>two-stage 所需的 RoIHead</strong> 和 <strong>one-stage 所需的 DenseHead</strong>，也就是说所有的 one-stage 算法的 head 模块都在<code>mmdet/models/dense_heads</code>中，而 two-stage 算法还包括额外的<code>mmdet/models/roi_heads</code>。</p><p>目前 V2.7 中已经实现的 dense_heads 包括：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'AnchorFreeHead'</span><span class="token punctuation">,</span> <span class="token string">'AnchorHead'</span><span class="token punctuation">,</span> <span class="token string">'GuidedAnchorHead'</span><span class="token punctuation">,</span> <span class="token string">'FeatureAdaption'</span><span class="token punctuation">,</span>    <span class="token string">'RPNHead'</span><span class="token punctuation">,</span> <span class="token string">'GARPNHead'</span><span class="token punctuation">,</span> <span class="token string">'RetinaHead'</span><span class="token punctuation">,</span> <span class="token string">'RetinaSepBNHead'</span><span class="token punctuation">,</span> <span class="token string">'GARetinaHead'</span><span class="token punctuation">,</span>    <span class="token string">'SSDHead'</span><span class="token punctuation">,</span> <span class="token string">'FCOSHead'</span><span class="token punctuation">,</span> <span class="token string">'RepPointsHead'</span><span class="token punctuation">,</span> <span class="token string">'FoveaHead'</span><span class="token punctuation">,</span>    <span class="token string">'FreeAnchorRetinaHead'</span><span class="token punctuation">,</span> <span class="token string">'ATSSHead'</span><span class="token punctuation">,</span> <span class="token string">'FSAFHead'</span><span class="token punctuation">,</span> <span class="token string">'NASFCOSHead'</span><span class="token punctuation">,</span>    <span class="token string">'PISARetinaHead'</span><span class="token punctuation">,</span> <span class="token string">'PISASSDHead'</span><span class="token punctuation">,</span> <span class="token string">'GFLHead'</span><span class="token punctuation">,</span> <span class="token string">'CornerHead'</span><span class="token punctuation">,</span> <span class="token string">'YOLACTHead'</span><span class="token punctuation">,</span>    <span class="token string">'YOLACTSegmHead'</span><span class="token punctuation">,</span> <span class="token string">'YOLACTProtonet'</span><span class="token punctuation">,</span> <span class="token string">'YOLOV3Head'</span><span class="token punctuation">,</span> <span class="token string">'PAAHead'</span><span class="token punctuation">,</span>    <span class="token string">'SABLRetinaHead'</span><span class="token punctuation">,</span> <span class="token string">'CentripetalHead'</span><span class="token punctuation">,</span> <span class="token string">'VFNetHead'</span><span class="token punctuation">,</span> <span class="token string">'TransformerHead'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>而 roi_heads 比较杂，就不列出了。</p><p>需要注意的是：<strong>two-stage 或者 mutli-stage 算法，会额外包括一个区域提取器 roi extractor，用于将不同大小的 RoI 特征图统一成相同大小</strong>。</p><p>虽然 head 部分的网络构建比较简单，但是由于正负样本属性定义、正负样本采样和 bbox 编解码模块都在 head 模块中进行组合调用，故 MMDetection <strong>中最复杂的模块就是 head</strong>。在最后的整体流程部分会对该模块进行详细分析。</p><h4 id="2-1-4-Enhance"><a href="#2-1-4-Enhance" class="headerlink" title="2.1.4 Enhance"></a>2.1.4 Enhance</h4><p>enhance 是<strong>即插即用、能够对特征进行增强的模块</strong>，其具体代码可以<strong>通过 dict 形式注册到 backbone、neck 和 head 中</strong>，非常方便(目前还不完善)。常用的 enhance 模块是 <strong>SPP、ASPP、RFB、Dropout、Dropblock、DCN 和各种注意力模块</strong> SeNet、Non_Local、CBA 等。目前 MMDetection 中部分模块支持 enhance 的接入，例如 ResNet 骨架中的 plugins，这个部分的解读放在具体算法模块中讲解。</p><p><img src="https://pic3.zhimg.com/80/v2-65a706efe224f0b7ffc7f4fd7a65f2ca_720w.jpg" alt="img"></p><h4 id="2-1-5-BBox-Assigner"><a href="#2-1-5-BBox-Assigner" class="headerlink" title="2.1.5 BBox Assigner"></a>2.1.5 BBox Assigner</h4><p><strong>正负样本属性分配模块</strong>作用是<strong>进行正负样本定义或者正负样本分配</strong>（可能也包括忽略样本定义），正样本就是常说的前景样本（可以是任何类别），负样本就是背景样本。因为目标检测是一个同时进行分类和回归的问题，对于分类场景必然需要确定正负样本，否则无法训练。该模块至关重要，不同的正负样本分配策略会带来显著的性能差异，目前大部分目标检测算法都会对这个部分进行改进，至关重要。一些典型的分配策略如下：</p><p><img src="https://pic3.zhimg.com/80/v2-12bae70e2ea2e4afb05d0d8d3f38ca56_720w.jpg" alt="img"></p><p>对应的代码在<code>mmdet/core/bbox/assigners</code>中，V2.7 主要包括：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'BaseAssigner'</span><span class="token punctuation">,</span> <span class="token string">'MaxIoUAssigner'</span><span class="token punctuation">,</span> <span class="token string">'ApproxMaxIoUAssigner'</span><span class="token punctuation">,</span>     <span class="token string">'PointAssigner'</span><span class="token punctuation">,</span> <span class="token string">'ATSSAssigner'</span><span class="token punctuation">,</span> <span class="token string">'CenterRegionAssigner'</span><span class="token punctuation">,</span> <span class="token string">'GridAssigner'</span><span class="token punctuation">,</span>    <span class="token string">'HungarianAssigner'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-1-7-BBox-Encoder"><a href="#2-1-7-BBox-Encoder" class="headerlink" title="2.1.7 BBox Encoder"></a>2.1.7 BBox Encoder</h4><p>为了<strong>更好的收敛和平衡多个 loss</strong>，具体解决办法非常多，而 bbox 编解码策略也算其中一个，bbox <strong>编码阶段</strong>对应的是<strong>对正样本的 gt bbox 采用某种编码变换</strong>（反操作就是 bbox 解码），最简单的编码是对 gt bbox 除以图片宽高进行归一化以平衡分类和回归分支，一些典型的编解码策略如下：</p><p><img src="https://pic4.zhimg.com/80/v2-1f8d5e5e45886423df474d168452f50b_720w.jpg" alt="img"></p><p>对应的代码在<code>mmdet/core/bbox/coder</code>中，V2.7 主要包括：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'BaseBBoxCoder'</span><span class="token punctuation">,</span> <span class="token string">'PseudoBBoxCoder'</span><span class="token punctuation">,</span> <span class="token string">'DeltaXYWHBBoxCoder'</span><span class="token punctuation">,</span>    <span class="token string">'LegacyDeltaXYWHBBoxCoder'</span><span class="token punctuation">,</span> <span class="token string">'TBLRBBoxCoder'</span><span class="token punctuation">,</span> <span class="token string">'YOLOBBoxCoder'</span><span class="token punctuation">,</span>    <span class="token string">'BucketingBBoxCoder'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-1-8-Loss"><a href="#2-1-8-Loss" class="headerlink" title="2.1.8 Loss"></a>2.1.8 Loss</h4><p>Loss 通常都分为<strong>分类和回归 loss</strong>，其<strong>对网络 head 输出的预测值和 bbox encoder 得到的 targets 进行梯度下降</strong>迭代训练。</p><p>loss 的设计也是各大算法重点改进对象，常用的 loss 如下：</p><p><img src="https://pic4.zhimg.com/80/v2-686b0b9ac6a82f9945ae454d18783227_720w.jpg" alt="img"></p><p>对应的代码在<code>mmdet/models/losses</code>中，V2.7 主要包括：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'cross_entropy'</span><span class="token punctuation">,</span> <span class="token string">'binary_cross_entropy'</span><span class="token punctuation">,</span>    <span class="token string">'mask_cross_entropy'</span><span class="token punctuation">,</span> <span class="token string">'CrossEntropyLoss'</span><span class="token punctuation">,</span> <span class="token string">'sigmoid_focal_loss'</span><span class="token punctuation">,</span>    <span class="token string">'FocalLoss'</span><span class="token punctuation">,</span> <span class="token string">'smooth_l1_loss'</span><span class="token punctuation">,</span> <span class="token string">'SmoothL1Loss'</span><span class="token punctuation">,</span> <span class="token string">'balanced_l1_loss'</span><span class="token punctuation">,</span>    <span class="token string">'BalancedL1Loss'</span><span class="token punctuation">,</span> <span class="token string">'mse_loss'</span><span class="token punctuation">,</span> <span class="token string">'MSELoss'</span><span class="token punctuation">,</span> <span class="token string">'iou_loss'</span><span class="token punctuation">,</span> <span class="token string">'bounded_iou_loss'</span><span class="token punctuation">,</span>    <span class="token string">'IoULoss'</span><span class="token punctuation">,</span> <span class="token string">'BoundedIoULoss'</span><span class="token punctuation">,</span> <span class="token string">'GIoULoss'</span><span class="token punctuation">,</span> <span class="token string">'DIoULoss'</span><span class="token punctuation">,</span> <span class="token string">'CIoULoss'</span><span class="token punctuation">,</span> <span class="token string">'GHMC'</span><span class="token punctuation">,</span>    <span class="token string">'GHMR'</span><span class="token punctuation">,</span> <span class="token string">'reduce_loss'</span><span class="token punctuation">,</span> <span class="token string">'weight_reduce_loss'</span><span class="token punctuation">,</span> <span class="token string">'weighted_loss'</span><span class="token punctuation">,</span> <span class="token string">'L1Loss'</span><span class="token punctuation">,</span>    <span class="token string">'l1_loss'</span><span class="token punctuation">,</span> <span class="token string">'isr_p'</span><span class="token punctuation">,</span> <span class="token string">'carl_loss'</span><span class="token punctuation">,</span> <span class="token string">'AssociativeEmbeddingLoss'</span><span class="token punctuation">,</span>    <span class="token string">'GaussianFocalLoss'</span><span class="token punctuation">,</span> <span class="token string">'QualityFocalLoss'</span><span class="token punctuation">,</span> <span class="token string">'DistributionFocalLoss'</span><span class="token punctuation">,</span>    <span class="token string">'VarifocalLoss'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看出 MMDetection 中已经实现了非常多的 loss，可以直接使用。</p><h4 id="2-1-9-Training-tricks"><a href="#2-1-9-Training-tricks" class="headerlink" title="2.1.9 Training tricks"></a>2.1.9 Training tricks</h4><p>训练技巧非常多，常说的调参很大一部分工作都是在设置这部分超参。这部分内容比较杂乱，很难做到完全统一，目前主流的 tricks 如下所示:</p><p><img src="https://pic3.zhimg.com/80/v2-569a12b6d4a20f8619a27b48d5b2fa42_720w.jpg" alt="img"></p><p>MMDetection 目前这部分还会继续完善。</p><h3 id="2-2-测试核心组件"><a href="#2-2-测试核心组件" class="headerlink" title="2.2 测试核心组件"></a>2.2 测试核心组件</h3><p>测试核心组件和训练非常类似，但是简单很多，除了必备的网络构建部分外( <strong>backbone、neck、head 和 enhance</strong> )，不需要正负样本定义、正负样本采样和 loss 计算三个最难的部分，但是其<strong>额外需要一个 bbox 后处理模块</strong>和<strong>测试 trick</strong>。</p><h4 id="2-2-1-BBox-Decoder"><a href="#2-2-1-BBox-Decoder" class="headerlink" title="2.2.1 BBox Decoder"></a>2.2.1 BBox Decoder</h4><p><strong>训练时候进行了编码，那么对应的测试环节需要进行解码。</strong>根据编码的不同，解码也是不同的。举个简单例子：假设训练时候对宽高是直接除以图片宽高进行归一化的，那么解码过程也仅仅需要乘以图片宽高即可。其代码和 bbox encoder 放在一起，在<code>mmdet/core/bbox/coder</code>中。</p><h4 id="2-2-2-BBox-PostProcess"><a href="#2-2-2-BBox-PostProcess" class="headerlink" title="2.2.2 BBox PostProcess"></a>2.2.2 BBox PostProcess</h4><p>在<strong>得到原图尺度 bbox 后，由于可能会出现重叠 bbox 现象</strong>，故一般都需要进行后处理，最常用的后处理就是<strong>非极大值抑制以及其变种</strong>。</p><p>其对应的文件在<code>mmdet/core/post_processing</code>中，V2.7 主要包括：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'multiclass_nms'</span><span class="token punctuation">,</span> <span class="token string">'merge_aug_proposals'</span><span class="token punctuation">,</span> <span class="token string">'merge_aug_bboxes'</span><span class="token punctuation">,</span>    <span class="token string">'merge_aug_scores'</span><span class="token punctuation">,</span> <span class="token string">'merge_aug_masks'</span><span class="token punctuation">,</span> <span class="token string">'fast_nms'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-2-3-Testing-tricks"><a href="#2-2-3-Testing-tricks" class="headerlink" title="2.2.3 Testing tricks"></a>2.2.3 Testing tricks</h4><p>为了提高检测性能，测试阶段也会采用 trick。这个阶段的 tricks 也非常多，难以完全统一，最典型的是<strong>多尺度测试</strong>以及<strong>各种模型集成手段</strong>，典型配置如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'MultiScaleFlipAug'</span><span class="token punctuation">,</span>    img_scale<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1333</span><span class="token punctuation">,</span> <span class="token number">800</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    flip<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    transforms<span class="token operator">=</span><span class="token punctuation">[</span>        <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'Resize'</span><span class="token punctuation">,</span> keep_ratio<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'RandomFlip'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'Normalize'</span><span class="token punctuation">,</span> <span class="token operator">**</span>img_norm_cfg<span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'Pad'</span><span class="token punctuation">,</span> size_divisor<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'ImageToTensor'</span><span class="token punctuation">,</span> keys<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'img'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'Collect'</span><span class="token punctuation">,</span> keys<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'img'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://pic3.zhimg.com/80/v2-16e307727f0c3e941ec72c21f214b982_720w.jpg" alt="img"></p><h3 id="2-3-训练测试算法流程"><a href="#2-3-训练测试算法流程" class="headerlink" title="2.3 训练测试算法流程"></a>2.3 训练测试算法流程</h3><p>在分析完每个训练流程的各个核心组件后，为了方便大家理解整个算法构建，下面分析 MMDetection 是<strong>如何组合各个组件进行训练的</strong>，这里<strong>以 one-stage 检测器为例</strong>，two-stage 也比较类似。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SingleStageDetector</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 构建backbone、neck和head</span>        self<span class="token punctuation">.</span>backbone <span class="token operator">=</span> build_backbone<span class="token punctuation">(</span>backbone<span class="token punctuation">)</span>        <span class="token keyword">if</span> neck <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>neck <span class="token operator">=</span> build_neck<span class="token punctuation">(</span>neck<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bbox_head <span class="token operator">=</span> build_head<span class="token punctuation">(</span>bbox_head<span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward_train</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token comment"># 先运行backbone+neck进行特征提取</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>extract_feat<span class="token punctuation">(</span>img<span class="token punctuation">)</span>        <span class="token comment"># 对head进行forward train，输出loss</span>        losses <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">.</span>forward_train<span class="token punctuation">(</span>            x<span class="token punctuation">,</span>             img_metas<span class="token punctuation">,</span>             gt_bboxes<span class="token punctuation">,</span>                                        gt_labels<span class="token punctuation">,</span>             gt_bboxes_ignore        <span class="token punctuation">)</span>        <span class="token keyword">return</span> losses  <span class="token keyword">def</span> <span class="token function">simple_test</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 先运行backbone+neck进行特征提取</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>extract_feat<span class="token punctuation">(</span>img<span class="token punctuation">)</span>        <span class="token comment"># head输出预测特征图</span>        outs <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment"># bbox解码和还原</span>        bbox_list <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">.</span>get_bboxes<span class="token punctuation">(</span>            <span class="token operator">*</span>outs<span class="token punctuation">,</span>             img_metas<span class="token punctuation">,</span>             rescale<span class="token operator">=</span>rescale        <span class="token punctuation">)</span>        <span class="token comment"># 重组结果返回</span>        bbox_results <span class="token operator">=</span> <span class="token punctuation">[</span>            bbox2result<span class="token punctuation">(</span>                det_bboxes<span class="token punctuation">,</span>                 det_labels<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">.</span>num_classes            <span class="token punctuation">)</span>            <span class="token keyword">for</span> det_bboxes<span class="token punctuation">,</span> det_labels <span class="token keyword">in</span> bbox_list        <span class="token punctuation">]</span>        <span class="token keyword">return</span> bbox_results<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>以上就是整个检测器算法训练和测试最简逻辑，可以发现训练部分最核心的就是<code>bbox_head.forward_train</code>，测试部分最核心的是<code>bbox_head.get_bboxes</code>，下面单独简要分析。</p><h4 id="2-3-1-bbox-head-forward-train"><a href="#2-3-1-bbox-head-forward-train" class="headerlink" title="2.3.1 bbox_head.forward_train"></a>2.3.1 bbox_head.forward_train</h4><p>forward_train 是通用函数，如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward_train</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 调用每个head自身的forward方法</span>    outs <span class="token operator">=</span> self<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token keyword">if</span> gt_labels <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>        loss_inputs <span class="token operator">=</span> outs <span class="token operator">+</span> <span class="token punctuation">(</span>gt_bboxes<span class="token punctuation">,</span> img_metas<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        loss_inputs <span class="token operator">=</span> outs <span class="token operator">+</span> <span class="token punctuation">(</span>gt_bboxes<span class="token punctuation">,</span> gt_labels<span class="token punctuation">,</span> img_metas<span class="token punctuation">)</span>    <span class="token comment"># 计算每个head自身的loss方法</span>    losses <span class="token operator">=</span> self<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>        <span class="token operator">*</span>loss_inputs<span class="token punctuation">,</span>         gt_bboxes_ignore<span class="token operator">=</span>gt_bboxes_ignore    <span class="token punctuation">)</span>    <span class="token comment"># 返回</span>    <span class="token keyword">return</span> losses<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对于不同的 head，虽然 forward 内容不一样，但是依然可以抽象为： <code>outs = self(x)</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> feats<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token comment"># 多尺度特征图，一个一个迭代进行forward_single</span>   <span class="token keyword">return</span> multi_apply<span class="token punctuation">(</span>self<span class="token punctuation">.</span>forward_single<span class="token punctuation">,</span> feats<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">forward_single</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token comment"># 运行各个head独特的head forward方法，得到预测图</span>   <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>   <span class="token keyword">return</span> cls_score<span class="token punctuation">,</span> bbox_pred<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>而对于不同的 head，其 loss 计算部分也比较复杂，可以简单抽象为：<code>losses = self.loss(...)</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">loss</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 1 生成anchor-base需要的anchor或者anchor-free需要的points</span>    <span class="token comment"># 2 利用gt bbox对特征图或者anchor计算其正负和忽略样本属性</span>    <span class="token comment"># 3 进行正负样本采样</span>    <span class="token comment"># 4 对gt bbox进行bbox编码</span>    <span class="token comment"># 5 loss计算，并返回</span>    <span class="token keyword">return</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>loss_cls<span class="token operator">=</span>losses_cls<span class="token punctuation">,</span> loss_bbox<span class="token operator">=</span>losses_bbox<span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-3-2-bbox-head-get-bboxes"><a href="#2-3-2-bbox-head-get-bboxes" class="headerlink" title="2.3.2 bbox_head.get_bboxes"></a>2.3.2 bbox_head.get_bboxes</h4><p>get_bboxes函数更加简单</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_bboxes</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token comment"># 1 生成anchor-base需要的anchor或者anchor-free需要的points</span>   <span class="token comment"># 2 遍历每个输出层，遍历batch内部的每张图片，对每张图片先提取指定个数的预测结果，缓解后面后处理压力；对保留的位置进行bbox解码和还原到原图尺度</span>   <span class="token comment"># 3 统一nms后处理</span>   <span class="token keyword">return</span> det_bboxes<span class="token punctuation">,</span> det_labels<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h2><p>本文重点分析了一个目标检测器是如何通过多个核心组件堆叠而成，不涉及具体代码，大家只需要总体把握即可，其中最应该了解的是：<strong>任何一个目标检测算法都可以分成 n 个核心组件，组件和组件之间是隔离的，方便复用和设计</strong>。当面对一个新算法时候我们可以先分析其主要是改进了哪几个核心组件，然后就可以高效的掌握该算法。</p><p>另外还有一些重要的模块没有分析，特别是 dataset、dataloader 和分布式训练相关的检测代码，由于篇幅限制就不介绍了，如有需要欢迎在评论区留言。</p><p>再次欢迎大家使用 MMDetection，也非常欢迎社区贡献！</p><p>最后附上总图：</p><p><img src="https://pic3.zhimg.com/80/v2-c4e6229a1fd42692d090108481be34a6_720w.jpg" alt="img"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>智能视频感知与分析工作室招新考核题目</title>
      <link href="/2022/03/18/zhao-xin-kao-he/"/>
      <url>/2022/03/18/zhao-xin-kao-he/</url>
      
        <content type="html"><![CDATA[<h1 id="智能视频感知与分析工作室招新考核题目"><a href="#智能视频感知与分析工作室招新考核题目" class="headerlink" title="智能视频感知与分析工作室招新考核题目"></a>智能视频感知与分析工作室招新考核题目</h1><h2 id="时间：2022-x2F-03-x2F-19-2022-x2F-03-x2F-26"><a href="#时间：2022-x2F-03-x2F-19-2022-x2F-03-x2F-26" class="headerlink" title="时间：2022/03/19 ~ 2022/03/26"></a>时间：2022/03/19 ~ 2022/03/26</h2><p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20220318222607061.png" alt="image-20220318222607061"></p><hr><h2 id="技术类（算法设计组、轻量化及产品设计组）"><a href="#技术类（算法设计组、轻量化及产品设计组）" class="headerlink" title="技术类（算法设计组、轻量化及产品设计组）"></a>技术类（算法设计组、轻量化及产品设计组）</h2><p><strong>组队题目要求：</strong></p><ul><li><strong>自由组队，每队<code>1-3</code>人，分工明确；</strong></li><li><strong>由题目清单选择题目，亦可自选题目；</strong></li></ul><p><strong>项目规范要求：</strong></p><ul><li>需要程序实现，制作PPT并展示（5-10分钟展示时间）；</li><li>上传至<code>Github</code>并提交链接（如自选题目涉及自有知识产权，可不提供）；</li><li>提交实验结果录屏演示；</li><li>工程结构规范、注释丰富；</li><li>必须包含<code>Readme.md</code>说明文件；</li><li>语言不限；</li><li>如能将实现过程进行记录汇总为Blog可作为加分项。</li></ul><p><strong>题目清单：</strong></p><ol><li>基于U-Net的图像分割；包括：搭建U-Net网络结构、实验数据处理、网络的训练与测试。（<a href="https://drive.google.com/file/d/1OQBErLlZ-xNGz5PsoOaJwjG9E-N4dcbE/view?usp=sharing">数据集链接</a>）提示：很多博客都有类似讲解，复现难度不大，数据集很小（30训练+30测试），CPU也可训练。不要求准确率，自主编写代码实现即可。</li><li>基于OpenCV的实时人脸识别（可直接调用OpenCV人脸检测器，但所使用原理方法需要阐述清楚）；</li><li>基于Dlib的人脸识别（可直接调用，阐述其与OpenCV人脸检测器方法的原理及实现区别）；</li><li>基于Yolo的图像目标检测技术（不要求训练，只需要利用官方给定权重预测图像即可）；</li><li>LeNet手写体字符识别；</li><li>基于OpenMP、MPI、CUDA或OneAPI（四选一、或结合）的卷积操作加速（C\C++实现可优先考虑）；</li><li>基于嵌入式设备（例如有树莓派等，可做）目标检测或图片分类；</li><li>机械设计制造、实体模型设计、3D打印等（可自选的相关内容）；</li><li>复现任一较新的深度学习方法（可自选的相关内容）。</li></ol><h2 id="非技术类（策划管理组及UI交互设计组）"><a href="#非技术类（策划管理组及UI交互设计组）" class="headerlink" title="非技术类（策划管理组及UI交互设计组）"></a>非技术类（策划管理组及UI交互设计组）</h2><p><strong>要求：</strong></p><ul><li><strong>自由组队，每队<code>1-2</code>人；</strong></li><li><strong>从以下题目任选其一，可撰写策划案、制作PPT或图形化界面，亦可自选主题；</strong></li><li><strong>制作并提交项目计划书(提交PPT、或设计草图等)。</strong></li></ul><p><strong>题目清单</strong></p><ol><li><p><code>策划</code>调研国内外智能监控视频分析系统的发展趋势，分析各系统不足与优势，根据调研结果，制作不少于10页的PPT（项目计划书格式），拟定未来基本研究方向（要求：提交PPT或其他策划资料）。</p></li><li><p><code>策划</code>调研校园暴力事件识别系统的潜在应用场景，深入剖析行业痛点需求，给出分析思考，结合相应技术进行分析更佳，提交PPT及其他材料；</p></li><li><p><code>策划</code>策划结合人工智能技术，剖析身边技术方案、产品等方面的需求，给出整体性的策划方法，明确需求、市场、技术背景等要素。</p></li><li><p><code>GUI</code>复现QT-GUI项目PyDracula，进行自定义美化编辑，并在窗口内实现：读取本地视频进行播放，在窗口可控制视频的播放与暂停。<a href="https://github.com/Wanderson-Magalhaes/Modern_GUI_PyDracula_PySide6_or_PyQt6">项目地址</a>（要求：提交UI运行视频与代码github链接）</p></li><li><p><code>设计</code>独立设计的任一作品（例如图片、海报等，自有版权的记得加水印等措施），阐明设计思想，设计理念；</p></li></ol><hr><h2 id="组队及报名地址"><a href="#组队及报名地址" class="headerlink" title="组队及报名地址"></a>组队及报名地址</h2><h3 id="智能视频感知与分析工作室招新考核登记表"><a href="#智能视频感知与分析工作室招新考核登记表" class="headerlink" title="智能视频感知与分析工作室招新考核登记表"></a><a href="https://docs.qq.com/sheet/DWGxweHlkVldZQ1RE">智能视频感知与分析工作室招新考核登记表</a></h3>]]></content>
      
      
      
        <tags>
            
            <tag> 考核题目 </tag>
            
            <tag> 智能视频感知与分析工作室 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/03/15/hello-world/"/>
      <url>/2022/03/15/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span>$ hexo server$ hexo server$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ZerotTier内网穿透+VSCode配置远程开发+tmux基础操作</title>
      <link href="/2021/11/11/zerotier-nei-wang-chuan-tou-vscode-pei-zhi-yuan-cheng-kai-fa-tmux-ji-chu-cao-zuo/"/>
      <url>/2021/11/11/zerotier-nei-wang-chuan-tou-vscode-pei-zhi-yuan-cheng-kai-fa-tmux-ji-chu-cao-zuo/</url>
      
        <content type="html"><![CDATA[<h1 id="ZerotTier内网穿透-VSCode配置远程开发-tmux基础操作"><a href="#ZerotTier内网穿透-VSCode配置远程开发-tmux基础操作" class="headerlink" title="ZerotTier内网穿透+VSCode配置远程开发+tmux基础操作"></a>ZerotTier内网穿透+VSCode配置远程开发+tmux基础操作</h1><ul><li>平时在开发深度学习等相关项目时，往往需要大型服务器或工作站的支持，远程开发时使用向日葵等远程桌面软件往往不是那么明知，多人使用冲突、限速卡顿，代码体验极其**。</li><li>那么有没有更好的解决方案呢？单就小型团队而言，如果能将所有散布在各个实验室的机器使用内网穿透统一接入同一局域网，便可方便的使用ssh连接，搭配<code>vscode</code>的<code>ssh-remote</code>插件与<code>tmux</code>进行终端复用，便可实现本地无感的远程开发。</li></ul><h2 id="1-ZeroTier配置内网穿透"><a href="#1-ZeroTier配置内网穿透" class="headerlink" title="1. ZeroTier配置内网穿透"></a>1. ZeroTier配置内网穿透</h2><h3 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1 简介"></a>1.1 简介</h3><p><a href="https://www.zerotier.com/">ZeroTier </a>作为一款非常简单易用的内网穿透工具，<strong>不需要复杂配置</strong>，就能实现虚拟局域网的组建，让你可以在外也能连回实验室的NAS、服务器获取数据、远程开发。</p><h3 id="1-2-费用"><a href="#1-2-费用" class="headerlink" title="1.2 费用"></a>1.2 费用</h3><p>免费网络限制 100 台设备，超过了就要付费。100 台对于个人或者小团队使用来说都足够了。</p><h3 id="1-3-支持平台"><a href="#1-3-支持平台" class="headerlink" title="1.3 支持平台"></a>1.3 <a href="https://www.zerotier.com/download/">支持平台</a></h3><p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024133645939.png" alt="image-20211024133645939"></p><h3 id="1-4-使用步骤"><a href="#1-4-使用步骤" class="headerlink" title="1.4 使用步骤"></a>1.4 使用步骤</h3><ul><li>说明：如已有Network ID，直接执行步骤3，安装<a href="https://www.zerotier.com/download/">客户端</a>，加入Network ID即可</li></ul><ol><li><p>注册ZeroTier ID：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024141003246.png" alt="image-20211024141003246" style="zoom:50%;"></li><li><p>创建私有局域网，得到Network ID与子网地址：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024135727789.png" alt="image-20211024135727789" style="zoom:80%;"></li><li><p>安装<a href="https://www.zerotier.com/download/">客户端</a>，<strong>加入Network ID</strong></p><ul><li>windows下<code>ipconfig</code>，ubuntu下<code>ifconfig</code>出现ZeroTier的网段后说明连接成功（也可直接ping其他ip验证）：</li></ul><p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024140430169.png" alt="image-20211024140430169"></p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024143314079.png" alt="image-20211024143314079" style="zoom: 80%;"></li><li><p>管理：</p><ul><li>可以选择子网地址：</li></ul><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024135944372.png" alt="image-20211024135944372" style="zoom: 67%;"><ul><li><p>查看连接客户端，第三列机器即为所有机器的局域网IP，接下来的步骤即shh该ip地址：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024140138785.png" alt="image-20211024140138785" style="zoom: 50%;"></li></ul></li></ol><hr><h2 id="2-VSCode配置Remote-SSH插件"><a href="#2-VSCode配置Remote-SSH插件" class="headerlink" title="2. VSCode配置Remote-SSH插件"></a>2. VSCode配置Remote-SSH插件</h2><h3 id="2-1-安装OpenSSH"><a href="#2-1-安装OpenSSH" class="headerlink" title="2.1 安装OpenSSH"></a>2.1 安装OpenSSH</h3><ul><li><p>Remote-SSH插件是基于SHH的，所以首先要确保本机和远程服务器都安装好了OpenSHH</p></li><li><p>Ubuntu：</p><p>ubuntu默认并没有安装ssh服务，如果通过ssh链接ubuntu，需要自己手动安装ssh-server。判断是否安装ssh服务，可以通过如下命令进行：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">ssh</span> localhost   ssh: connect to <span class="token function">host</span> localhost port <span class="token number">22</span>: Connection refused   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>若出现上述情况，表示还没有安装，可通过以下命令安装：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> openssh-server<span class="token function">sudo</span> /etc/init.d/ssh start<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>安装启动后，可以通过如下命令查看服务是否正确启动：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">ps</span> -e<span class="token operator">|</span><span class="token function">grep</span> <span class="token function">ssh</span><span class="token number">6212</span> ?        00:00:00 sshd  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>如上表示安装成功</p></li><li><p>Windows：</p><p>Win10现在已经支持OpenSSH，可在设置-&gt;应用-&gt;可选功能中查看：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024143529860.png" alt="image-20211024143529860" style="zoom:67%;"><p>若未安装，直接使用添加功能-&gt;搜索SSH安装即可，其他安装方式可参考<a href="https://www.jianshu.com/p/f8ba3e51d60e">Windows安装OpenSSH支持SSH - 简书 (jianshu.com)</a></p></li><li><p>Mac OS：</p><p>Mac OS X系统已经默认安装了SSH，但是SSH服务并未启用，启用SSH服务的方法：</p><p>系统偏好设置-&gt;共享-&gt;勾选“远程登陆”：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024143911895.png" alt="image-20211024143911895" style="zoom:67%;"></li><li><p>验证ssh：</p><p>使用：<code>ssh &lt;username&gt;@&lt;ip&gt;</code>连接任意主机，输入密码连接成功即可：</p><p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024144324785.png" alt="image-20211024144324785"></p></li></ul><h3 id="2-2-配置Remote-SSH插件"><a href="#2-2-配置Remote-SSH插件" class="headerlink" title="2.2 配置Remote-SSH插件"></a>2.2 配置Remote-SSH插件</h3><ul><li><p>安装Remote-SSH：</p><p>在vscode的拓展商店中搜索Remote-SSH进行安装，安装完成后左侧会出现以下按钮：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024145412604.png" alt="image-20211024145412604" style="zoom:50%;"></li><li><p>配置config文件：</p><p>进入该拓展，点击SSH TARGETS上面的设置按钮，选择所要配置的ssh config文件（一般为第一个）：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024145620858.png" alt="image-20211024145620858" style="zoom:67%;"><p>配置远程服务器的名称、ip与用户名：</p><ul><li><p><code>Host</code>: 主机的自定义显示名，可以随便起</p></li><li><p><code>HostName</code>: 登录远程主机的内网IP，即1.4中主机内网穿透后得到的虚拟IP</p></li><li><p><code>User</code>: 登录远程主机的用户名</p></li><li><p><code>Port</code>: 用于登录远程主机的端口（可选）</p></li><li><p><code>IdentityFile</code>: 本地的id_rsa的路径（用于免密登陆的私钥）（多人使用不推荐配置私钥免密）（可选）</p></li></ul></li></ul><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024150620523.png" alt="image-20211024150620523" style="zoom:80%;"><ul><li>远程连接测试：</li></ul><p>​    配置完成后，该窗口下会出现所配置的主机，可以在新窗口下进行连接：</p><p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024151155738.png" alt="image-20211024151155738"></p><p>（第一次连接需要选择服务器操作系统）-&gt; 输入密码-&gt;等待服务器安装vscode远程端-&gt;打开远程项目文件夹后即可开始使用，所有的使用均和本地使用无任何差异：    </p><p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024151750787.png" alt="image-20211024151750787"></p><hr><h2 id="3-使用Tmux保证会话持续运行"><a href="#3-使用Tmux保证会话持续运行" class="headerlink" title="3.  使用Tmux保证会话持续运行"></a>3.  使用Tmux保证会话持续运行</h2><h3 id="3-1-Tmux简介"><a href="#3-1-Tmux简介" class="headerlink" title="3.1 Tmux简介"></a>3.1 Tmux简介</h3><ul><li><strong>目的：****避免训练过程中因为本地Terminal关闭后服务器上的进程也被关闭。</strong></li></ul><h4 id="为什么需要终端复用？"><a href="#为什么需要终端复用？" class="headerlink" title="为什么需要终端复用？"></a><a href="https://www.ruanyifeng.com/blog/2019/10/tmux.html">为什么需要终端复用</a>？</h4><blockquote><p>命令行的典型使用方式是，打开一个终端窗口（terminal window，以下简称”窗口”），在里面输入命令。<strong>用户与计算机的这种临时的交互，称为一次”会话”（session）</strong> 。</p></blockquote><blockquote><p>会话的一个重要特点是，窗口与其中启动的进程是连在一起的。打开窗口，会话开始；关闭窗口，会话结束，会话内部的进程也会随之终止，不管有没有运行完。</p></blockquote><blockquote><p>一个典型的例子就是，SSH 登录远程计算机，打开一个远程窗口执行命令。这时，网络突然断线，再次登录的时候，是找不回上一次执行的命令的。因为上一次 SSH 会话已经终止了，里面的进程也随之消失了。</p><p>为了解决这个问题，会话与窗口可以”解绑”：窗口关闭时，会话并不终止，而是继续运行，等到以后需要的时候，再让会话”绑定”其他窗口。</p></blockquote><h4 id="Tmux-终端复用的作用？"><a href="#Tmux-终端复用的作用？" class="headerlink" title="Tmux 终端复用的作用？"></a>Tmux 终端复用的作用？</h4><p>（1）它允许在单个窗口中，同时访问多个会话。这对于同时运行多个命令行程序很有用。</p><p>（2） 它可以让新窗口”接入”已经存在的会话。</p><p>（3）它允许每个会话有多个连接窗口，因此可以多人实时共享会话。</p><p>（4）它还支持窗口任意的垂直和水平拆分。</p><h3 id="3-2-Tmux的安装"><a href="#3-2-Tmux的安装" class="headerlink" title="3.2 Tmux的安装"></a>3.2 Tmux的安装</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># Ubuntu 或 Debian</span>$ <span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> tmux<span class="token comment"># CentOS 或 Fedora</span>$ <span class="token function">sudo</span> yum <span class="token function">install</span> tmux<span class="token comment"># Mac</span>$ brew <span class="token function">install</span> tmux<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-3-Tmux使用常用命令"><a href="#3-3-Tmux使用常用命令" class="headerlink" title="3.3 Tmux使用常用命令"></a>3.3 Tmux使用常用命令</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ tmux new -s <span class="token operator">&lt;</span>窗口名<span class="token operator">&gt;</span> <span class="token comment"># 打开新窗口</span>ctrl+b d  <span class="token comment"># 分离窗口</span>$ tmux info  <span class="token comment"># 列出当前所有 Tmux 会话信息</span>$ tmux attach -t <span class="token operator">&lt;</span>窗口名<span class="token operator">&gt;</span> <span class="token comment"># 连接窗口</span>ctrl+b %<span class="token comment"># 分割窗口</span>ctrl+b s <span class="token comment"># 切换窗口</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h2 id="4-远程文件拷贝"><a href="#4-远程文件拷贝" class="headerlink" title="4. 远程文件拷贝"></a>4. 远程文件拷贝</h2><ul><li><p>直接使用以下命令即可：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ <span class="token function">scp</span> <span class="token operator">&lt;</span>本地文件路径<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>远程主机用户名<span class="token operator">&gt;</span>@<span class="token operator">&lt;</span>远程主机ip<span class="token operator">&gt;</span>:<span class="token operator">&lt;</span>要拷贝到的远程主机路径<span class="token operator">&gt;</span><span class="token comment">#或</span>$ <span class="token function">scp</span> <span class="token operator">&lt;</span>远程主机用户名<span class="token operator">&gt;</span>@<span class="token operator">&lt;</span>远程主机ip<span class="token operator">&gt;</span>:<span class="token operator">&lt;</span>要拷贝的远程主机文件路径<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>拷贝到的本地文件路径<span class="token operator">&gt;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>例如:</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ <span class="token function">scp</span> .<span class="token punctuation">\</span>labels.zip hp3090@192.168.192.164:/media/hp3090/HDD-2T/renjunjie/WSOL_RS/dataset/C45V2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 内网穿透 </tag>
            
            <tag> tmux </tag>
            
            <tag> 远程开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker使用基本说明</title>
      <link href="/2021/04/17/docker-shi-yong-ji-ben-jiao-cheng/"/>
      <url>/2021/04/17/docker-shi-yong-ji-ben-jiao-cheng/</url>
      
        <content type="html"><![CDATA[<h1 id="docker使用基本说明"><a href="#docker使用基本说明" class="headerlink" title="docker使用基本说明"></a>docker使用基本说明</h1><h2 id="1-拉取镜像"><a href="#1-拉取镜像" class="headerlink" title="1. 拉取镜像"></a>1. 拉取镜像</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">sudo docker pull xdurjj/207base:v1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="2-从镜像启动容器"><a href="#2-从镜像启动容器" class="headerlink" title="2. 从镜像启动容器"></a>2. 从镜像启动容器</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">sudo docker run --gpus all -it --name="rjj_cpp" -v /media/hp3090/HDD-2T/RJJ_FOR_C++:/home xdurjj/207base:v1 /bin/bash# --gpus all 开启所有显卡支持# -it -i(iterative) 打开标准输入 -d(daemons) 启动之后挂起,类似于后台进程 -t(tty) 分配一个伪终端# --name 容器名# -v 指定挂载路径，宿主机目录:镜像内挂载路径<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-暂时退出容器"><a href="#3-暂时退出容器" class="headerlink" title="3. 暂时退出容器"></a>3. 暂时退出容器</h2><p>docker退出而不结束容器：CTRL+P+Q，vscode环境下需要替换掉默认的ctrl+q与ctrl+p的快捷键</p><h2 id="4-容器打包为镜像"><a href="#4-容器打包为镜像" class="headerlink" title="4. 容器打包为镜像"></a>4. 容器打包为镜像</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">sudo docker commit [选项] [容器ID或容器名]  [仓库名:标签]# 例：sudo docker commit -a 'jjren' -m '安装了mmdet' 9adeb5943045  xdurjj/207base:v1# -a:修改人# -m:备注<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="5-列出所有镜像与容器"><a href="#5-列出所有镜像与容器" class="headerlink" title="5. 列出所有镜像与容器"></a>5. 列出所有镜像与容器</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 容器sudo docker ps -a# 镜像sudo docker images<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="6-保存镜像至本地-tar"><a href="#6-保存镜像至本地-tar" class="headerlink" title="6. 保存镜像至本地.tar"></a>6. 保存镜像至本地.tar</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"># 保存sudo docker save 0fdaf3gg4jgd &gt; ./my_docker_image.tar# 加载sudo docker load &lt; ./my_docker_image.tar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="7-docker支持图形化界面"><a href="#7-docker支持图形化界面" class="headerlink" title="7. docker支持图形化界面"></a>7. docker支持图形化界面</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">docker run -it --name jjren_gui -v /etc/localtime:/etc/localtime:ro --net=host -e DISPLAY=:0 -v $HOME/.Xauthority:/root/.Xauthority -v /run/media/dell3080/软件/jjren:/home/ xdurjj/207base:v1 /bin/bash<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><hr><h2 id="关于机器"><a href="#关于机器" class="headerlink" title="关于机器"></a>关于机器</h2><ol><li>zerotier连接</li><li>vscode配置远程ssh</li><li>vscode安装远程container</li><li>连接至容器后正常开发<ul><li>文件位置</li><li>容器内mmdet位置</li></ul></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>遥感常用数据集介绍</title>
      <link href="/2020/12/07/yao-gan-shu-ju-ji-zheng-li/"/>
      <url>/2020/12/07/yao-gan-shu-ju-ji-zheng-li/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>【转载】：<a href="https://aistudio.baidu.com/aistudio/projectdetail/882508">遥感影像数据集汇总 - 飞桨AI Studio - 人工智能学习实训社区 (baidu.com)</a><br>在使用深度学习处理遥感影像的过程中，经常在数据集上遇到各种问题：</p><p> 找不到针对自己任务的数据集<br> 数据集网址404或龟速下载<br> 数据集在空间分辨率、图像尺寸等方面不符合需求</p><p>现在，<strong>遥感影像数据集汇总</strong> 项目可以助你解决上述问题。<br>该项目是一个遥感影像领域常用的深度学习数据集的汇总，包括数据集<strong>基本信息</strong>，并附上数据集<strong>源地址</strong>与<strong>Aistudi备份链接（包括详细的类别信息，提供高速下载）</strong>。</p><p>目前本项目共收录</p><ul><li><p>图像分类数据集27个（整理完结）；</p></li><li><p>目标检测数据集31+个（整理完结）；</p></li><li><p>图像分割数据集36+个（整理完结）；</p></li><li><p>变化数据集14个；</p></li><li><p>高光谱分类3个（整理完结）；</p></li><li><p>高光谱检测2个（整理完结）；</p></li><li><p>高光谱分割8个（整理完结）；</p></li><li><p>多标签分类2个（整理完结）；</p></li><li><p>视频跟踪1个（整理完结）；</p></li></ul><h1 id="遥感影像场景分类"><a href="#遥感影像场景分类" class="headerlink" title="遥感影像场景分类"></a>遥感影像场景分类</h1><p><strong>收集网络中开源的、关于遥感影像 场景分类的深度学习数据集</strong></p><p>main ref </p><blockquote><p><a href="https://captain-whu.github.io/DiRS/">https://captain-whu.github.io/DiRS/</a><br><a href="https://mp.weixin.qq.com/s/kThlSqItIwuCFTIfC_-5lQ">https://mp.weixin.qq.com/s/kThlSqItIwuCFTIfC_-5lQ</a></p></blockquote><h2 id="Moving-and-Stationary-TargetAcquisition-and-Recognition-MSTAR-8class"><a href="#Moving-and-Stationary-TargetAcquisition-and-Recognition-MSTAR-8class" class="headerlink" title="Moving and Stationary TargetAcquisition and Recognition (MSTAR) 8class"></a>Moving and Stationary TargetAcquisition and Recognition (MSTAR) 8class</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">368 * 368 * 3</td><td>8</td><td>9466</td><td>0.3m</td><td>STARLOS SAR</td><td>1996</td><td>Defense Advanced Research Projects Agency and the Air Force Research Laboratory</td></tr></tbody></table><ul><li>源地址：<a href="https://www.kaggle.com/atreyamajumdar/mstar-dataset-8-classes">https://www.kaggle.com/atreyamajumdar/mstar-dataset-8-classes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/78849">https://aistudio.baidu.com/aistudio/datasetdetail/78849</a></li></ul><h2 id="UCMerced-LandUse"><a href="#UCMerced-LandUse" class="headerlink" title="UCMerced_LandUse"></a>UCMerced_LandUse</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>21</td><td>21 * 100 = 2100</td><td>1 foot</td><td>USGS</td><td>2010</td><td>加利福尼亚大学</td></tr></tbody></table><ul><li>源地址：<a href="http://weegee.vision.ucmerced.edu/datasets/landuse.html">http://weegee.vision.ucmerced.edu/datasets/landuse.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/51628">https://aistudio.baidu.com/aistudio/datasetdetail/51628</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1ryrLb5HnCw0Uxzbpva9Vnw">https://pan.baidu.com/s/1ryrLb5HnCw0Uxzbpva9Vnw</a>  提取码：n71j</li></ul><h2 id="WHU-RS19"><a href="#WHU-RS19" class="headerlink" title="WHU-RS19"></a>WHU-RS19</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">600 * 600 * 3</td><td>19</td><td>19 * 50± = 1005</td><td>最高0.5 m</td><td>GoogleEarth</td><td>2012</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="http://captain.whu.edu.cn/repository.html">http://captain.whu.edu.cn/repository.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/51733">https://aistudio.baidu.com/aistudio/datasetdetail/51733</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1-w68vcKpj0NYlOltEWdi6w">https://pan.baidu.com/s/1-w68vcKpj0NYlOltEWdi6w</a>  提取码：gpjd</li></ul><h2 id="RSSCN7"><a href="#RSSCN7" class="headerlink" title="RSSCN7"></a>RSSCN7</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">400 * 400 * 3</td><td>7</td><td>7 * 400 = 2800</td><td>未知</td><td>GoogleEarth</td><td>2015</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://sites.google.com/site/qinzoucn/documents">https://sites.google.com/site/qinzoucn/documents</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52117">https://aistudio.baidu.com/aistudio/datasetdetail/52117</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1wS3-TgykUS5svytVsJnU-Q">https://pan.baidu.com/s/1wS3-TgykUS5svytVsJnU-Q</a>  提取码：ppy1</li></ul><h2 id="RS-C11"><a href="#RS-C11" class="headerlink" title="RS_C11"></a>RS_C11</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>11</td><td>10 * 100± = 1232</td><td>约0.2m</td><td>GoogleEarth</td><td>2016</td><td>中科院</td></tr></tbody></table><ul><li>源地址：<a href="https://www.researchgate.net/publication/271647282_RS_C11_Database/comments">https://www.researchgate.net/publication/271647282_RS_C11_Database/comments</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52227">https://aistudio.baidu.com/aistudio/datasetdetail/52227</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1sOIaya_CibyZhx2RTd3C_Q">https://pan.baidu.com/s/1sOIaya_CibyZhx2RTd3C_Q</a>  提取码：hdnr</li></ul><h2 id="NWPU-RESISC45"><a href="#NWPU-RESISC45" class="headerlink" title="NWPU-RESISC45"></a>NWPU-RESISC45</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>45</td><td>45 * 700 = 31500</td><td>0.2~30m</td><td>GoogleEarth</td><td>2016</td><td>西北工业大学</td></tr></tbody></table><ul><li>源地址：<a href="http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.html">http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/51873">https://aistudio.baidu.com/aistudio/datasetdetail/51873</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1gkvrsiN4r3CpAl3k-UfIkA">https://pan.baidu.com/s/1gkvrsiN4r3CpAl3k-UfIkA</a>  提取码：3n0g</li></ul><h2 id="AID"><a href="#AID" class="headerlink" title="AID"></a>AID</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">600 * 600 * 3</td><td>30</td><td>30 * 300± = 10000</td><td>0.5~8m</td><td>GoogleEarth</td><td>2017</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://captain-whu.github.io/AID/">https://captain-whu.github.io/AID/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52025">https://aistudio.baidu.com/aistudio/datasetdetail/52025</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1dWMqt6y8Xe7g2gjpygOALQ">https://pan.baidu.com/s/1dWMqt6y8Xe7g2gjpygOALQ</a>  提取码：svzw</li></ul><h2 id="GID"><a href="#GID" class="headerlink" title="GID"></a>GID</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">56 * 56 * 4<br>56 * 56 * 3</td><td>15</td><td>15 * 2000 = 30000</td><td>未知</td><td>高分2</td><td>2018</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://x-ytong.github.io/project/GID.html">https://x-ytong.github.io/project/GID.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55324">https://aistudio.baidu.com/aistudio/datasetdetail/55324</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1Xux-3nKTbu1v3Mp5tdyM7A">https://pan.baidu.com/s/1Xux-3nKTbu1v3Mp5tdyM7A</a>  提取码：d3qs</li></ul><h2 id="RSD46-WHU"><a href="#RSD46-WHU" class="headerlink" title="RSD46-WHU"></a>RSD46-WHU</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>46</td><td>117000</td><td>0.5~2m</td><td>GoogleEarth、天地图</td><td>2017</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/RSIA-LIESMARS-WHU/RSD46-WHU">https://github.com/RSIA-LIESMARS-WHU/RSD46-WHU</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52359">https://aistudio.baidu.com/aistudio/datasetdetail/52359</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1WNR8wV86jl_g04JX7bCpNQ">https://pan.baidu.com/s/1WNR8wV86jl_g04JX7bCpNQ</a>  提取码：gn2a</li></ul><h2 id="PatternNet"><a href="#PatternNet" class="headerlink" title="PatternNet"></a>PatternNet</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>38</td><td>38 * 800 = 30400</td><td>0.062~4.693m</td><td>GoogleMap</td><td>2018</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://sites.google.com/view/zhouwx/dataset#h.p_Tgef10WTuEFr">https://sites.google.com/view/zhouwx/dataset#h.p_Tgef10WTuEFr</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52411">https://aistudio.baidu.com/aistudio/datasetdetail/52411</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1_1KgQFi7QaUp8p6dz3TbWw">https://pan.baidu.com/s/1_1KgQFi7QaUp8p6dz3TbWw</a>  提取码：j2y2</li></ul><h2 id="AIRS"><a href="#AIRS" class="headerlink" title="AIRS"></a>AIRS</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">10000 * 10000 * 3</td><td>1</td><td>857 train，94 val， 96 test</td><td>0.075m</td><td>LINZ Data Service</td><td>2018</td><td>University of Tokyo等</td></tr></tbody></table><ul><li>源地址：<a href="https://www.airs-dataset.com/">https://www.airs-dataset.com/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/74274">https://aistudio.baidu.com/aistudio/datasetdetail/74274</a></li></ul><h2 id="Satellite-Images-of-Hurricane-Damage"><a href="#Satellite-Images-of-Hurricane-Damage" class="headerlink" title="Satellite Images of Hurricane Damage"></a>Satellite Images of Hurricane Damage</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">128 * 128 * 3</td><td>2</td><td>23000</td><td>未知</td><td>2018</td><td>University of Washington等</td></tr></tbody></table><ul><li>源地址：<a href="https://www.kaggle.com/kmader/satellite-images-of-hurricane-damage">https://www.kaggle.com/kmader/satellite-images-of-hurricane-damage</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/88155">https://aistudio.baidu.com/aistudio/datasetdetail/88155</a></li></ul><h2 id="How-to-make-high-resolution-remote-sensing-image-dataset"><a href="#How-to-make-high-resolution-remote-sensing-image-dataset" class="headerlink" title="How-to-make-high-resolution-remote-sensing-image-dataset"></a>How-to-make-high-resolution-remote-sensing-image-dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>数据源</th><th>发布时间</th><th>空间分辨率</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>5</td><td>533</td><td>谷歌地球</td><td>2018</td><td>0.075m</td></tr></tbody></table><ul><li>源地址：<a href="https://blog.csdn.net/u012193416/article/details/79472533">https://blog.csdn.net/u012193416/article/details/79472533</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/88597">https://aistudio.baidu.com/aistudio/datasetdetail/88597</a></li></ul><h2 id="OPTIMAL-31"><a href="#OPTIMAL-31" class="headerlink" title="OPTIMAL-31"></a>OPTIMAL-31</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>31</td><td>31* 60 = 1860</td><td>未知</td><td>GoogleEarth</td><td>2019</td><td>西北工业大学</td></tr></tbody></table><ul><li>源地址：<a href="http://crabwq.github.io/">http://crabwq.github.io/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/51798">https://aistudio.baidu.com/aistudio/datasetdetail/51798</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1ETHM44DuZYpKkUTqTxs4-w">https://pan.baidu.com/s/1ETHM44DuZYpKkUTqTxs4-w</a>  提取码：1dyd</li></ul><h2 id="WiDS-Datathon-2019"><a href="#WiDS-Datathon-2019" class="headerlink" title="WiDS Datathon 2019"></a>WiDS Datathon 2019</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>1</td><td>11000train</td><td>Planet</td><td>2019</td><td>Stanford</td><td>3m</td></tr></tbody></table><ul><li>源地址：<a href="https://www.kaggle.com/c/widsdatathon2019/data">https://www.kaggle.com/c/widsdatathon2019/data</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/76927">https://aistudio.baidu.com/aistudio/datasetdetail/76927</a></li></ul><h2 id="Continual-Learning-Benchmark-for-Remote-Sensing-Image-Scene-Classification-CLRS"><a href="#Continual-Learning-Benchmark-for-Remote-Sensing-Image-Scene-Classification-CLRS" class="headerlink" title="Continual Learning Benchmark for Remote Sensing Image Scene Classification (CLRS)"></a>Continual Learning Benchmark for Remote Sensing Image Scene Classification (CLRS)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>25</td><td>15,000</td><td>Google Earth, Bing Map, Google Map, and Tianditu</td><td>2020</td><td>中南大学</td><td>0.26 m to 8.85 m</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/lehaifeng/CLRS">https://github.com/lehaifeng/CLRS</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/76417">https://aistudio.baidu.com/aistudio/datasetdetail/76417</a></li></ul><h2 id="SenseEarth-Classify"><a href="#SenseEarth-Classify" class="headerlink" title="SenseEarth Classify"></a>SenseEarth Classify</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">100 * 100 ~ 12655 * 12655 * 3</td><td>28</td><td>~70000</td><td>未知</td><td>2020</td><td>商汤科技</td><td>0.2~153m</td></tr></tbody></table><ul><li>源地址：<a href="https://rs.sensetime.com/competition/index.html#/info">https://rs.sensetime.com/competition/index.html#/info</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52728">https://aistudio.baidu.com/aistudio/datasetdetail/52728</a></li></ul><h2 id="Multi-View-Datasets，CV-BrCT"><a href="#Multi-View-Datasets，CV-BrCT" class="headerlink" title="Multi-View Datasets，CV-BrCT"></a>Multi-View Datasets，CV-BrCT</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500 * 475 * 3</td><td>8</td><td>24171 * 2</td><td>航空影像及地面街景影像</td><td>2020</td><td>Federal University of Minas Gerais</td></tr></tbody></table><ul><li>源地址：<a href="http://www.patreo.dcc.ufmg.br/2020/07/22/multi-view-datasets/">http://www.patreo.dcc.ufmg.br/2020/07/22/multi-view-datasets/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/58013">https://aistudio.baidu.com/aistudio/datasetdetail/58013</a></li></ul><h2 id="Multi-View-Datasets，AiRound"><a href="#Multi-View-Datasets，AiRound" class="headerlink" title="Multi-View Datasets，AiRound"></a>Multi-View Datasets，AiRound</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500 * 475 * 3</td><td>11</td><td>3495 * 4</td><td>Sentinel-2等多源数据</td><td>2020</td><td>Federal University of Minas Gerais</td></tr></tbody></table><ul><li>源地址：<a href="http://www.patreo.dcc.ufmg.br/2020/07/22/multi-view-datasets/">http://www.patreo.dcc.ufmg.br/2020/07/22/multi-view-datasets/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/58760">https://aistudio.baidu.com/aistudio/datasetdetail/58760</a></li></ul><h2 id="SIRI-WHU"><a href="#SIRI-WHU" class="headerlink" title="SIRI-WHU"></a>SIRI-WHU</h2><h3 id="SIRI-WHU：google"><a href="#SIRI-WHU：google" class="headerlink" title="SIRI-WHU：google"></a>SIRI-WHU：google</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">200 * 200 * 3</td><td>12</td><td>12 * 200 = 2400</td><td>2m</td><td>GoogleEarth</td><td>2016</td><td>武汉大学</td></tr></tbody></table><h3 id="SIRI-WHU：USGS"><a href="#SIRI-WHU：USGS" class="headerlink" title="SIRI-WHU：USGS"></a>SIRI-WHU：USGS</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">10000 * 9000 * 3</td><td>4</td><td>1</td><td>2 foot</td><td>USGS</td><td>2016</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="http://www.lmars.whu.edu.cn/prof_web/zhongyanfei/e-code.html">http://www.lmars.whu.edu.cn/prof_web/zhongyanfei/e-code.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/51921">https://aistudio.baidu.com/aistudio/datasetdetail/51921</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/14C4Km92FekHy2dWsrXLckA">https://pan.baidu.com/s/14C4Km92FekHy2dWsrXLckA</a>  提取码：6259</li></ul><h2 id="RSI-CB"><a href="#RSI-CB" class="headerlink" title="RSI-CB"></a>RSI-CB</h2><h3 id="RSI-CB128"><a href="#RSI-CB128" class="headerlink" title="RSI-CB128"></a>RSI-CB128</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>45</td><td>36000</td><td>0.3–3m</td><td>多源</td><td>2017</td><td>中南大学</td></tr></tbody></table><h3 id="RSI-CB256"><a href="#RSI-CB256" class="headerlink" title="RSI-CB256"></a>RSI-CB256</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>35</td><td>24000</td><td>0.3–3m</td><td>多源</td><td>2017</td><td>中南大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/lehaifeng/RSI-CB">https://github.com/lehaifeng/RSI-CB</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52487">https://aistudio.baidu.com/aistudio/datasetdetail/52487</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1f77k5g8mhsSzxPantyxemA">https://pan.baidu.com/s/1f77k5g8mhsSzxPantyxemA</a>  提取码：246r</li></ul><h2 id="SAT"><a href="#SAT" class="headerlink" title="SAT"></a>SAT</h2><h3 id="SAT-4"><a href="#SAT-4" class="headerlink" title="SAT-4"></a>SAT-4</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>备注</th></tr></thead><tbody><tr><td align="left">28 * 28 * 4</td><td>4</td><td>500,000</td><td>1-6m</td><td>NAIP dataset</td><td>2015</td><td>路易斯安那州立大学与NASA</td><td>采用MATLAB的.mat数据存储格式</td></tr></tbody></table><h3 id="SAT-6"><a href="#SAT-6" class="headerlink" title="SAT-6"></a>SAT-6</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>备注</th></tr></thead><tbody><tr><td align="left">28 * 28 * 4</td><td>6</td><td>405,000</td><td>1-6m</td><td>NAIP dataset</td><td>2015</td><td>路易斯安那州立大学与NASA</td><td>采用MATLAB的.mat数据存储格式</td></tr></tbody></table><ul><li>源地址：<a href="http://csc.lsu.edu/~saikat/deepsat/">http://csc.lsu.edu/~saikat/deepsat/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52534">https://aistudio.baidu.com/aistudio/datasetdetail/52534</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1LKCAPAz-DVGTkjQ1DMS0_Q">https://pan.baidu.com/s/1LKCAPAz-DVGTkjQ1DMS0_Q</a>  提取码：12du</li></ul><h2 id="V-RSIR"><a href="#V-RSIR" class="headerlink" title="V-RSIR"></a>V-RSIR</h2><h3 id="rs-VArcGIS"><a href="#rs-VArcGIS" class="headerlink" title="rs VArcGIS"></a>rs VArcGIS</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>38</td><td>59071</td><td>0.07 ~ 19.11m</td><td>ArcGIS World Imagery</td><td>2020</td><td>中南大学</td></tr></tbody></table><h3 id="rs-VBing"><a href="#rs-VBing" class="headerlink" title="rs VBing"></a>rs VBing</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>38</td><td>58944</td><td>0.07 ~ 38.22m</td><td>Bing World Imagery</td><td>2020</td><td>中南大学</td></tr></tbody></table><h3 id="rs-VGoogle"><a href="#rs-VGoogle" class="headerlink" title="rs VGoogle"></a>rs VGoogle</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>38</td><td>59404</td><td>0.075 ~ 9.555m</td><td>VGoogle</td><td>2020</td><td>中南大学</td></tr></tbody></table><ul><li>源地址：<br><a href="https://aistudio.baidu.com/aistudio/datasetdetail/47630">https://aistudio.baidu.com/aistudio/datasetdetail/47630</a><br><a href="https://aistudio.baidu.com/aistudio/datasetdetail/47638">https://aistudio.baidu.com/aistudio/datasetdetail/47638</a><br><a href="https://aistudio.baidu.com/aistudio/datasetdetail/47619">https://aistudio.baidu.com/aistudio/datasetdetail/47619</a></li></ul><h2 id="rs-sensetime"><a href="#rs-sensetime" class="headerlink" title="rs sensetime"></a>rs sensetime</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">100 * 100 * 3 ~ <br>12655 * 12655 * 3</td><td>50</td><td>60040</td><td>0.2~153m</td><td>未知</td><td>2020</td><td>商汤科技SenseEarth</td></tr></tbody></table><ul><li>源地址：<a href="https://rs.sensetime.com/competition/index.html#/data">https://rs.sensetime.com/competition/index.html#/data</a></li></ul><h2 id="rscup"><a href="#rscup" class="headerlink" title="rscup"></a>rscup</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>45</td><td>197,121</td><td>未知</td><td>2019</td><td>rscup组委会</td></tr></tbody></table><ul><li>源地址：<a href="http://rscup.bjxintong.com.cn/#/theme/1">http://rscup.bjxintong.com.cn/#/theme/1</a></li></ul><h2 id="Planet-Understanding-the-Amazon-from-Space"><a href="#Planet-Understanding-the-Amazon-from-Space" class="headerlink" title="Planet: Understanding the Amazon from Space"></a>Planet: Understanding the Amazon from Space</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>备注</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3<br>256 * 256 * 4</td><td>17</td><td>40480</td><td>3m</td><td>plant传感器</td><td>2017</td><td>Planet、SCCON公司</td><td>多标签</td></tr></tbody></table><ul><li>源地址：<a href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data">https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data</a></li></ul><h2 id="brazilian-coffee-scenes"><a href="#brazilian-coffee-scenes" class="headerlink" title="brazilian coffee scenes"></a>brazilian coffee scenes</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">64 * 64 * 3</td><td>2</td><td>2,876</td><td>未知</td><td>SPOT传感器</td><td>2015</td><td>米纳斯联邦大学</td></tr></tbody></table><ul><li>源地址：<a href="http://patreo.dcc.ufmg.br/2017/11/12/brazilian-coffee-scenes-dataset">http://patreo.dcc.ufmg.br/2017/11/12/brazilian-coffee-scenes-dataset</a></li></ul><h1 id="遥感影像目标检测"><a href="#遥感影像目标检测" class="headerlink" title="遥感影像目标检测"></a>遥感影像目标检测</h1><p><strong>收集网络中开源的、关于遥感影像 目标检测的深度学习数据集</strong></p><p>main ref </p><blockquote><p><a href="https://captain-whu.github.io/DiRS/">https://captain-whu.github.io/DiRS/</a><br><a href="https://mp.weixin.qq.com/s/QGIFeriC3s0xvok-TVhmPQ">https://mp.weixin.qq.com/s/QGIFeriC3s0xvok-TVhmPQ</a><br><a href="https://zhuanlan.zhihu.com/p/113579163">https://zhuanlan.zhihu.com/p/113579163</a></p></blockquote><h2 id="TAS"><a href="#TAS" class="headerlink" title="TAS"></a>TAS</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">792 * 636 * 3</td><td>1</td><td>30</td><td>1,319</td><td>HBB</td><td>Google Earth</td><td>2008</td><td>斯坦福大学</td></tr></tbody></table><ul><li>源地址：<a href="http://ai.stanford.edu/~gaheitz/Research/TAS/">http://ai.stanford.edu/~gaheitz/Research/TAS/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53806">https://aistudio.baidu.com/aistudio/datasetdetail/53806</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1jGUThjNj3GsU0EIAQlQaiw%20">https://pan.baidu.com/s/1jGUThjNj3GsU0EIAQlQaiw</a> 提取码：t7lo</li></ul><h2 id="OIRDS"><a href="#OIRDS" class="headerlink" title="OIRDS"></a>OIRDS</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256<del>640 * 256</del>640 * 3</td><td>5</td><td>900</td><td>1800</td><td>约0.15m</td><td>OBB</td><td>USGS、DARPA、VIVID</td><td>2009</td><td>雷神公司等</td></tr></tbody></table><ul><li>源地址：<a href="https://sourceforge.net/projects/oirds/">https://sourceforge.net/projects/oirds/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53461">https://aistudio.baidu.com/aistudio/datasetdetail/53461</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/173yxlf279R8EESlT7mxbPw">https://pan.baidu.com/s/173yxlf279R8EESlT7mxbPw</a>  提取码：k8di</li></ul><h2 id="SZTAKI-INRIA-Building-Detection-Benchmark"><a href="#SZTAKI-INRIA-Building-Detection-Benchmark" class="headerlink" title="SZTAKI-INRIA Building Detection Benchmark"></a>SZTAKI-INRIA Building Detection Benchmark</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">700± * 700± * 3</td><td>1</td><td>9</td><td>665</td><td>OBB</td><td>未知</td><td>2012</td><td>MTA SZTAKI</td></tr></tbody></table><ul><li>源地址：<a href="http://web.eee.sztaki.hu/remotesensing/building_benchmark.html">http://web.eee.sztaki.hu/remotesensing/building_benchmark.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/77674">https://aistudio.baidu.com/aistudio/datasetdetail/77674</a></li></ul><h2 id="UCAS-AOD"><a href="#UCAS-AOD" class="headerlink" title="UCAS_AOD"></a>UCAS_AOD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1000± * 1000± * 3</td><td>2</td><td>976</td><td>6,950</td><td>OBB</td><td>Google Earth</td><td>2014</td><td>中科院</td></tr></tbody></table><ul><li>源地址：<a href="https://onedrive.hyper.ai/home/UCAS-AOD">https://onedrive.hyper.ai/home/UCAS-AOD</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53318">https://aistudio.baidu.com/aistudio/datasetdetail/53318</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1E9L-S6baxLDjSDTYlm985g">https://pan.baidu.com/s/1E9L-S6baxLDjSDTYlm985g</a>  提取码：sd6l</li></ul><h2 id="NWPU-VHR-10"><a href="#NWPU-VHR-10" class="headerlink" title="NWPU VHR-10"></a>NWPU VHR-10</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500<del>1100 * 500</del>1100 * 3</td><td>10</td><td>1510</td><td>14,596</td><td>HBB</td><td>Google Earth、Vaihingen</td><td>2014</td><td>西北工业大学</td></tr></tbody></table><ul><li>源地址：<a href="http://www.escience.cn/people/gongcheng/NWPU-VHR-10.html">http://www.escience.cn/people/gongcheng/NWPU-VHR-10.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52812">https://aistudio.baidu.com/aistudio/datasetdetail/52812</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1Wm73acTD1WfBM_YZlKMjXw">https://pan.baidu.com/s/1Wm73acTD1WfBM_YZlKMjXw</a>  提取码：35wf</li></ul><h2 id="HRSC2016"><a href="#HRSC2016" class="headerlink" title="HRSC2016"></a>HRSC2016</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1100± * 1100± * 3</td><td>27</td><td>1,061</td><td>2,976</td><td>OBB</td><td>Google Earth</td><td>2016</td><td>西北工业大学</td></tr></tbody></table><ul><li>源地址：<a href="https://sites.google.com/site/hrsc2016/">https://sites.google.com/site/hrsc2016/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54106">https://aistudio.baidu.com/aistudio/datasetdetail/54106</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1BO2UX6dICVa33qUqIMCFrw">https://pan.baidu.com/s/1BO2UX6dICVa33qUqIMCFrw</a>  提取码：shbp</li></ul><h2 id="DLR3k"><a href="#DLR3k" class="headerlink" title="DLR3k"></a>DLR3k</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">5616 * 3744 * 3</td><td>7</td><td>20</td><td>14,235</td><td>0.13m</td><td>OBB</td><td>无人机(Canon Eos 1Ds Mark III)</td><td>2016</td><td>德国航天航空中心</td></tr></tbody></table><ul><li>源地址：<a href="https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-12760/22294_read-52777">https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-12760/22294_read-52777</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/xxxxx">https://aistudio.baidu.com/aistudio/datasetdetail/xxxxx</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1hIbSQd-XdHxpmYIdFYHCQw%20">https://pan.baidu.com/s/1hIbSQd-XdHxpmYIdFYHCQw</a> 提取码：1lpx</li></ul><h2 id="RSOD"><a href="#RSOD" class="headerlink" title="RSOD"></a>RSOD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1044<del>1288 * 915</del>992 * 3</td><td>4</td><td>976</td><td>6,950</td><td>HBB</td><td>Google Earth、天地图</td><td>2017</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/RSIA-LIESMARS-WHU/RSOD-Dataset-">https://github.com/RSIA-LIESMARS-WHU/RSOD-Dataset-</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52980">https://aistudio.baidu.com/aistudio/datasetdetail/52980</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1lUooUOJE2QCpCXSBrzKysQ">https://pan.baidu.com/s/1lUooUOJE2QCpCXSBrzKysQ</a>  提取码：pqih</li></ul><h2 id="TGRS-HRRSD"><a href="#TGRS-HRRSD" class="headerlink" title="TGRS-HRRSD"></a>TGRS-HRRSD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">152<del>10569 * 152</del>10569 * 3</td><td>13</td><td>21,761</td><td>55,740</td><td>0.15 ~ 1.2m</td><td>HBB</td><td>Google Earth、百度地图</td><td>2017</td><td>中科院</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/CrazyStoneonRoad/TGRS-HRRSD-Dataset">https://github.com/CrazyStoneonRoad/TGRS-HRRSD-Dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53186">https://aistudio.baidu.com/aistudio/datasetdetail/53186</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1hru_vjugopeJCXT7SguhcQ%20">https://pan.baidu.com/s/1hru_vjugopeJCXT7SguhcQ</a> 提取码：row5</li></ul><h2 id="SSDD"><a href="#SSDD" class="headerlink" title="SSDD"></a>SSDD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500 * 500 * 3</td><td>1</td><td>1160</td><td>2456</td><td>1~15m</td><td>HBB、OBB</td><td>RadarSat-2、TerraSAR-X、Sentinel-1</td><td>2017</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://zhuanlan.zhihu.com/p/58404659">https://zhuanlan.zhihu.com/p/58404659</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54806">https://aistudio.baidu.com/aistudio/datasetdetail/54806</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1tcKcCTjUXJJW--erUFyzqg%20">https://pan.baidu.com/s/1tcKcCTjUXJJW--erUFyzqg</a> 提取码：hb0d</li></ul><h2 id="OpenSARShip"><a href="#OpenSARShip" class="headerlink" title="OpenSARShip"></a>OpenSARShip</h2><table><thead><tr><th align="left">类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1</td><td>41</td><td>11346</td><td>Chip</td><td>Sentinel-1</td><td>2017</td><td>上海交通大学</td></tr></tbody></table><ul><li>源地址：<a href="http://opensar.sjtu.edu.cn/">http://opensar.sjtu.edu.cn/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/77017">https://aistudio.baidu.com/aistudio/datasetdetail/77017</a></li></ul><h2 id="ships-in-satellite-imagery"><a href="#ships-in-satellite-imagery" class="headerlink" title="ships in satellite imagery"></a>ships in satellite imagery</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">80 * 80 * 3</td><td>1</td><td>4000</td><td>1000+</td><td>Planet</td><td>2017</td><td>Planet Team</td><td>3m</td></tr></tbody></table><ul><li>源地址：<a href="https://www.kaggle.com/rhammell/ships-in-satellite-imagery">https://www.kaggle.com/rhammell/ships-in-satellite-imagery</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/77233">https://aistudio.baidu.com/aistudio/datasetdetail/77233</a></li></ul><h2 id="LEVIR"><a href="#LEVIR" class="headerlink" title="LEVIR"></a>LEVIR</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800 * 600 * 3</td><td>3</td><td>22,000</td><td>11,000</td><td>0.2∼1.0m</td><td>HBB</td><td>Google Earth</td><td>2018</td><td>北京航天航空大学</td></tr></tbody></table><ul><li>源地址：<a href="http://levir.buaa.edu.cn/Code.htm">http://levir.buaa.edu.cn/Code.htm</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53714">https://aistudio.baidu.com/aistudio/datasetdetail/53714</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1kfTozB7SHIH1ioaJdzHwFA%20">https://pan.baidu.com/s/1kfTozB7SHIH1ioaJdzHwFA</a> 提取码：eydb</li></ul><h2 id="VisDrone2019-DET"><a href="#VisDrone2019-DET" class="headerlink" title="VisDrone2019-DET"></a>VisDrone2019-DET</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500<del>2,000* 500</del>2,000 * 3</td><td>10</td><td>10,209</td><td>54,200</td><td>HBB</td><td>无人机数据</td><td>2018</td><td>天津大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/VisDrone/VisDrone-Dataset">https://github.com/VisDrone/VisDrone-Dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54054">https://aistudio.baidu.com/aistudio/datasetdetail/54054</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1ZvWrcRimr-UMU1QI1P6Y8g">https://pan.baidu.com/s/1ZvWrcRimr-UMU1QI1P6Y8g</a>  提取码：fcjs</li></ul><h2 id="MASATI"><a href="#MASATI" class="headerlink" title="MASATI"></a>MASATI</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">512± * 512± * 3</td><td>7</td><td>7,389</td><td>未知</td><td>HBB</td><td>Bing Maps</td><td>2018</td><td>阿利坎特大学</td></tr></tbody></table><ul><li>源地址：<a href="https://www.iuii.ua.es/datasets/masati/">https://www.iuii.ua.es/datasets/masati/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53895">https://aistudio.baidu.com/aistudio/datasetdetail/53895</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1t84BiCmTY8PTBnjBGf0gbQ%20">https://pan.baidu.com/s/1t84BiCmTY8PTBnjBGf0gbQ</a> 提取码：npwo</li></ul><h2 id="ITCVD"><a href="#ITCVD" class="headerlink" title="ITCVD"></a>ITCVD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>备注</th></tr></thead><tbody><tr><td align="left">5616 * 3744 * 3</td><td>1</td><td>135</td><td>23543</td><td>0.1 m</td><td>HBB</td><td>航拍影像</td><td>2018</td><td>University of Twente Research Information</td><td>标注为mat格式</td></tr></tbody></table><ul><li>源地址：<a href="https://research.utwente.nl/en/datasets/itcvd-dataset">https://research.utwente.nl/en/datasets/itcvd-dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54674">https://aistudio.baidu.com/aistudio/datasetdetail/54674</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1TA2_HU3qb96-lXkZJ5BtEg%20">https://pan.baidu.com/s/1TA2_HU3qb96-lXkZJ5BtEg</a> 提取码：6hht</li></ul><h2 id="DIOR"><a href="#DIOR" class="headerlink" title="DIOR"></a>DIOR</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800 * 800 * 3</td><td>20</td><td>23463</td><td>190288</td><td>0.5 ~ 30 m</td><td>HBB</td><td>Google Earth</td><td>2019</td><td>西北工业大学</td></tr></tbody></table><ul><li>源地址：<a href="http://www.escience.cn/people/JunweiHan/DIOR.html">http://www.escience.cn/people/JunweiHan/DIOR.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53045">https://aistudio.baidu.com/aistudio/datasetdetail/53045</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1Z3xJC_l5MpGSFuW0u4shuQ">https://pan.baidu.com/s/1Z3xJC_l5MpGSFuW0u4shuQ</a>  提取码：vt56</li></ul><h2 id="AIR-SARShip"><a href="#AIR-SARShip" class="headerlink" title="AIR-SARShip"></a>AIR-SARShip</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1000 * 1000 * 3</td><td>1</td><td>300</td><td>2040</td><td>1~3m</td><td>HBB</td><td>高分3</td><td>2019</td><td>《雷达学报》编辑部</td></tr></tbody></table><ul><li>源地址：<a href="http://radars.ie.ac.cn/web/data/getData?dataType=SARDataset">http://radars.ie.ac.cn/web/data/getData?dataType=SARDataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54270">https://aistudio.baidu.com/aistudio/datasetdetail/54270</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1NnLVijCr0BN8CIg86srDUA">https://pan.baidu.com/s/1NnLVijCr0BN8CIg86srDUA</a>  提取码：bfri</li></ul><h2 id="SAR-Ship"><a href="#SAR-Ship" class="headerlink" title="SAR-Ship"></a>SAR-Ship</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>1</td><td>210</td><td>43,819</td><td>1.7~25m</td><td>HBB</td><td>高分3、哨兵1</td><td>2019</td><td>中科院</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/CAESAR-Radi/SAR-Ship-Dataset">https://github.com/CAESAR-Radi/SAR-Ship-Dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54361">https://aistudio.baidu.com/aistudio/datasetdetail/54361</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/13MViO0EdhCZSNlesF-32Bg%20">https://pan.baidu.com/s/13MViO0EdhCZSNlesF-32Bg</a> 提取码：s0qf</li></ul><h2 id="iSAID"><a href="#iSAID" class="headerlink" title="iSAID"></a>iSAID</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800<del>13, 000 * 800</del>13, 000 * 3</td><td>15</td><td>2,806</td><td>655,451</td><td>OBB</td><td>Google Earth、JL-1、GF-2</td><td>2019</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://captain-whu.github.io/iSAID/index.html">https://captain-whu.github.io/iSAID/index.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/73458">https://aistudio.baidu.com/aistudio/datasetdetail/73458</a></li></ul><h2 id="Bridge-Dataset"><a href="#Bridge-Dataset" class="headerlink" title="Bridge Dataset"></a>Bridge Dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4800 * 2843 * 3</td><td>1</td><td>500</td><td>500+</td><td>HBB</td><td>Google Earth、 OpenStreetMap</td><td>2019</td><td>Federal University of Minas Gerais</td></tr></tbody></table><ul><li>源地址：<a href="http://www.patreo.dcc.ufmg.br/2019/07/10/bridge-dataset/">http://www.patreo.dcc.ufmg.br/2019/07/10/bridge-dataset/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/57457">https://aistudio.baidu.com/aistudio/datasetdetail/57457</a></li></ul><h2 id="HRSID"><a href="#HRSID" class="headerlink" title="HRSID"></a>HRSID</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800 * 800 * 3</td><td>1</td><td>5604</td><td>16951</td><td>0.5~3 m</td><td>HBB</td><td>Sentinel-1B、TerraSAR-X、TanDEM-X</td><td>2020</td><td>电子科技大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/chaozhong2010/HRSID">https://github.com/chaozhong2010/HRSID</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54512">https://aistudio.baidu.com/aistudio/datasetdetail/54512</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/13MViO0EdhCZSNlesF-32Bg%20">https://pan.baidu.com/s/13MViO0EdhCZSNlesF-32Bg</a> 提取码：s0qf</li></ul><h2 id="RarePlanes"><a href="#RarePlanes" class="headerlink" title="RarePlanes"></a>RarePlanes</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>10</td><td>1244 train，263 test</td><td>~14,700</td><td>HBB</td><td>WorldView-3</td><td>2020</td><td>In-Q-Tel、AI.Reverie</td><td>0.3~1.5m</td></tr></tbody></table><ul><li>源地址：<a href="https://www.cosmiqworks.org/RarePlanes/">https://www.cosmiqworks.org/RarePlanes/</a><br><a href="https://www.graviti.cn/open-datasets/RarePlanes">https://www.graviti.cn/open-datasets/RarePlanes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/70215">https://aistudio.baidu.com/aistudio/datasetdetail/70215</a></li></ul><h2 id="FAIR1M"><a href="#FAIR1M" class="headerlink" title="FAIR1M"></a>FAIR1M</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">1000<del>10000 * 1000</del>10000 * 3</td><td>36</td><td>15000+</td><td>100 0000+</td><td>OBB</td><td>Gaofen satellites、Google Earth</td><td>2021</td><td>中科院等</td><td>0.3 ~ 0.8m</td></tr></tbody></table><ul><li>源地址：<a href="http://gaofen-challenge.com/">http://gaofen-challenge.com</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/78453">https://aistudio.baidu.com/aistudio/datasetdetail/78453</a></li></ul><h2 id="VEDAI"><a href="#VEDAI" class="headerlink" title="VEDAI"></a>VEDAI</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">512 * 512 * 4<br>1024 * 1024 * 4</td><td>9</td><td>1210</td><td>3640</td><td>0.125m</td><td>OBB</td><td>Utah AGRC</td><td>2015</td><td>卡昂大学</td></tr></tbody></table><ul><li>源地址：<a href="https://downloads.greyc.fr/vedai/">https://downloads.greyc.fr/vedai/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53383">https://aistudio.baidu.com/aistudio/datasetdetail/53383</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1t3qCIUnEpJ1lIUTZpjlCyw">https://pan.baidu.com/s/1t3qCIUnEpJ1lIUTZpjlCyw</a>  提取码：doid</li></ul><h2 id="DOTA"><a href="#DOTA" class="headerlink" title="DOTA"></a>DOTA</h2><h3 id="DOTA1-0"><a href="#DOTA1-0" class="headerlink" title="DOTA1.0"></a>DOTA1.0</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800<del>4000 * 800</del>4000 * 3</td><td>15</td><td>2806</td><td>188282</td><td>OBB</td><td>Google Earth、高分2、吉林1</td><td>2018</td><td>武汉大学</td></tr></tbody></table><h3 id="DOTA1-5"><a href="#DOTA1-5" class="headerlink" title="DOTA1.5"></a>DOTA1.5</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800<del>4000 * 800</del>4000 * 3</td><td>16</td><td>2806</td><td>400000±</td><td>OBB</td><td>Google Earth、高分2、吉林1</td><td>2019</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://captain-whu.github.io/DOTA/index.html">https://captain-whu.github.io/DOTA/index.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53125">https://aistudio.baidu.com/aistudio/datasetdetail/53125</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/19AhrPGnbrPwCdHsThv0XBQ">https://pan.baidu.com/s/19AhrPGnbrPwCdHsThv0XBQ</a>  提取码：avsl</li></ul><h2 id="SZTAKI-INRIA"><a href="#SZTAKI-INRIA" class="headerlink" title="SZTAKI-INRIA"></a>SZTAKI-INRIA</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800± * 800±</td><td>1</td><td>9</td><td>665</td><td>OBB</td><td>未知</td><td>2012</td><td>MTA SZTAKI、INRIA Sophia-Antipolis</td></tr></tbody></table><ul><li>源地址：<a href="http://web.eee.sztaki.hu/remotesensing/building_benchmark.html">http://web.eee.sztaki.hu/remotesensing/building_benchmark.html</a></li></ul><h2 id="COWC"><a href="#COWC" class="headerlink" title="COWC"></a>COWC</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">2,000<del>19,000 * 2,000</del>19,000</td><td>1</td><td>53</td><td>32,716</td><td>0.15m</td><td>one dot</td><td>Utah</td><td>2016</td><td>劳伦斯利弗莫尔国家实验室</td></tr></tbody></table><ul><li>源地址：<a href="https://gdo152.llnl.gov/cowc/">https://gdo152.llnl.gov/cowc/</a></li></ul><h2 id="Functional-Map-of-the-World-Challenge"><a href="#Functional-Map-of-the-World-Challenge" class="headerlink" title="Functional Map of the World Challenge"></a>Functional Map of the World Challenge</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4bands<br>8bands</td><td>63</td><td>31</td><td>1,000,000</td><td>one dot</td><td>Utah</td><td>2016</td><td>劳伦斯利弗莫尔国家实验室</td></tr></tbody></table><ul><li>源地址：<a href="https://www.iarpa.gov/challenges/fmow.html">https://www.iarpa.gov/challenges/fmow.html</a><br><a href="https://github.com/fMoW/dataset">https://github.com/fMoW/dataset</a></li></ul><h2 id="CARPK"><a href="#CARPK" class="headerlink" title="CARPK"></a>CARPK</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">未知</td><td>1</td><td>1,573</td><td>106,233</td><td>未知</td><td>无人机</td><td>2017</td><td>台湾国立大学</td></tr></tbody></table><ul><li>源地址：<a href="https://lafi.github.io/LPN/">https://lafi.github.io/LPN/</a></li></ul><h2 id="rscup-1"><a href="#rscup-1" class="headerlink" title="rscup"></a>rscup</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1024 * 1024 * 3</td><td>18</td><td>未知</td><td>未知</td><td>OBB</td><td>GF, JL, Google Earth, Aerial</td><td>2019</td><td>rscup组委会</td></tr></tbody></table><ul><li>源地址：<a href="http://rscup.bjxintong.com.cn/#/theme/2">http://rscup.bjxintong.com.cn/#/theme/2</a></li></ul><h2 id="planet-understanding-the-amazon-from-space"><a href="#planet-understanding-the-amazon-from-space" class="headerlink" title="planet-understanding-the-amazon-from-space"></a>planet-understanding-the-amazon-from-space</h2><ul><li>源地址：<a href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data">https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data</a></li></ul><h2 id="airbus-ship-detection"><a href="#airbus-ship-detection" class="headerlink" title="airbus-ship-detection"></a>airbus-ship-detection</h2><ul><li>源地址：<a href="https://www.kaggle.com/c/airbus-ship-detection">https://www.kaggle.com/c/airbus-ship-detection</a></li></ul><h2 id="Open-AI-Tanzania-Building-Footprint-Segmentation-Challenge"><a href="#Open-AI-Tanzania-Building-Footprint-Segmentation-Challenge" class="headerlink" title="Open AI Tanzania Building Footprint Segmentation Challenge"></a>Open AI Tanzania Building Footprint Segmentation Challenge</h2><ul><li>源地址：<a href="https://competitions.codalab.org/competitions/20100">https://competitions.codalab.org/competitions/20100</a></li></ul><h2 id="MAFAT-Challenge-Fine-Grained-Classification-of-Objects-from-Aerial-Imagery"><a href="#MAFAT-Challenge-Fine-Grained-Classification-of-Objects-from-Aerial-Imagery" class="headerlink" title="MAFAT Challenge - Fine-Grained Classification of Objects from Aerial Imagery"></a>MAFAT Challenge - Fine-Grained Classification of Objects from Aerial Imagery</h2><ul><li>源地址：<a href="https://competitions.codalab.org/competitions/19854">https://competitions.codalab.org/competitions/19854</a></li></ul><h1 id="遥感影像图像分割"><a href="#遥感影像图像分割" class="headerlink" title="遥感影像图像分割"></a>遥感影像图像分割</h1><p><strong>收集网络中开源的、关于遥感影像 图像分割的深度学习数据集</strong></p><p>main ref </p><blockquote><p><a href="https://captain-whu.github.io/DiRS/">https://captain-whu.github.io/DiRS/</a><br><a href="https://mp.weixin.qq.com/s/x-sgus9bJz7esbsRvb-IfA">https://mp.weixin.qq.com/s/x-sgus9bJz7esbsRvb-IfA</a><br><a href="https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-12760/">https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-12760/</a><br><a href="http://www.patreo.dcc.ufmg.br/category/downloads/datasets/">http://www.patreo.dcc.ufmg.br/category/downloads/datasets/</a></p></blockquote><h2 id="Massachusetts-Roads"><a href="#Massachusetts-Roads" class="headerlink" title="Massachusetts Roads"></a>Massachusetts Roads</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1500 * 1500 * 3</td><td>1</td><td>804</td><td>航空影像</td><td>2013</td><td>University of Toronto</td></tr></tbody></table><ul><li>源地址：<a href="https://www.cs.toronto.edu/~vmnih/data/">https://www.cs.toronto.edu/~vmnih/data/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56961">https://aistudio.baidu.com/aistudio/datasetdetail/56961</a></li></ul><h2 id="Massachusetts-Builds"><a href="#Massachusetts-Builds" class="headerlink" title="Massachusetts Builds"></a>Massachusetts Builds</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1500 * 1500 * 3</td><td>1</td><td>151</td><td>航空影像</td><td>2013</td><td>University of Toronto</td></tr></tbody></table><ul><li>源地址：<a href="https://www.cs.toronto.edu/~vmnih/data/">https://www.cs.toronto.edu/~vmnih/data/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/57019">https://aistudio.baidu.com/aistudio/datasetdetail/57019</a></li></ul><h2 id="Zurich-Summer"><a href="#Zurich-Summer" class="headerlink" title="Zurich Summer"></a>Zurich Summer</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">600<del>1600 * 600</del>1600 * 4</td><td>8</td><td>20</td><td>QuickBird</td><td>2015</td><td>The University of Edinburgh, Scotland (UK)</td><td>0.62m</td></tr></tbody></table><ul><li>源地址：<a href="https://sites.google.com/view/zhouwx/dataset#h.p_hQS2jYeaFpV0">https://sites.google.com/view/zhouwx/dataset#h.p_hQS2jYeaFpV0</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55102">https://aistudio.baidu.com/aistudio/datasetdetail/55102</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1_Cydh88pJXKJnmlvUv2y6Q">https://pan.baidu.com/s/1_Cydh88pJXKJnmlvUv2y6Q</a>  提取码：u7cr</li></ul><h2 id="ERM-PAIW"><a href="#ERM-PAIW" class="headerlink" title="ERM-PAIW"></a>ERM-PAIW</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4000± * 4000± * 3</td><td>1</td><td>41</td><td>航空影像</td><td>2015</td><td>German Aerospace Center (DLR)</td></tr></tbody></table><ul><li>源地址：<a href="https://www.cs.toronto.edu/~vmnih/data/">https://www.cs.toronto.edu/~vmnih/data/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/57293">https://aistudio.baidu.com/aistudio/datasetdetail/57293</a></li></ul><h2 id="HD-Maps"><a href="#HD-Maps" class="headerlink" title="HD-Maps"></a>HD-Maps</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4000± * 4000± * 3</td><td>4</td><td>20</td><td>航空影像</td><td>2016</td><td>German Aerospace Center (DLR)</td></tr></tbody></table><ul><li>源地址：<a href="https://www.cs.toronto.edu/~vmnih/data/">https://www.cs.toronto.edu/~vmnih/data/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/57162">https://aistudio.baidu.com/aistudio/datasetdetail/57162</a></li></ul><h2 id="BDCI2017"><a href="#BDCI2017" class="headerlink" title="BDCI2017"></a>BDCI2017</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">8000± * 8000± * 3</td><td>5</td><td>5</td><td>未知</td><td>2017</td><td>BDCI</td></tr></tbody></table><ul><li>源地址：<a href="https://www.datafountain.cn/competitions/270">https://www.datafountain.cn/competitions/270</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55424">https://aistudio.baidu.com/aistudio/datasetdetail/55424</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1sww-vuY9m4snAJY9IFRnIA">https://pan.baidu.com/s/1sww-vuY9m4snAJY9IFRnIA</a>  提取码：ocnn</li></ul><h2 id="Learning-Aerial-Image-Segmentation-From-Online-Maps"><a href="#Learning-Aerial-Image-Segmentation-From-Online-Maps" class="headerlink" title="Learning Aerial Image Segmentation From Online Maps"></a>Learning Aerial Image Segmentation From Online Maps</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">3000± * 3000± * 3</td><td>2</td><td>1671</td><td>Google Maps、OpenStreetMap</td><td>2017</td><td>TH Zürich</td></tr></tbody></table><ul><li>源地址：<a href="https://zenodo.org/record/1154821#.X4rCKS6HqUm">https://zenodo.org/record/1154821#.X4rCKS6HqUm</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56140">https://aistudio.baidu.com/aistudio/datasetdetail/56140</a></li></ul><h2 id="2018-Open-AI-Tanzania-Building-Footprint-Segmentation-Challenge-TBF"><a href="#2018-Open-AI-Tanzania-Building-Footprint-Segmentation-Challenge-TBF" class="headerlink" title="2018 Open AI Tanzania Building Footprint Segmentation Challenge(TBF)"></a>2018 Open AI Tanzania Building Footprint Segmentation Challenge(TBF)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">40000± * 40000± * 3</td><td>1</td><td>13</td><td>航空影像</td><td>2018</td><td>SUZA</td></tr></tbody></table><ul><li>源地址：<a href="https://competitions.codalab.org/competitions/20100">https://competitions.codalab.org/competitions/20100</a><br><a href="http://www.graphnetcloud.cn/4-10">http://www.graphnetcloud.cn/4-10</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/76145">https://aistudio.baidu.com/aistudio/datasetdetail/76145</a></li></ul><h2 id="WHDLD"><a href="#WHDLD" class="headerlink" title="WHDLD"></a>WHDLD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>6</td><td>4940</td><td>UC Merced</td><td>2018</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://sites.google.com/view/zhouwx/dataset#h.p_hQS2jYeaFpV0">https://sites.google.com/view/zhouwx/dataset#h.p_hQS2jYeaFpV0</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55589">https://aistudio.baidu.com/aistudio/datasetdetail/55589</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1MgOUJpmClc_h4CLzJVQYvQ">https://pan.baidu.com/s/1MgOUJpmClc_h4CLzJVQYvQ</a>  提取码：gvui</li></ul><h2 id="DLRSD"><a href="#DLRSD" class="headerlink" title="DLRSD"></a>DLRSD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>21</td><td>2100</td><td>USGS National Map</td><td>2018</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://sites.google.com/view/zhouwx/dataset#h.p_hQS2jYeaFpV0">https://sites.google.com/view/zhouwx/dataset#h.p_hQS2jYeaFpV0</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55005">https://aistudio.baidu.com/aistudio/datasetdetail/55005</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1CmRdyNdz85ji0Z7JsrCHDw">https://pan.baidu.com/s/1CmRdyNdz85ji0Z7JsrCHDw</a>  提取码：1y38</li></ul><h2 id="DeepGlobe-Land-Cover-Classification-Challenge"><a href="#DeepGlobe-Land-Cover-Classification-Challenge" class="headerlink" title="DeepGlobe Land Cover Classification Challenge"></a>DeepGlobe Land Cover Classification Challenge</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">2448 * 2448 * 3</td><td>7</td><td>803</td><td>DigitalGlobe</td><td>2018</td><td>CVPR</td><td>0.5m</td></tr></tbody></table><ul><li>源地址：<a href="http://deepglobe.org/challenge.html">http://deepglobe.org/challenge.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55681">https://aistudio.baidu.com/aistudio/datasetdetail/55681</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1D-RpKNNuP5L-hLo4H-8txA">https://pan.baidu.com/s/1D-RpKNNuP5L-hLo4H-8txA</a>  提取码：lk1a</li></ul><h2 id="DeepGlobe-Road-Detection-Challenge"><a href="#DeepGlobe-Road-Detection-Challenge" class="headerlink" title="DeepGlobe Road Detection Challenge"></a>DeepGlobe Road Detection Challenge</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1024 * 1024 * 3</td><td>1</td><td>6,226</td><td>DigitalGlobe</td><td>2018</td><td>CVPR</td></tr></tbody></table><ul><li>源地址：<a href="http://deepglobe.org/challenge.html">http://deepglobe.org/challenge.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55682">https://aistudio.baidu.com/aistudio/datasetdetail/55682</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1uvZ7dUOjkt9_i120hr9JUQ">https://pan.baidu.com/s/1uvZ7dUOjkt9_i120hr9JUQ</a>  提取码：74be</li></ul><h2 id="Aeroscapes"><a href="#Aeroscapes" class="headerlink" title="Aeroscapes"></a>Aeroscapes</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">720 * 720 * 3</td><td>11</td><td>3269</td><td>航空影像</td><td>2018</td><td>Carnegie Mellon University</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/ishann/aeroscapes">https://github.com/ishann/aeroscapes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55222">https://aistudio.baidu.com/aistudio/datasetdetail/55222</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1-AcaV3nTDsBibbzk6Y92kw">https://pan.baidu.com/s/1-AcaV3nTDsBibbzk6Y92kw</a>  提取码：zodm</li></ul><h2 id="Map-Challenge"><a href="#Map-Challenge" class="headerlink" title="Map Challenge"></a>Map Challenge</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>标注格式</th></tr></thead><tbody><tr><td align="left">300 * 300 * 3</td><td>1</td><td>341,058</td><td>Google Map</td><td>2018</td><td>crowdAI</td><td>json</td></tr></tbody></table><ul><li>源地址：<a href="https://www.crowdai.org/challenges/mapping-challenge">https://www.crowdai.org/challenges/mapping-challenge</a> ；<a href="https://www.jianshu.com/p/90efc39975da">https://www.jianshu.com/p/90efc39975da</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54858">https://aistudio.baidu.com/aistudio/datasetdetail/54858</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1keRAm0OEanCDuRUPpUra6A%20">https://pan.baidu.com/s/1keRAm0OEanCDuRUPpUra6A</a> 提取码：pm0m</li></ul><h2 id="38-Cloud-A-Cloud-Segmentation-Dataset"><a href="#38-Cloud-A-Cloud-Segmentation-Dataset" class="headerlink" title="38-Cloud: A Cloud Segmentation Dataset"></a>38-Cloud: A Cloud Segmentation Dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">384* 384 * 4</td><td>1</td><td>8400</td><td>Landsat 8</td><td>2018</td><td>Science Simon Fraser University</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/SorourMo/38-Cloud-A-Cloud-Segmentation-Dataset">https://github.com/SorourMo/38-Cloud-A-Cloud-Segmentation-Dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56236">https://aistudio.baidu.com/aistudio/datasetdetail/56236</a></li></ul><h2 id="WHU-Building-Dataset-Satellite-dataset-Ⅰ-global-cities"><a href="#WHU-Building-Dataset-Satellite-dataset-Ⅰ-global-cities" class="headerlink" title="WHU Building Dataset,Satellite dataset Ⅰ (global cities)"></a>WHU Building Dataset,Satellite dataset Ⅰ (global cities)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>1</td><td>204</td><td>QuickBird, Worldview series, IKONOS, ZY-3</td><td>2019</td><td>武汉大学</td><td>0.3 ~ 2.5m</td></tr></tbody></table><ul><li>源地址：<a href="https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html">https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55774">https://aistudio.baidu.com/aistudio/datasetdetail/55774</a></li></ul><h2 id="WHU-Building-Dataset-Satellite-dataset-Ⅱ-East-Asia"><a href="#WHU-Building-Dataset-Satellite-dataset-Ⅱ-East-Asia" class="headerlink" title="WHU Building Dataset,Satellite dataset Ⅱ (East Asia)"></a>WHU Building Dataset,Satellite dataset Ⅱ (East Asia)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>1</td><td>17388</td><td>QuickBird, Worldview series, IKONOS, ZY-3</td><td>2019</td><td>武汉大学</td><td>0.45 m</td></tr></tbody></table><ul><li>源地址：<a href="https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html">https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56356">https://aistudio.baidu.com/aistudio/datasetdetail/56356</a></li></ul><h2 id="WHU-Building-Dataset-Aerial-imagery-dataset"><a href="#WHU-Building-Dataset-Aerial-imagery-dataset" class="headerlink" title="WHU Building Dataset,Aerial imagery dataset"></a>WHU Building Dataset,Aerial imagery dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>1</td><td>8,189</td><td>未知</td><td>2019</td><td>武汉大学</td><td>0.3 m</td></tr></tbody></table><ul><li>源地址：<a href="https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html">https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56502">https://aistudio.baidu.com/aistudio/datasetdetail/56502</a></li></ul><h2 id="DroneDeploy"><a href="#DroneDeploy" class="headerlink" title="DroneDeploy"></a>DroneDeploy</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">6000± * 6000± * 3</td><td>7</td><td>35 train, 8 val, 12 test</td><td>航空影像drones</td><td>2019</td><td>DroneDeploy</td><td>0.1 m</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/dronedeploy/dd-ml-segmentation-benchmark">https://github.com/dronedeploy/dd-ml-segmentation-benchmark</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/79283">https://aistudio.baidu.com/aistudio/datasetdetail/79283</a></li></ul><h2 id="RoadTracer"><a href="#RoadTracer" class="headerlink" title="RoadTracer"></a>RoadTracer</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">4096 * 4096 * 3</td><td>1</td><td>3000</td><td>Google earth、OSM</td><td>2019</td><td>MIT</td><td>0.6m</td></tr></tbody></table><ul><li>源地址：<a href="%20https://github.com/mitroadmaps/roadtracer/">https://github.com/mitroadmaps/roadtracer/</a><br><a href="https://github.com/tansor/VecRoad">https://github.com/tansor/VecRoad</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/74848">https://aistudio.baidu.com/aistudio/datasetdetail/74848</a></li></ul><h2 id="ORSSD"><a href="#ORSSD" class="headerlink" title="ORSSD"></a>ORSSD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500± * 500± * 3</td><td>8</td><td>600train，200test</td><td>Google Earth</td><td>2019</td><td>北京交通大学</td></tr></tbody></table><ul><li>源地址：<a href="https://hub.fastgit.org/rmcong/ORSSD-dataset">https://hub.fastgit.org/rmcong/ORSSD-dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/98275">https://aistudio.baidu.com/aistudio/datasetdetail/98275</a></li></ul><h2 id="EORSSD"><a href="#EORSSD" class="headerlink" title="EORSSD"></a>EORSSD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500± * 500± * 3</td><td>8</td><td>1400train, 600test</td><td>Google Earth</td><td>2020</td><td>北京交通大学</td></tr></tbody></table><ul><li>源地址：[<a href="https://hub.fastgit.org/rmcong/EORSSD-dataset">https://hub.fastgit.org/rmcong/EORSSD-dataset</a> ]<a href="https://hub.fastgit.org/rmcong/EORSSD-dataset">https://hub.fastgit.org/rmcong/EORSSD-dataset</a> )</li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/98372">https://aistudio.baidu.com/aistudio/datasetdetail/98372</a></li></ul><h2 id="Land-Cover-from-Aerial-Imagery（landcover-ai）"><a href="#Land-Cover-from-Aerial-Imagery（landcover-ai）" class="headerlink" title="Land Cover from Aerial Imagery（landcover_ai）"></a>Land Cover from Aerial Imagery（landcover_ai）</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">9000 * 9500 * 3，4200 * 4700 * 3</td><td>3</td><td>41</td><td>public geodetic resource</td><td>2020</td><td>linuxpolska</td><td>0.25m，0.5m</td></tr></tbody></table><ul><li>源地址：<a href="https://landcover.ai/">https://landcover.ai/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/76629">https://aistudio.baidu.com/aistudio/datasetdetail/76629</a></li></ul><h2 id="UAVid"><a href="#UAVid" class="headerlink" title="UAVid"></a>UAVid</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4096 * 2160 * 3 or 3840 * 2160 * 3</td><td>8</td><td>300</td><td>航空影像</td><td>2020</td><td>University of Twente</td></tr></tbody></table><ul><li>源地址：<a href="https://www.uavid.nl/">https://www.uavid.nl/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55774">https://aistudio.baidu.com/aistudio/datasetdetail/55774</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/14y_il9Axq_vMYQGdfz9JqQ%20">https://pan.baidu.com/s/14y_il9Axq_vMYQGdfz9JqQ</a> 提取码：4dv7</li></ul><h2 id="95-Cloud-An-Extension-to-38-Cloud-Dataset"><a href="#95-Cloud-An-Extension-to-38-Cloud-Dataset" class="headerlink" title="95-Cloud: An Extension to 38-Cloud Dataset"></a>95-Cloud: An Extension to 38-Cloud Dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">384 * 384 * 4</td><td>1</td><td>34,701</td><td>Landsat 8</td><td>2020</td><td>Simon Fraser University</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/SorourMo/95-Cloud-An-Extension-to-38-Cloud-Dataset">https://github.com/SorourMo/95-Cloud-An-Extension-to-38-Cloud-Dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56839">https://aistudio.baidu.com/aistudio/datasetdetail/56839</a></li></ul><h2 id="AI-遥感影像"><a href="#AI-遥感影像" class="headerlink" title="AI+遥感影像"></a>AI+遥感影像</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>8</td><td>100000</td><td>未知</td><td>2020</td><td>全国人工智能大赛组委会</td></tr></tbody></table><ul><li>源地址：<a href="https://www.datafountain.cn/competitions/457">https://www.datafountain.cn/competitions/457</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/51568">https://aistudio.baidu.com/aistudio/datasetdetail/51568</a></li></ul><h2 id="BDCI2020"><a href="#BDCI2020" class="headerlink" title="BDCI2020"></a>BDCI2020</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>7</td><td>145981</td><td>未知</td><td>2020</td><td>BDCI</td></tr></tbody></table><ul><li>源地址：<a href="https://www.datafountain.cn/competitions/475">https://www.datafountain.cn/competitions/475</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56051">https://aistudio.baidu.com/aistudio/datasetdetail/56051</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1XiECAvdc0UIOpyPHMvR09w">https://pan.baidu.com/s/1XiECAvdc0UIOpyPHMvR09w</a>  提取码：71sz</li></ul><h2 id="mini-Inria-Aerial-Image-Labeling-Dataset"><a href="#mini-Inria-Aerial-Image-Labeling-Dataset" class="headerlink" title="mini Inria Aerial Image Labeling Dataset"></a>mini Inria Aerial Image Labeling Dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>1</td><td>30000 train, 2500 test</td><td>航空影像</td><td>2021</td><td>天池大赛</td><td>0.3 m</td></tr></tbody></table><ul><li>源地址：<a href="https://tianchi.aliyun.com/competition/entrance/531872/introduction">https://tianchi.aliyun.com/competition/entrance/531872/introduction</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/70361">https://aistudio.baidu.com/aistudio/datasetdetail/70361</a></li></ul><h2 id="ISPRS-2D-Semantic-Labeling-Contest"><a href="#ISPRS-2D-Semantic-Labeling-Contest" class="headerlink" title="ISPRS 2D Semantic Labeling Contest"></a>ISPRS 2D Semantic Labeling Contest</h2><h3 id="Postdam"><a href="#Postdam" class="headerlink" title="Postdam"></a>Postdam</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">6000 * 6000 * 3</td><td>6</td><td>38</td><td>航空影像</td><td>2012</td><td>ISPRS</td><td>0.05m</td></tr></tbody></table><h3 id="Vaihingen"><a href="#Vaihingen" class="headerlink" title="Vaihingen"></a>Vaihingen</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">1000<del>4000 * 1000</del>4000 * 3</td><td>6</td><td>33</td><td>航空影像</td><td>2012</td><td>ISPRS</td><td>0.09m</td></tr></tbody></table><ul><li>源地址：<a href="https://www2.isprs.org/commissions/comm2/wg4/benchmark/semantic-labeling/">https://www2.isprs.org/commissions/comm2/wg4/benchmark/semantic-labeling/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55153">https://aistudio.baidu.com/aistudio/datasetdetail/55153</a>  ； <a href="https://aistudio.baidu.com/aistudio/datasetdetail/55408">https://aistudio.baidu.com/aistudio/datasetdetail/55408</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1Mu-rzPorbtsYr1XJpkJq-Q">https://pan.baidu.com/s/1Mu-rzPorbtsYr1XJpkJq-Q</a>  提取码：n4i0</li></ul><h2 id="GID-1"><a href="#GID-1" class="headerlink" title="GID"></a>GID</h2><h3 id="GID-Fine-Land-cover-Classification-15classes"><a href="#GID-Fine-Land-cover-Classification-15classes" class="headerlink" title="GID Fine Land-cover Classification_15classes"></a>GID Fine Land-cover Classification_15classes</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">7200 * 6800 * 4 <br>7200 * 6800 * 3</td><td>15</td><td>10</td><td>高分2</td><td>2018</td><td>武汉大学</td><td>0.8 to 10m</td></tr></tbody></table><h3 id="GID-Large-scale-Classification-5classes"><a href="#GID-Large-scale-Classification-5classes" class="headerlink" title="GID Large-scale Classification_5classes"></a>GID Large-scale Classification_5classes</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">7200 * 6800 * 4 <br>7200 * 6800 * 3</td><td>5</td><td>150</td><td>高分2</td><td>2018</td><td>武汉大学</td><td>0.8 to 10m</td></tr></tbody></table><ul><li>源地址：<a href="https://x-ytong.github.io/project/GID.html">https://x-ytong.github.io/project/GID.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54878">https://aistudio.baidu.com/aistudio/datasetdetail/54878</a>  ； <a href="https://aistudio.baidu.com/aistudio/datasetdetail/54934">https://aistudio.baidu.com/aistudio/datasetdetail/54934</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1Xux-3nKTbu1v3Mp5tdyM7A">https://pan.baidu.com/s/1Xux-3nKTbu1v3Mp5tdyM7A</a>  提取码：d3qs</li></ul><h2 id="UDD"><a href="#UDD" class="headerlink" title="UDD"></a>UDD</h2><h3 id="UDD5"><a href="#UDD5" class="headerlink" title="UDD5"></a>UDD5</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4096± * 2160± * 3</td><td>5</td><td>120 trian，40 val</td><td>无人机数据（DJI Phantom 4）</td><td>2018</td><td>北京大学</td></tr></tbody></table><h3 id="UDD6"><a href="#UDD6" class="headerlink" title="UDD6"></a>UDD6</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4096± * 2160± * 3</td><td>6</td><td>106 trian，35 val</td><td>无人机数据（DJI Phantom 4）</td><td>2018</td><td>北京大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/MarcWong/UDD">https://github.com/MarcWong/UDD</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/75675">https://aistudio.baidu.com/aistudio/datasetdetail/75675</a></li></ul><h2 id="BH-DATASET"><a href="#BH-DATASET" class="headerlink" title="BH-DATASET"></a>BH-DATASET</h2><h3 id="BH-POOLS"><a href="#BH-POOLS" class="headerlink" title="BH-POOLS"></a>BH-POOLS</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">3840 * 2160 * 3</td><td>1</td><td>200</td><td>GoogleEarth</td><td>2020</td><td>Federal University of Minas Gerais</td></tr></tbody></table><h3 id="BH-WATERTANKS"><a href="#BH-WATERTANKS" class="headerlink" title="BH-WATERTANKS"></a>BH-WATERTANKS</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">3840 * 2160 * 3</td><td>1</td><td>200</td><td>GoogleEarth</td><td>2020</td><td>Federal University of Minas Gerais</td></tr></tbody></table><ul><li>源地址：<a href="http://www.patreo.dcc.ufmg.br/2020/07/29/bh-pools-watertanks-datasets/">http://www.patreo.dcc.ufmg.br/2020/07/29/bh-pools-watertanks-datasets/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/57579">https://aistudio.baidu.com/aistudio/datasetdetail/57579</a></li></ul><h2 id="Inria-Aerial-Image-Labeling-Dataset"><a href="#Inria-Aerial-Image-Labeling-Dataset" class="headerlink" title="Inria Aerial Image Labeling Dataset"></a>Inria Aerial Image Labeling Dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">未知</td><td>1</td><td>未知</td><td>航空影像</td><td>2017</td><td>Inria Sophia Antipolis - Mediterran ee, TITANE team; Inria Saclay, TAO team, France</td><td>0.3 m</td></tr></tbody></table><ul><li>源地址：<a href="https://project.inria.fr/aerialimagelabeling/">https://project.inria.fr/aerialimagelabeling/</a> ；<a href="https://hyper.ai/datasets/5428">https://hyper.ai/datasets/5428</a></li></ul><h2 id="rscup-2"><a href="#rscup-2" class="headerlink" title="rscup"></a>rscup</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">7200 * 6800 * 4</td><td>16</td><td>train 8, val 2, test 10</td><td>高分二号MSS影像</td><td>2019</td><td>rscup组委会</td><td>4 m</td></tr></tbody></table><ul><li>源地址：<a href="http://rscup.bjxintong.com.cn/#/theme/3">http://rscup.bjxintong.com.cn/#/theme/3</a></li></ul><h2 id="suichang-dataset"><a href="#suichang-dataset" class="headerlink" title="suichang dataset"></a>suichang dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">256 * 256 * 4</td><td>10</td><td>16017</td><td>高分系列</td><td>2021</td><td>浙江大学、天池大赛</td><td>0.8~ 2m</td></tr></tbody></table><ul><li>源地址：<a href="https://tianchi.aliyun.com/competition/entrance/531860/rankingList">https://tianchi.aliyun.com/competition/entrance/531860/rankingList</a></li></ul><h2 id="LRSNY"><a href="#LRSNY" class="headerlink" title="LRSNY"></a>LRSNY</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">1000 *  1000 * 3</td><td>1</td><td>716 train, 220 val, 432 test</td><td>未知</td><td>2021</td><td>IEEE</td><td>0.5 m</td></tr></tbody></table><ul><li>源地址：<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9333652">https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9333652</a><br><a href="ftp://154.85.52.76/LRSNY/">ftp://154.85.52.76/LRSNY/</a></li></ul><h1 id="遥感影像变化检测"><a href="#遥感影像变化检测" class="headerlink" title="遥感影像变化检测"></a>遥感影像变化检测</h1><p><strong>收集网络中开源的、关于遥感影像 变化检测的深度学习数据集</strong></p><p>main ref </p><blockquote><p><a href="https://captain-whu.github.io/DiRS/">https://captain-whu.github.io/DiRS/</a><br><a href="https://mp.weixin.qq.com/s/GN9SEztb61cm0ukn81Szbg">https://mp.weixin.qq.com/s/GN9SEztb61cm0ukn81SzbgA</a></p></blockquote><h2 id="SZTAKI-INRIA-AirChange"><a href="#SZTAKI-INRIA-AirChange" class="headerlink" title="SZTAKI-INRIA AirChange"></a>SZTAKI-INRIA AirChange</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">952 * 640 * 3, 2 time</td><td>1</td><td>13 * 2</td><td>未知</td><td>2009</td><td>MTA SZTAKI</td><td>1.5m</td></tr></tbody></table><ul><li>源地址：<a href="http://web.eee.sztaki.hu/remotesensing/airchange_benchmark.html">http://web.eee.sztaki.hu/remotesensing/airchange_benchmark.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/77781">https://aistudio.baidu.com/aistudio/datasetdetail/77781</a></li></ul><h2 id="AIST-Building-Change-Detection-ABCD"><a href="#AIST-Building-Change-Detection-ABCD" class="headerlink" title="AIST Building Change Detection(ABCD)"></a>AIST Building Change Detection(ABCD)</h2><h3 id="fixed-scale"><a href="#fixed-scale" class="headerlink" title="fixed-scale"></a>fixed-scale</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">160 * 160 * 6, 2 time</td><td>1</td><td>4,253 * 2</td><td>aerial images</td><td>2017</td><td>AIST</td><td>1.5m</td></tr></tbody></table><h3 id="resized"><a href="#resized" class="headerlink" title="resized"></a>resized</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">128 * 128 * 3, 2 time</td><td>1</td><td>4,223 * 2</td><td>aerial images</td><td>2017</td><td>AIST</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/gistairc/ABCDdataset">https://github.com/gistairc/ABCDdataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/79596">https://aistudio.baidu.com/aistudio/datasetdetail/79596</a></li></ul><h2 id="WHU-Building-Change-Detection-Dataset"><a href="#WHU-Building-Change-Detection-Dataset" class="headerlink" title="WHU Building Change Detection Dataset"></a>WHU Building Change Detection Dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">32,207 * 15,354 * 3, 2 time</td><td>1</td><td>1 * 2</td><td>航空影像</td><td>2018</td><td>武汉大学</td><td>0.2m</td></tr></tbody></table><ul><li>源地址：<a href="https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html">https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/79035">https://aistudio.baidu.com/aistudio/datasetdetail/79035</a></li></ul><h2 id="season-varying"><a href="#season-varying" class="headerlink" title="season-varying"></a>season-varying</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3, 2 time</td><td>1</td><td>10000train, 3000val, 3000test</td><td>Google Earth (DigitalGlobe)</td><td>2018</td><td>GosNIIAS</td><td>0.03~1m</td></tr></tbody></table><ul><li>源地址：<a href="https://paperswithcode.com/dataset/cdd-dataset-season-varying">https://paperswithcode.com/dataset/cdd-dataset-season-varying</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/78676">https://aistudio.baidu.com/aistudio/datasetdetail/78676</a></li></ul><h2 id="Onera-Satellite-Change-Detection-OSCD"><a href="#Onera-Satellite-Change-Detection-OSCD" class="headerlink" title="Onera Satellite Change Detection (OSCD)"></a>Onera Satellite Change Detection (OSCD)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">600± * 600± * 13, 2 time</td><td>1</td><td>14 * 2 train, 10 * 2 test</td><td>Sentinel-2</td><td>2018</td><td>Universit´e Paris-Saclay、 T´el´ecom ParisTech</td></tr></tbody></table><ul><li>源地址：<a href="https://rcdaudt.github.io/oscd/">https://rcdaudt.github.io/oscd/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/72898">https://aistudio.baidu.com/aistudio/datasetdetail/72898</a></li></ul><h2 id="Multi-temporal-Scene-WuHan-MtS-WH"><a href="#Multi-temporal-Scene-WuHan-MtS-WH" class="headerlink" title="Multi-temporal Scene WuHan (MtS-WH)"></a>Multi-temporal Scene WuHan (MtS-WH)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">7200 * 6000 * 4，2 time</td><td>9</td><td>190 * 2</td><td>IKONOS传感器</td><td>2019</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="http://sigma.whu.edu.cn/newspage.php?q=2019_03_26">http://sigma.whu.edu.cn/newspage.php?q=2019_03_26</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/70452">https://aistudio.baidu.com/aistudio/datasetdetail/70452</a></li></ul><h2 id="High-Resolution-Semantic-Change-HRSCD"><a href="#High-Resolution-Semantic-Change-HRSCD" class="headerlink" title="High Resolution Semantic Change (HRSCD)"></a>High Resolution Semantic Change (HRSCD)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">10000 * 10000 * 3, 2 time</td><td>6</td><td>291</td><td>IGS’s BD ORTHO database</td><td>2019</td><td>ETH Zürich / EcoVision Lab</td><td>0.5 m</td></tr></tbody></table><ul><li>源地址：<a href="https://rcdaudt.github.io/hrscd/">https://rcdaudt.github.io/hrscd/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/87088">https://aistudio.baidu.com/aistudio/datasetdetail/87088</a></li></ul><h2 id="Change-Detection-Dataset-CDD"><a href="#Change-Detection-Dataset-CDD" class="headerlink" title="Change Detection Dataset(CDD)"></a>Change Detection Dataset(CDD)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">984 * 740 * 224,  600 * 500 * 224, 390 * 200 * 242,  2 time</td><td>3,5</td><td>3 * 2</td><td>HYPERION,AVIRIS sensor</td><td>2019</td><td>未知</td><td>未知</td></tr></tbody></table><ul><li>源地址：<a href="https://gitlab.citius.usc.es/hiperespectral/ChangeDetectionDataset">https://gitlab.citius.usc.es/hiperespectral/ChangeDetectionDataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/89523">https://aistudio.baidu.com/aistudio/datasetdetail/89523</a></li></ul><h2 id="xBD"><a href="#xBD" class="headerlink" title="xBD"></a>xBD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1024 * 1024,  3/4/8 band, 4 state</td><td>4</td><td>22068</td><td>DigitalGlobe</td><td>2020</td><td>MIT</td></tr></tbody></table><ul><li>源地址：<a href="https://xview2.org/dataset">https://xview2.org/dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/86660">https://aistudio.baidu.com/aistudio/datasetdetail/86660</a></li></ul><h2 id="Google-dataset"><a href="#Google-dataset" class="headerlink" title="Google dataset"></a>Google dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">1000<del>5000 * 1000</del>5000 * 3, 2 time</td><td>1</td><td>20</td><td>Google Earth</td><td>2020</td><td>ieee</td><td>0.55 m</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/daifeng2016/Change-Detection-Dataset-for-High-Resolution-Satellite-Imagery">https://github.com/daifeng2016/Change-Detection-Dataset-for-High-Resolution-Satellite-Imagery</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/75099">https://aistudio.baidu.com/aistudio/datasetdetail/75099</a></li></ul><h2 id="LEVIR-CD"><a href="#LEVIR-CD" class="headerlink" title="LEVIR-CD"></a>LEVIR-CD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">1024 * 1024 * 3, 2 time</td><td>1</td><td>637</td><td>Google Earth</td><td>2020</td><td>北京航空航天大学</td><td>0.5 m</td></tr></tbody></table><ul><li>源地址：<a href="https://justchenhao.github.io/LEVIR/">https://justchenhao.github.io/LEVIR/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/75459">https://aistudio.baidu.com/aistudio/datasetdetail/75459</a></li></ul><h2 id="SenseEarth-ChangeDetection"><a href="#SenseEarth-ChangeDetection" class="headerlink" title="SenseEarth ChangeDetection"></a>SenseEarth ChangeDetection</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3, 2 time</td><td>6</td><td>2968 train, 847 val</td><td>未知</td><td>2020</td><td>商汤科技</td><td>0.5~3m</td></tr></tbody></table><ul><li>源地址：<a href="https://rs.sensetime.com/competition/index.html#/info">https://rs.sensetime.com/competition/index.html#/info</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53484">https://aistudio.baidu.com/aistudio/datasetdetail/53484</a></li></ul><h2 id="SEmantic-Change-detectiON-Data-SECOND"><a href="#SEmantic-Change-detectiON-Data-SECOND" class="headerlink" title="SEmantic Change detectiON Data(SECOND)"></a>SEmantic Change detectiON Data(SECOND)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3, 2 time</td><td>6</td><td>4662 * 2</td><td>未知</td><td>2020</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="http://www.captain-whu.com/project/SCD/">http://www.captain-whu.com/project/SCD/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/87088">https://aistudio.baidu.com/aistudio/datasetdetail/87088</a></li></ul><h2 id="Sun-Yat-Sen-University-SYSU-CD"><a href="#Sun-Yat-Sen-University-SYSU-CD" class="headerlink" title="Sun Yat-Sen University (SYSU)-CD"></a>Sun Yat-Sen University (SYSU)-CD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3, 2 time</td><td>1</td><td>20000 * 2</td><td>aerial image</td><td>2021</td><td>中山大学</td></tr></tbody></table><ul><li>源地址：<a href="https://hub.fastgit.org/liumency/SYSU-CD">https://hub.fastgit.org/liumency/SYSU-CD</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/98596">https://aistudio.baidu.com/aistudio/datasetdetail/98596</a></li></ul><h2 id="rscup-3"><a href="#rscup-3" class="headerlink" title="rscup"></a>rscup</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">960 * 960 * 4, 2 time</td><td>1</td><td>未知</td><td>未知</td><td>2020</td><td>rscup组委会</td></tr></tbody></table><ul><li>源地址：<a href="http://rscup.bjxintong.com.cn/#/theme/4">http://rscup.bjxintong.com.cn/#/theme/4</a></li></ul><h1 id="遥感影像场景分类——多-x2F-高光谱"><a href="#遥感影像场景分类——多-x2F-高光谱" class="headerlink" title="遥感影像场景分类——多/高光谱"></a>遥感影像场景分类——多/高光谱</h1><p><strong>收集网络中开源的、关于高光谱遥感影像 图像分类的深度学习数据集</strong></p><h2 id="EuroSAT"><a href="#EuroSAT" class="headerlink" title="EuroSAT"></a>EuroSAT</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">64 * 64 * 3<br>64 * 64 * 13</td><td>10</td><td>27,000</td><td>10 m</td><td>哨兵2</td><td>2018</td><td>德国凯泽斯劳滕大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/phelber/eurosat">https://github.com/phelber/eurosat</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52650">https://aistudio.baidu.com/aistudio/datasetdetail/52650</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/15PEr2iWIq-U0kBF7Jp8Fmg">https://pan.baidu.com/s/15PEr2iWIq-U0kBF7Jp8Fmg</a>  提取码：d34f</li></ul><h2 id="TG1HRSSC"><a href="#TG1HRSSC" class="headerlink" title="TG1HRSSC"></a>TG1HRSSC</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th><th>空间分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 1，512 * 512 * 54，512 * 512 * 52</td><td>9</td><td>204</td><td>天宫一号</td><td>2021</td><td>中国科学院空间应用工程与技术中心</td><td>0.5—0.8μm、1band(PAN), 0.4—1.0μm、54band((VNI), 1.0—2.5μm、52band((SWI),</td><td>5m(PAN), 10m(VNI), 20m(SWI)</td></tr></tbody></table><ul><li>源地址：<a href="http://www.msadc.cn/main/setsubDetail?id=1369487569196158978">http://www.msadc.cn/main/setsubDetail?id=1369487569196158978</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/86229">https://aistudio.baidu.com/aistudio/datasetdetail/86229</a></li></ul><h2 id="NaSC-TG2"><a href="#NaSC-TG2" class="headerlink" title="NaSC-TG2"></a>NaSC-TG2</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th><th>空间分辨率</th></tr></thead><tbody><tr><td align="left">128 * 128 * 3， 128 * 128 * 14</td><td>10</td><td>20000</td><td>天宫二号</td><td>2021</td><td>中国科学院空间应用工程与技术中心</td><td>0.40–1.04 µm</td><td></td></tr></tbody></table><ul><li>源地址：<a href="http://www.msadc.cn/main/setsubDetail?id=1370312964720037889">http://www.msadc.cn/main/setsubDetail?id=1370312964720037889</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/86451">https://aistudio.baidu.com/aistudio/datasetdetail/86451</a></li></ul><h1 id="遥感影像目标检测——多-x2F-高光谱"><a href="#遥感影像目标检测——多-x2F-高光谱" class="headerlink" title="遥感影像目标检测——多/高光谱"></a>遥感影像目标检测——多/高光谱</h1><p><strong>收集网络中开源的、关于高光谱遥感影像 图像分类的深度学习数据集</strong></p><h2 id="Dstl-Satellite-Imagery-Feature-Detection"><a href="#Dstl-Satellite-Imagery-Feature-Detection" class="headerlink" title="Dstl Satellite Imagery Feature Detection"></a>Dstl Satellite Imagery Feature Detection</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">13348 * 3392 * 3，837 * 848 * 8，134 * 136 * 8</td><td>10</td><td>25 train, 32 test</td><td>250+</td><td>MultiPolygons</td><td>WorldView 3</td><td>2017</td><td>DigitalGlobe</td><td>全色0.31 m, 多光谱1.24 m, 短波红外7.5 m</td></tr></tbody></table><ul><li>源地址：<a href="https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection/data">https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection/data</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/87506">https://aistudio.baidu.com/aistudio/datasetdetail/87506</a></li></ul><h2 id="xView"><a href="#xView" class="headerlink" title="xView"></a>xView</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">3000± * 3000± * 3<br>3000± * 3000± * 8</td><td>60</td><td>1129</td><td>1,000,000</td><td>0.3m</td><td>HBB</td><td>WorldView 3</td><td>2018</td><td>DIUx、NGA</td></tr></tbody></table><ul><li>源地址：<a href="https://challenge.xviewdataset.org/data-download">https://challenge.xviewdataset.org/data-download</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53622">https://aistudio.baidu.com/aistudio/datasetdetail/53622</a></li></ul><h1 id="遥感影像图像分割——多-x2F-高光谱"><a href="#遥感影像图像分割——多-x2F-高光谱" class="headerlink" title="遥感影像图像分割——多/高光谱"></a>遥感影像图像分割——多/高光谱</h1><p><strong>收集网络中开源的、关于高光谱遥感影像 图像分割的深度学习数据集</strong></p><p>main ref </p><blockquote><p><a href="https://mp.weixin.qq.com/s/S_TEoWyYFtdjtDeEXmpBPg">https://mp.weixin.qq.com/s/S_TEoWyYFtdjtDeEXmpBPg</a><br><a href="http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes">http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes</a></p></blockquote><h2 id="Salinas"><a href="#Salinas" class="headerlink" title="Salinas"></a>Salinas</h2><h3 id="Salinas-scene"><a href="#Salinas-scene" class="headerlink" title="Salinas scene"></a>Salinas scene</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">512 * 217 * 224</td><td>16</td><td>1</td><td>AVIRIS sensor</td><td>2011</td><td>未知</td></tr></tbody></table><h3 id="Salinas-A-scene"><a href="#Salinas-A-scene" class="headerlink" title="Salinas-A scene"></a>Salinas-A scene</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">83 * 86 * 224</td><td>6</td><td>1</td><td>AVIRIS sensor</td><td>2011</td><td>未知</td></tr></tbody></table><ul><li>源地址：<a href="http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes">http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/82020">https://aistudio.baidu.com/aistudio/datasetdetail/82020</a></li></ul><h2 id="Pavia-Centre-and-University"><a href="#Pavia-Centre-and-University" class="headerlink" title="Pavia Centre and University"></a>Pavia Centre and University</h2><h3 id="Pavia-Centre-scene"><a href="#Pavia-Centre-scene" class="headerlink" title="Pavia Centre scene"></a>Pavia Centre scene</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th></tr></thead><tbody><tr><td align="left">1096 * 1096 * 102</td><td>9</td><td>1</td><td>ROSIS sensor</td><td>2011</td><td>NPavia university</td><td>0.43-0.86μm</td></tr></tbody></table><h3 id="Pavia-University-scene"><a href="#Pavia-University-scene" class="headerlink" title="Pavia University scene"></a>Pavia University scene</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th></tr></thead><tbody><tr><td align="left">610 * 610 * 103</td><td>9</td><td>1</td><td>ROSIS sensor)</td><td>2011</td><td>Pavia university</td><td>0.43-0.86μm</td></tr></tbody></table><ul><li>源地址：<a href="http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes">http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/81260">https://aistudio.baidu.com/aistudio/datasetdetail/81260</a></li></ul><h2 id="Washington-DC-MALL"><a href="#Washington-DC-MALL" class="headerlink" title="Washington DC MALL"></a>Washington DC MALL</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th></tr></thead><tbody><tr><td align="left">1280 * 307 * 191</td><td>7</td><td>1</td><td>机载高光谱数据</td><td>2013</td><td>Spectral Information Technology Application Center of Virginia</td><td>0.4到2.4 µm</td></tr></tbody></table><ul><li>源地址：<a href="http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes">http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/83251">https://aistudio.baidu.com/aistudio/datasetdetail/83251</a></li></ul><h2 id="IKennedy-Space-Center-KSC"><a href="#IKennedy-Space-Center-KSC" class="headerlink" title="IKennedy Space Center (KSC)"></a>IKennedy Space Center (KSC)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th></tr></thead><tbody><tr><td align="left">512 * 614 * 176</td><td>13</td><td>1</td><td>NASA AVIRIS</td><td>2014</td><td>Pavia university</td><td>400–2500 nm</td></tr></tbody></table><ul><li>源地址：<a href="http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes">http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/81260">https://aistudio.baidu.com/aistudio/datasetdetail/81260</a></li></ul><h2 id="Botswana"><a href="#Botswana" class="headerlink" title="Botswana"></a>Botswana</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th></tr></thead><tbody><tr><td align="left">1476 * 256 * 145</td><td>14</td><td>1</td><td>Hyperion sensor on EO-1</td><td>2014</td><td>未知</td><td>未知</td></tr></tbody></table><ul><li>源地址：<a href="http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes">http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/82578">https://aistudio.baidu.com/aistudio/datasetdetail/82578</a></li></ul><h2 id="Indian-Pines"><a href="#Indian-Pines" class="headerlink" title="Indian Pines"></a>Indian Pines</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th></tr></thead><tbody><tr><td align="left">145 * 145 * 224；614 * 1848 * 224；2678 * 614 * 224</td><td>16</td><td>3</td><td>AVIRIS sensor</td><td>2015</td><td>Pursue univeristy</td><td>400–2500 nm</td></tr></tbody></table><ul><li>源地址：<a href="https://purr.purdue.edu/publications/1947/1">https://purr.purdue.edu/publications/1947/1</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/80970">https://aistudio.baidu.com/aistudio/datasetdetail/80970</a></li></ul><h2 id="HyRANK"><a href="#HyRANK" class="headerlink" title="HyRANK"></a>HyRANK</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">250± * 1000± * 176</td><td>14</td><td>2train, 3val</td><td>Hyperion sensor (EO-1, USGS)</td><td>2018</td><td>National Technical University of Athen</td></tr></tbody></table><ul><li>源地址：<a href="https://www2.isprs.org/commissions/comm3/wg4/hyrank/">https://www2.isprs.org/commissions/comm3/wg4/hyrank/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/80840">https://aistudio.baidu.com/aistudio/datasetdetail/80840</a></li></ul><h2 id="RIT-18"><a href="#RIT-18" class="headerlink" title="RIT-18"></a>RIT-18</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">9,393 * 5,642 * 7；8,833 * 6,918 * 7；12,446 * 7,654 * 7</td><td>18</td><td>3</td><td>Tetracam Micro-MCA6</td><td>2018</td><td>Rochester Institute of Technology</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/rmkemker/RIT-18">https://github.com/rmkemker/RIT-18</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/98991">https://aistudio.baidu.com/aistudio/datasetdetail/98991</a></li></ul><h1 id="遥感影像多标签分类"><a href="#遥感影像多标签分类" class="headerlink" title="遥感影像多标签分类"></a>遥感影像多标签分类</h1><h2 id="MLRSNet"><a href="#MLRSNet" class="headerlink" title="MLRSNet"></a>MLRSNet</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th><th>最大标签数</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>46</td><td>109,161</td><td>Google Earth</td><td>2020</td><td>中国地质大学</td><td>0.1~10m</td><td>13</td></tr></tbody></table><ul><li>源地址：<a href="https://data.mendeley.com/datasets/7j9bv9vwsx/2">https://data.mendeley.com/datasets/7j9bv9vwsx/2</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/76804">https://aistudio.baidu.com/aistudio/datasetdetail/76804</a></li></ul><h2 id="BigEarthNet"><a href="#BigEarthNet" class="headerlink" title="BigEarthNet"></a>BigEarthNet</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>备注</th></tr></thead><tbody><tr><td align="left">120 * 120 <br> 60 * 60 <br> 20 * 20;</td><td>43</td><td>590326</td><td>10 m <br> 20 m <br> 60m</td><td>哨兵2</td><td>2019</td><td>柏林工业大学</td><td>多标签</td></tr></tbody></table><ul><li>源地址：<a href="http://bigearth.net/">http://bigearth.net/</a></li></ul><h1 id="遥感影像图像标题"><a href="#遥感影像图像标题" class="headerlink" title="遥感影像图像标题"></a>遥感影像图像标题</h1><p><strong>收集网络中开源的、关于图像标题的深度学习数据集</strong></p><p>main ref </p><blockquote><p><a href="https://github.com/201528014227051/RSICD_optimal">https://github.com/201528014227051/RSICD_optimal</a></p></blockquote><h2 id="UCM-caption"><a href="#UCM-caption" class="headerlink" title="UCM caption"></a>UCM caption</h2><table><thead><tr><th align="left">图像大小</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>名词数</th><th>句子总数</th><th>单样本句数</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>2100</td><td>USGS National Map</td><td>2016</td><td>中科院</td><td>21</td><td>10500</td><td>5</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/201528014227051/RSICD_optimal">https://github.com/201528014227051/RSICD_optimal</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/90740">https://aistudio.baidu.com/aistudio/datasetdetail/90740</a></li></ul><h2 id="Sydney-caption"><a href="#Sydney-caption" class="headerlink" title="Sydney caption"></a>Sydney caption</h2><table><thead><tr><th align="left">图像大小</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>名词数</th><th>句子总数</th><th>单样本句数</th></tr></thead><tbody><tr><td align="left">500 * 500 * 3</td><td>613</td><td>Google Earth</td><td>2016</td><td>中科院</td><td>7</td><td>3065</td><td>5</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/201528014227051/RSICD_optimal">https://github.com/201528014227051/RSICD_optimal</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/91126">https://aistudio.baidu.com/aistudio/datasetdetail/91126</a></li></ul><h2 id="RSICD"><a href="#RSICD" class="headerlink" title="RSICD"></a>RSICD</h2><table><thead><tr><th align="left">图像大小</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>名词数</th><th>句子总数</th><th>单样本句数</th></tr></thead><tbody><tr><td align="left">224 * 244 * 3</td><td>10921</td><td>Google Earth, Baidu Map, MapABC, Tianditu</td><td>2017</td><td>IEEE</td><td>30</td><td>2433</td><td>5</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/201528014227051/RSICD_optimal">https://github.com/201528014227051/RSICD_optimal</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/90307">https://aistudio.baidu.com/aistudio/datasetdetail/90307</a></li></ul><h1 id="遥感影像视频目标检测"><a href="#遥感影像视频目标检测" class="headerlink" title="遥感影像视频目标检测"></a>遥感影像视频目标检测</h1><h1 id="遥感影像视频跟踪"><a href="#遥感影像视频跟踪" class="headerlink" title="遥感影像视频跟踪"></a>遥感影像视频跟踪</h1><p><strong>收集网络中开源的、关于图像标题的深度学习数据集</strong></p><h2 id="UAV123"><a href="#UAV123" class="headerlink" title="UAV123"></a>UAV123</h2><table><thead><tr><th align="left">视频大小</th><th>视频帧数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">720 * 1280 * 3, jpg; 123 video sub-sequences</td><td>110,000 + frames, 10/30 FPS</td><td>UAV (DJIS1000)、UAV simulator</td><td>2017</td><td>King Abdullah University of Science and Technology</td></tr></tbody></table><ul><li>源地址：<a href="https://cemse.kaust.edu.sa/ivul/uav123">https://cemse.kaust.edu.sa/ivul/uav123</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/91853">https://aistudio.baidu.com/aistudio/datasetdetail/91853</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>文章介绍</title>
      <link href="/2018/09/07/shi-li/"/>
      <url>/2018/09/07/shi-li/</url>
      
        <content type="html"><![CDATA[<p>主题使用指南：<a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md">https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md</a></p>]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Typora </tag>
            
            <tag> Markdown </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
