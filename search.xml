<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2022/04/21/untitled/"/>
      <url>/2022/04/21/untitled/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2022/04/15/zai-yin-he-qi-lin-gao-ji-fu-wu-qi-cao-zuo-xi-tong-v10-shang-an-zhuang-docker/"/>
      <url>/2022/04/15/zai-yin-he-qi-lin-gao-ji-fu-wu-qi-cao-zuo-xi-tong-v10-shang-an-zhuang-docker/</url>
      
        <content type="html"><![CDATA[<h1 id="在银河麒麟高级服务器操作系统V10上安装docker"><a href="#在银河麒麟高级服务器操作系统V10上安装docker" class="headerlink" title="在银河麒麟高级服务器操作系统V10上安装docker"></a>在银河麒麟高级服务器操作系统V10上安装docker</h1><p><a href="https://www.zhihu.com/people/neverland-60">整点bug</a></p><blockquote><p>银河麒麟高级服务器操作系统 V10 是针对企业级关键业务，适应虚拟化、 云计算、大数据、工业互联网时代对主机系统可靠性、安全性、性能、扩展性和 实时性的需求，依据 CMMI 5 级标准研制的提供内生安全、云原生支持、国产 平台深入优化、高性能、易管理的新一代自主服务器操作系统；同源支持飞腾、 龙芯、申威、兆芯、海光、鲲鹏等自主平台；可支撑构建大型数据中心服务器高 可用集群、负载均衡集群、分布式集群文件系统、虚拟化应用和容器云平台等， 可部署在物理服务器和虚拟化环境、私有云、公有云和混合云环境；应用于政府、 国防、金融、教育、财税、公安、审计、交通、医疗、制造等领域。</p></blockquote><p>公司有个项目需要将系统部署在 <strong>kylinos</strong>上，刚开始还有点头疼，害怕各种程序无法安装和使用，等安装好服务器进行使用的时候发现这不就是基于centos的嘛，虽然基于哪个版本不知道，但是可以测试的，于是我一顿操作，最后发现它是基于Centos8的，系统内核版本是 4.19，问题不大，既然是基于Centos8的，那Centos8上能跑的程序，在这肯定也能跑，然后我就开始了愉快（痛苦）的安装docker之旅了。</p><h3 id="配置阿里云Centos8镜像源"><a href="#配置阿里云Centos8镜像源" class="headerlink" title="配置阿里云Centos8镜像源"></a><strong>配置阿里云Centos8镜像源</strong></h3><p>之所以要配置 Centos8 的镜像源是因为在安装docker的时候需要额外的一些依赖，而这些依赖在麒麟官方的源里面是没有的。</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-8.repo<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="配置阿里云-docker-镜像源"><a href="#配置阿里云-docker-镜像源" class="headerlink" title="配置阿里云 docker 镜像源"></a><strong>配置阿里云 docker 镜像源</strong></h3><pre class="line-numbers language-text" data-language="text"><code class="language-text">yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.reposed -i 's+download.docker.com+mirrors.aliyun.com/docker-ce+' /etc/yum.repos.d/docker-ce.repo<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="定义-yum-变量-amp-修改-repo"><a href="#定义-yum-变量-amp-修改-repo" class="headerlink" title="定义 yum 变量&amp;修改 repo"></a><strong>定义 yum 变量&amp;修改 repo</strong></h3><p>修改 centos 和 docker <code>repo</code>文件中的 <code>$releasever</code> 为 <code>centos_version</code> ，原因是在麒麟服务器操作系统V10中 <code>$releasever</code>被修改为了 10，而我们需要使用 centos 8的镜像源，如果你不替换，基本上仓库的每一个地址都是404。</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">echo "8" &gt; /etc/yum/vars/centos_versionsed -i 's/$releasever/$centos_version/g' /etc/yum.repos.d/docker-ce.reposed -i 's/$releasever/$centos_version/g' /etc/yum.repos.d/CentOS-Base.repo<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="建立yum缓存"><a href="#建立yum缓存" class="headerlink" title="建立yum缓存"></a><strong>建立yum缓存</strong></h3><p>没啥可说的</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">yum makecache<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="查看docker-ce-版本"><a href="#查看docker-ce-版本" class="headerlink" title="查看docker-ce 版本"></a><strong>查看docker-ce 版本</strong></h3><pre class="line-numbers language-text" data-language="text"><code class="language-text">yum list docker-ce --showduplicates | sort -rdocker-ce.x86_64               3:20.10.9-3.el8                 docker-ce-stabledocker-ce.x86_64               3:20.10.8-3.el8                 docker-ce-stabledocker-ce.x86_64               3:20.10.7-3.el8                 docker-ce-stabledocker-ce.x86_64               3:20.10.6-3.el8                 docker-ce-stabledocker-ce.x86_64               3:20.10.5-3.el8                 docker-ce-stabledocker-ce.x86_64               3:20.10.4-3.el8                 docker-ce-stabledocker-ce.x86_64               3:20.10.3-3.el8                 docker-ce-stabledocker-ce.x86_64               3:20.10.2-3.el8                 docker-ce-stabledocker-ce.x86_64               3:20.10.1-3.el8                 docker-ce-stabledocker-ce.x86_64               3:20.10.12-3.el8                docker-ce-stabledocker-ce.x86_64               3:20.10.11-3.el8                docker-ce-stabledocker-ce.x86_64               3:20.10.10-3.el8                docker-ce-stabledocker-ce.x86_64               3:20.10.0-3.el8                 docker-ce-stabledocker-ce.x86_64               3:19.03.15-3.el8                docker-ce-stabledocker-ce.x86_64               3:19.03.15-3.el8                @docker-ce-stabledocker-ce.x86_64               3:19.03.14-3.el8                docker-ce-stabledocker-ce.x86_64               3:19.03.13-3.el8                docker-ce-stable<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a><strong>安装docker</strong></h3><p>这里要安装 docker-ce 19.03 版本，因为我在使用最新版 20.10 启动容器时出现了未知的权限问题，而麒麟服务器操作系统资料相对较少，我未能找到相应的解决方案，只好退而求其次，换到上一个稳定版本。</p><p>20.10 版本错误信息如下：</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">docker: Error response from daemon: OCI runtime create failed: container_linux.go:318: starting container process caused "permission denied": unknown.ERRO[0000] error waiting for container: context canceled<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>还是安装 19.03 版本吧。</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">yum install docker-ce-19.03.15 docker-ce-cli-19.03.15 containerd.io -y<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="启动docker"><a href="#启动docker" class="headerlink" title="启动docker"></a><strong>启动docker</strong></h3><pre class="line-numbers language-text" data-language="text"><code class="language-text">systemctl start dockersystemctl enable docker<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="启动-hello-world-进行测试"><a href="#启动-hello-world-进行测试" class="headerlink" title="启动 hello-world 进行测试"></a><strong>启动 hello-world 进行测试</strong></h3><pre class="line-numbers language-text" data-language="text"><code class="language-text">root@localhost ~]# docker run hello-worldUnable to find image 'hello-world:latest' locallylatest: Pulling from library/hello-world2db29710123e: Pull completeDigest: sha256:2498fce14358aa50ead0cc6c19990fc6ff866ce72aeb5546e1d59caac3d0d60fStatus: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.    (amd64) 3. The Docker daemon created a new container from that image which runs the    executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it    to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>完美使用 -:)</p><p>发布于 2021-12-22 00:22</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>轻松掌握 MMDetection 中常用算法(二)：Faster R-CNN|Mask R-CNN</title>
      <link href="/2022/04/12/mmdetection-xue-xi-bi-ji-4-gou-jian-faster-r-cnn-ji-pei-zhi/"/>
      <url>/2022/04/12/mmdetection-xue-xi-bi-ji-4-gou-jian-faster-r-cnn-ji-pei-zhi/</url>
      
        <content type="html"><![CDATA[<h1 id="轻松掌握-MMDetection-中常用算法-二-：Faster-R-CNN-Mask-R-CNN"><a href="#轻松掌握-MMDetection-中常用算法-二-：Faster-R-CNN-Mask-R-CNN" class="headerlink" title="轻松掌握 MMDetection 中常用算法(二)：Faster R-CNN|Mask R-CNN"></a>轻松掌握 MMDetection 中常用算法(二)：Faster R-CNN|Mask R-CNN</h1><h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0 前言"></a>0 前言</h2><p>在<a href="https://zhuanlan.zhihu.com/p/346198300">轻松掌握 MMDetection 中常用算法(一)：RetinaNet 及配置详解</a>一文中，对经典 one-stage 目标检测算法 RetinaNet 以及相关配置参数进行了详细说明，本文解读经典 two-stage 算法 Faster R-CNN 以及改进版 Mask R-CNN。需要特别注意的是：如果涉及到和 RetinaNet 相同的配置，本文不再进一步描述，读者请查看 RetinaNet 一文解读。</p><p>项目地址：</p><p><a href="https://link.zhihu.com/?target=https://github.com/open-mmlab/mmdetection">https://github.com/open-mmlab/mmdetectiongithub.com/open-mmlab/mmdetection</a></p><p>欢迎 star</p><h2 id="1-Faster-R-CNN-和-Mask-R-CNN-简介"><a href="#1-Faster-R-CNN-和-Mask-R-CNN-简介" class="headerlink" title="1 Faster R-CNN 和 Mask R-CNN 简介"></a>1 Faster R-CNN 和 Mask R-CNN 简介</h2><p><strong>Faster R-CNN</strong> （Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks） 是目标检测领域最为经典的方法之一，<strong>通过 RPN(Region Proposal Networks) 区域提取网络</strong>和 <strong>R-CNN 网络联合训练</strong>实现高效目标检测。其简要发展历程为：</p><ol><li><strong>R-CNN。</strong>首先通过<strong>传统的 selective search</strong> 算法在图片上<strong>预取 2000 个左右 Region Proposal</strong>；接着将这些 Region Proposal 通过前处理<strong>统一尺寸输入到 CNN</strong> 中进行特征提取；然后把所提取的<strong>特征输入到 SVM</strong> 支持向量机中进行分类；最后对<strong>分类后的 Region Proposal 进行 bbox 回归</strong>。此时算法的整个过程较为繁琐，速度也较慢。</li><li><strong>Fast R-CNN。</strong>首先通过<strong>传统的 selective search</strong> 算法在图片上<strong>预取 2000 个左右 Region Proposal</strong>；接着对<strong>整张图片进行特征提取</strong>；然后利用 Region Proposal 坐标<strong>在 CNN 的最后一个特征图上进去 RoI 特征图提取</strong>；最后<strong>将所有 RoI 特征输入到分类和回归模块</strong>中。此时算法的整个过程相比 R-CNN 得到极大的简化，但依然无法联合训练。</li><li><strong>Faster R-CNN。</strong>首先通过可学习的 <strong>RPN 网络进行 Region Proposal 的预取</strong>；接着利用 Region Proposal 坐标在 <strong>CNN 的特征图上进行 RoI 特征图提取</strong>；然后<strong>利用 RoI Pooling 层进行空间池化</strong>使其所有特征图输出尺寸相同；最后将所有特征图输入到后续的 <strong>FC 层进行分类和回归</strong>。此时算法的整个过程一气呵成，实现了端到端训练。</li></ol><p>Faster R-CNN 的出现改变了整个目标检测算法的发展历程。之所以叫做 two-stage 检测器，原因是其<strong>包括一个区域提取网络 RPN 和 RoI Refine 网络 R-CNN，同时为了将 RPN 提取的不同大小的 RoI 特征图组成 batch 输入到后面的 R-CNN 中，在两者中间还插入了一个 RoI Pooling 层，可以保证任意大小特征图输入都可以变成指定大小输出</strong>。简要结构图如下所示：</p><p><img src="https://pic3.zhimg.com/80/v2-fc4a5e8c33187dbdcde632d1c7746792_720w.jpg" alt="img"></p><p>如果和 RetinaNet 进行类比的话，其过程相当于 <strong>二分类的 RetinaNet + RoI Pooling + 简单 FC 层多分类和回归网络</strong>，Faster R-CNN 也属于 Anchor-based 类算法。</p><p>Faster R-CNN 之后，考虑到多尺度预测问题，后续又提出了改进版本<strong>特征金字塔 FPN(Feature Pyramid Networks for Object Detection)<strong>。 通过分析目前目标检测中存在的图像金字塔、单层预测和多层预测问题，提出了一个简单的，通过从上到下路径和横向连接，结合高分辨率、弱语义信息的特征层和低分辨率、强语义信息的特征融合，实现类似图像金字塔效果，</strong>顶层特征通过上采样和低层特征做融合，而且每层都是独立预测的</strong>，效果显著，如下图所示：</p><p><img src="https://pic4.zhimg.com/80/v2-5a78ef8716761b468a1ae5f4d9810d13_720w.jpg" alt="img"></p><p><strong>由于其强大的性能，更加模块化现代化的设计，现在提到 Faster R-CNN, 一般默认是指的 FPN 网络。本文解读的 Faster R-CNN 网络实际上也是指的 FPN。</strong></p><p>在 FPN 提出后，Kaiming He 等进一步对其进行任务扩展，<strong>提出了 Mask R-CNN</strong>，通过<strong>新增 mask 掩码分支实现实例分割任务</strong>，其最大特点是<strong>任务扩展性强</strong>，通过新增不同分支就可以实现不同的扩展任务。<em>例如可以将 mask 分支替换为关键点分支即可实现多人姿态估计。</em>除此之外，为解决特征图与原始图像上的 RoI 不对准的问题，<strong>提出了 ROIAlign 模块</strong>。其简要示意图如下：</p><p><img src="https://pic3.zhimg.com/80/v2-1a0e112124c91b3c6c6df64359613b76_720w.jpg" alt="img"></p><p>带有 FPN 的 Faster R-CNN 和 Mask R-CNN 算法是目前的主流算法，应用非常广泛。并且由于 Faster R-CNN 与 Mask R-CNN 属于同一系列，因此本文将这两个核心算法同时解读。</p><h2 id="2-Faster-R-CNN-代码详解"><a href="#2-Faster-R-CNN-代码详解" class="headerlink" title="2 Faster R-CNN 代码详解"></a>2 Faster R-CNN 代码详解</h2><p>为方便算法与代码的解读，Faster R-CNN 模型整体流程如下所示：</p><p><img src="https://pic1.zhimg.com/80/v2-2ea42c8f593cb15b486025289f35e1b8_720w.jpg" alt="img"></p><ol><li>图片输入到 ResNet 中进行<strong>特征提取</strong>，输出 4 个特征图，按照特征图从大到小排列，分别是 C2 C3 C4 C5，stride = 4,8,16,32</li><li>4 个特征图输入到 FPN 模块中进行<strong>特征融合</strong>，输出 5 个通道数相同的特征图,分别是 p2 ~ p6，stride = 4,8,16,32,64</li><li>FPN 输出的 5 个特征图，输入到同一个 RPN 或者说 5 个相同的 RPN 中，每个分支都进行<strong>前后景分类和 bbox 回归</strong>，然后就可以和 label 计算 loss</li><li>在 5 个 RPN 分支输出的基础上，采用 RPN test 模块<strong>输出指定个数的 Region Proposal</strong>，将 Region Proposal 按照重映射规则，在对应的 p2 ~ p5 特征图上进行特征提取，注意并没有使用 p6 层特征图，从而得到指定个数例如 2k 个 Region Proposal 特征图</li><li>将 2k 个不同大小的 RoI 区域<strong>特征图</strong>输入到 RoIAlign 或者 RoIPool 层中进行<strong>统一采样</strong>，得到指定输出 shape 的 2k 个特征图</li><li>组成 batch <strong>输入到两层 FC 中进行多类别的分类和回归，</strong>其 loss 和 RPN 层 loss 相加进行联合训练</li></ol><p>下面将会对每个模块进行详细的分析。值得注意的是，除了 RoI Head 模块外，其他模块在前一篇 RetinaNet 都有介绍，大家也可以作为参考，方便辅助理解。</p><h3 id="2-1-Backbone"><a href="#2-1-Backbone" class="headerlink" title="2.1 Backbone"></a>2.1 Backbone</h3><p><img src="https://pic1.zhimg.com/80/v2-34e9618f1a3e91e31e70f2248255b508_720w.jpg" alt="img"></p><p>由于 Faster R-CNN 是后续各个算法的 baseline 且用途非常广泛，OpenMMLab 提供了非常多的模型配置供研究或者不同任务 fintune 用，几乎覆盖了所有常用配置，如下所示：</p><ul><li>1x、2x 和 3x 的模型配置和权重</li><li>多尺度训练配置和权重</li><li>不同骨架的配置和权重</li><li>PyTorch 和 Caffe style 的配置和权重</li><li>各种 loss 对比配置和权重</li><li>不包含 FPN 的 Faster R-CNN 配置和权重</li><li>常用类别例如 person 的配置和权重，可作为<strong>下游任务例如行人检测的预训练权重，性能极佳</strong></li></ul><p>以 ResNet50 为例，其配置和 RetinaNet 完全相同，此处不再描述。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 使用 pytorch 提供的在 imagenet 上面训练过的权重作为预训练权重</span>pretrained<span class="token operator">=</span><span class="token string">'torchvision://resnet50'</span><span class="token punctuation">,</span>backbone<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># 骨架网络类名</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'ResNet'</span><span class="token punctuation">,</span>    <span class="token comment"># 表示使用 ResNet50</span>    depth<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span>    <span class="token comment"># ResNet 系列包括 stem+ 4个 stage 输出</span>    num_stages<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    <span class="token comment"># 表示本模块输出的特征图索引，(0, 1, 2, 3),表示4个 stage 输出都需要，</span>    <span class="token comment"># 其 stride 为 (4,8,16,32)，channel 为 (256, 512, 1024, 2048)</span>    <span class="token comment"># stride表示模型的下采样率</span>    out_indices<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 表示固定 stem 加上第一个 stage 的权重，不进行训练</span>    frozen_stages<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>    <span class="token comment"># 所有的 BN 层的可学习参数都不需要梯度，也就不会进行参数更新</span>    norm_cfg<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'BN'</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># backbone 所有的 BN 层的均值和方差都直接采用全局预训练值，不进行更新</span>    norm_eval<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token comment"># 默认采用 pytorch 模式</span>    style<span class="token operator">=</span><span class="token string">'pytorch'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-2-Neck"><a href="#2-2-Neck" class="headerlink" title="2.2 Neck"></a>2.2 Neck</h3><p>虽然都采用了 FPN，但是 Faster R-CNN 的配置和 RetinaNet 不同，</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">neck<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'FPN'</span><span class="token punctuation">,</span>    <span class="token comment"># ResNet 模块输出的4个尺度特征图通道数</span>    in_channels<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">2048</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token comment"># FPN 输出的每个尺度输出特征图通道</span>    out_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>    <span class="token comment"># FPN 输出特征图个数</span>    num_outs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>和 RetinaNet 的区别是 ResNet 输出的 4 个特征图都会被利用。其详细流程是：</p><ul><li>将c2 c3 c4 c5 4 个特征图全部经过各自 1x1 卷积进行通道变换变成 m2~m5，输出通道统一为 256</li><li>从 m5 开始，先进行 2 倍最近邻上采样，然后和 m4 进行 add 操作，得到新的 m4</li><li>将新 m4 进行 2 倍最近邻上采样，然后和 m3 进行 add 操作，得到新的 m3</li><li>将新 m3 进行 2 倍最近邻上采样，然后和 m2 进行 add 操作，得到新的 m2</li><li>对 m5 和新的融合后的 m4 ~ m2，都进行各自的 3x3 卷积，得到 4 个尺度的最终输出 p5 ~ p2</li><li>将 c5 进行 3x3 且 stride=2 的卷积操作，得到 p6，目的是提供一个感受野非常大的特征图，有利于检测超大物体</li></ul><p>故 FPN 模块实现了c2 ~ c5 4 个特征图输入，p2 ~ p6 5个特征图输出，其 strides = (4,8,16,32,64)。</p><h3 id="2-3-RPN-Head"><a href="#2-3-RPN-Head" class="headerlink" title="2.3 RPN Head"></a>2.3 RPN Head</h3><p>其完整配置如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">rpn_head<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'RPNHead'</span><span class="token punctuation">,</span>    <span class="token comment"># FPN 层输出特征图通道数</span>    in_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>    <span class="token comment"># 中间特征图通道数</span>    feat_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>    <span class="token comment"># 后面分析</span>    anchor_generator<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'AnchorGenerator'</span><span class="token punctuation">,</span>        scales<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        ratios<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 后面分析</span>    bbox_coder<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'DeltaXYWHBBoxCoder'</span><span class="token punctuation">,</span>        target_means<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        target_stds<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 后面分析</span>    loss_cls<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'CrossEntropyLoss'</span><span class="token punctuation">,</span> use_sigmoid<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    loss_bbox<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'L1Loss'</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>相比 RetinaNet，RPN Head 网络比较简单，就一个卷积进行特征通道变换，加上两个输出分支即可，如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_init_layers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Initialize layers of the head."""</span>    <span class="token comment"># 特征通道变换</span>    self<span class="token punctuation">.</span>rpn_conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>        self<span class="token punctuation">.</span>in_channels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>feat_channels<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment"># 分类分支，类别固定是2，表示前后景分类</span>    <span class="token comment"># 并且由于 cls loss 是 bce，故实际上 self.cls_out_channels=1</span>    self<span class="token punctuation">.</span>rpn_cls <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>self<span class="token punctuation">.</span>feat_channels<span class="token punctuation">,</span>                             self<span class="token punctuation">.</span>num_anchors <span class="token operator">*</span> self<span class="token punctuation">.</span>cls_out_channels<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment"># 回归分支，固定输出4个数值，表示基于 anchor 的变换值</span>    self<span class="token punctuation">.</span>rpn_reg <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>self<span class="token punctuation">.</span>feat_channels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_anchors <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>5 个 RPN Head 共享所有分类或者回归分支的卷积权重，经过 Head 模块的前向流程输出一共是 5*2 个特征图。</p><h3 id="2-4-BBox-Assigner"><a href="#2-4-BBox-Assigner" class="headerlink" title="2.4 BBox Assigner"></a>2.4 BBox Assigner</h3><p>RPN 这部分设计和 RetinaNet 原理完全相同，差别只在参数而已。</p><p><strong>(1) AnchorGenerator</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">anchor_generator<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'AnchorGenerator'</span><span class="token punctuation">,</span>    <span class="token comment"># 相当于 octave_base_scale，表示每个特征图的 base scales</span>    scales<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token comment"># 每个特征图有 3 个高宽比例</span>    ratios<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token comment"># 特征图对应的 stride，必须和特征图 stride 一致，不可以随意更改</span>    strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>相比不包括 FPN 的 Faster R-CNN 算法，由于其 RPN Head 是多尺度特征图，为了适应这种变化，anchor 设置进行了适当修改，FPN 输出的多尺度信息可以帮助区分不同大小物体识别问题，每一层就不再需要不包括 FPN 的 Faster R-CNN 算法那么多 anchor 了。</p><p>可以看出一共 5 个输出层，每个输出层包括 3 个高宽比例和 1 种尺度，也就是说每一层的每个特征图坐标处都包括 3 个 anchor，一共 15 个 anchor，相比 RetinaNet 少了很多，其具体实现看 RetinaNet 算法解读文章。</p><p><strong>(2) BBox Assigner</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">assigner<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># 最大 IoU 原则分配器</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'MaxIoUAssigner'</span><span class="token punctuation">,</span>    <span class="token comment"># 正样本阈值</span>    pos_iou_thr<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">,</span>    <span class="token comment"># 负样本阈值</span>    neg_iou_thr<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span>    <span class="token comment"># 正样本阈值下限</span>    min_pos_iou<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span>    <span class="token comment"># 忽略 bboxes 的阈值，-1 表示不忽略</span>    ignore_iof_thr<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>分配准则和 RetinaNet 完全相同，除了参数不同以外。简要总结为：</p><ul><li>如果 anchor 和所有 gt bbox 的最大 iou 值小于 0.3，那么该 anchor 就是背景样本</li><li>如果 anchor 和所有 gt bbox 的最大 iou 值大于等于 0.7，那么该 anchor 就是高质量正样本，该阈值比较高，这个阈值设置需要和后续的 R-CNN 模块匹配</li><li>如果 gt bbox 和所有 anchor 的最大 iou 值大于等于 0.3(可以看出可能有某些 gt bbox 没有和任意 anchor 匹配)，那么该 gt bbox 所对应的 anchor 也是正样本</li><li>其余样本全部为忽略样本，但是由于 <code>neg_iou_thr</code> 和 <code>min_pos_iou</code> 相等，故不存在忽略样本</li></ul><h3 id="2-5-BBox-Sampler"><a href="#2-5-BBox-Sampler" class="headerlink" title="2.5 BBox Sampler"></a>2.5 BBox Sampler</h3><p>和 RetinaNet 采用 Focal Loss 处理正负样本不平衡不同，Faster R-CNN 是通过正负样本采样模块来克服。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">sampler<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># 随机采样</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'RandomSampler'</span><span class="token punctuation">,</span>    <span class="token comment"># 采样后每张图片的训练样本总数，不包括忽略样本</span>    num<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>    <span class="token comment"># 正样本比例</span>    pos_fraction<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>    <span class="token comment"># 正负样本比例，用于确定负样本采样个数上界</span>    neg_pos_ub<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>    <span class="token comment"># 是否加入 gt 作为 proposals 以增加高质量正样本数</span>    add_gt_as_proposals<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>核心参数的具体含义是：</p><ul><li>num = 256 表示采样后每张图片的样本总数，<code>pos_fraction</code> 表示其中的正样本比例，具体是正样本采样 128 个，那么理论上负样本采样也是 128 个</li><li><code>neg_pos_ub</code> 表示负和正样本比例上限，用于确定负样本采样个数上界，例如打算采样 1000 个样本，正样本打算采样 500 个，但是可能正样本才 200 个，那么正样本实际上只能采样 200 个，如果设置 <code>neg_pos_ub=-1</code> 那么就会对负样本采样 800 个，用于凑足 1000 个，但是如果设置了 <code>neg_pos_ub</code> 比例，例如 1.5，那么负样本最多采样 200x1.5=300 个，最终返回的样本实际上不够 1000 个，默认情况 <code>neg_pos_ub=-1</code></li><li><code>add_gt_as_proposals=True</code> 是防止高质量正样本太少而加入的，可以保证前期收敛更快、更稳定，属于训练技巧，在 RPN 模块设置为 False，主要用于 R-CNN，因为前期 RPN 提供的正样本不够，可能会导致训练不稳定或者前期收敛慢的问题。</li></ul><p>其实现过程比较简单，如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> self<span class="token punctuation">.</span>add_gt_as_proposals <span class="token keyword">and</span> <span class="token builtin">len</span><span class="token punctuation">(</span>gt_bboxes<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>    <span class="token comment"># 增加 gt 作为 proposals</span>    bboxes <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>gt_bboxes<span class="token punctuation">,</span> bboxes<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>    assign_result<span class="token punctuation">.</span>add_gt_<span class="token punctuation">(</span>gt_labels<span class="token punctuation">)</span><span class="token comment"># 计算正样本个数</span>num_expected_pos <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num <span class="token operator">*</span> self<span class="token punctuation">.</span>pos_fraction<span class="token punctuation">)</span><span class="token comment"># 正样本随机采样</span>pos_inds <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_sampler<span class="token punctuation">.</span>_sample_pos<span class="token punctuation">(</span>    assign_result<span class="token punctuation">,</span> num_expected_pos<span class="token punctuation">,</span> bboxes<span class="token operator">=</span>bboxes<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token comment"># 去重</span>pos_inds <span class="token operator">=</span> pos_inds<span class="token punctuation">.</span>unique<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 计算负样本数</span>num_sampled_pos <span class="token operator">=</span> pos_inds<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span>num_expected_neg <span class="token operator">=</span> self<span class="token punctuation">.</span>num <span class="token operator">-</span> num_sampled_pos<span class="token keyword">if</span> self<span class="token punctuation">.</span>neg_pos_ub <span class="token operator">&gt;=</span> <span class="token number">0</span><span class="token punctuation">:</span>   <span class="token comment"># 计算负样本个数上限</span>    _pos <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_sampled_pos<span class="token punctuation">)</span>    neg_upper_bound <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>neg_pos_ub <span class="token operator">*</span> _pos<span class="token punctuation">)</span>    <span class="token keyword">if</span> num_expected_neg <span class="token operator">&gt;</span> neg_upper_bound<span class="token punctuation">:</span>        num_expected_neg <span class="token operator">=</span> neg_upper_bound<span class="token comment"># 负样本随机采样</span>neg_inds <span class="token operator">=</span> self<span class="token punctuation">.</span>neg_sampler<span class="token punctuation">.</span>_sample_neg<span class="token punctuation">(</span>    assign_result<span class="token punctuation">,</span> num_expected_neg<span class="token punctuation">,</span> bboxes<span class="token operator">=</span>bboxes<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token comment"># 去重  </span>neg_inds <span class="token operator">=</span> neg_inds<span class="token punctuation">.</span>unique<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>而具体的随机采样函数如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 随机采样正样本</span><span class="token keyword">def</span> <span class="token function">_sample_pos</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> assign_result<span class="token punctuation">,</span> num_expected<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Randomly sample some positive samples."""</span>    pos_inds <span class="token operator">=</span> torch<span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span>assign_result<span class="token punctuation">.</span>gt_inds <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">,</span> as_tuple<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> pos_inds<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>        pos_inds <span class="token operator">=</span> pos_inds<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> pos_inds<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;=</span> num_expected<span class="token punctuation">:</span>        <span class="token keyword">return</span> pos_inds    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>random_choice<span class="token punctuation">(</span>pos_inds<span class="token punctuation">,</span> num_expected<span class="token punctuation">)</span><span class="token comment"># 随机采样负样本</span><span class="token keyword">def</span> <span class="token function">_sample_neg</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> assign_result<span class="token punctuation">,</span> num_expected<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Randomly sample some negative samples."""</span>    neg_inds <span class="token operator">=</span> torch<span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span>assign_result<span class="token punctuation">.</span>gt_inds <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> as_tuple<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> neg_inds<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>        neg_inds <span class="token operator">=</span> neg_inds<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>neg_inds<span class="token punctuation">)</span> <span class="token operator">&lt;=</span> num_expected<span class="token punctuation">:</span>        <span class="token keyword">return</span> neg_inds    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>random_choice<span class="token punctuation">(</span>neg_inds<span class="token punctuation">,</span> num_expected<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>经过随机采样函数后，可以有效控制 RPN 网络计算 loss 时正负样本平衡问题。</p><h3 id="2-6-BBox-Encoder-Decoder"><a href="#2-6-BBox-Encoder-Decoder" class="headerlink" title="2.6 BBox Encoder Decoder"></a>2.6 BBox Encoder Decoder</h3><p>本部分参数和实现方式和 RetinaNet 中介绍的完全相同，请参照相关解读。</p><h3 id="2-7-Loss"><a href="#2-7-Loss" class="headerlink" title="2.7 Loss"></a>2.7 Loss</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">loss_cls<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'CrossEntropyLoss'</span><span class="token punctuation">,</span> use_sigmoid<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>loss_bbox<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'L1Loss'</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>RPN 采用的 loss 是常用的 bce loss 和 l1 loss，不需要详细描述。</p><h3 id="2-8-RPN-Test"><a href="#2-8-RPN-Test" class="headerlink" title="2.8 RPN Test"></a>2.8 RPN Test</h3><p>到目前为止， RPN 的整个训练流程就分析完后，但是实际上 RPN 是作为一个 RoI 提取模块，真正核心的是 R-CNN 部分，为了实现联合训练，RPN 不仅仅要自己进行训练，还要同时输出 RoI，然后利用这些 RoI 去 FPN 输出的特征图上进行截取，最后输入给 R-CNN 进行分类和回归。一个核心的问题是如何得到这些 RoI，实际上是调用了 RPN 的 test 过程。由于 RPN 也是和 RetinaNet 一样的 one-stage 算法，其大概过程和 RetinaNet 基本一致：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">rpn<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># 是否跨层进行 NMS 操作</span>    nms_across_levels<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>    <span class="token comment"># nms 前每个输出层最多保留 1000 个预测框</span>    nms_pre<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span>    <span class="token comment"># nms 后每张图片最多保留 1000 个预测框</span>    nms_post<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span>    <span class="token comment"># 每张图片最终输出检测结果最多保留 1000 个，RPN 层没有使用这个参数</span>    max_num<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span>    <span class="token comment"># nms 阈值</span>    nms_thr<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">,</span>    <span class="token comment"># 过滤掉的最小 bbox 尺寸</span>    min_bbox_size<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>1.对 batch 输入图片经过 Backbone+FPN+RPN Head 后输出 5 个特征图，每个图包括两个分支 <code>rpn_cls_score</code>，<code>rpn_bbox_pred</code>，首先遍历每张图，然后遍历每张图片中的每个输出层进行后续处理</p><p>2.对每层的分类 <code>rpn_cls_score</code> 进行 sigmoid 操作得到概率值</p><p>3.按照分类预测分值排序，保留前 <code>nms_pre</code> 个预测结果</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> cfg<span class="token punctuation">.</span>nms_pre <span class="token operator">&gt;</span> <span class="token number">0</span> <span class="token keyword">and</span> scores<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">&gt;</span> cfg<span class="token punctuation">.</span>nms_pre<span class="token punctuation">:</span>    <span class="token comment"># sort is faster than topk</span>    <span class="token comment"># _, topk_inds = scores.topk(cfg.nms_pre)</span>    ranked_scores<span class="token punctuation">,</span> rank_inds <span class="token operator">=</span> scores<span class="token punctuation">.</span>sort<span class="token punctuation">(</span>descending<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    topk_inds <span class="token operator">=</span> rank_inds<span class="token punctuation">[</span><span class="token punctuation">:</span>cfg<span class="token punctuation">.</span>nms_pre<span class="token punctuation">]</span>    scores <span class="token operator">=</span> ranked_scores<span class="token punctuation">[</span><span class="token punctuation">:</span>cfg<span class="token punctuation">.</span>nms_pre<span class="token punctuation">]</span>    rpn_bbox_pred <span class="token operator">=</span> rpn_bbox_pred<span class="token punctuation">[</span>topk_inds<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>    anchors <span class="token operator">=</span> anchors<span class="token punctuation">[</span>topk_inds<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>4.对每张图片的 5 个输出层都运行 2 ~ 3 步骤，将预测结果全部收集，然后进行解码</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>mlvl_scores<span class="token punctuation">)</span>anchors <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>mlvl_valid_anchors<span class="token punctuation">)</span>rpn_bbox_pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>mlvl_bbox_preds<span class="token punctuation">)</span>proposals <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_coder<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>    anchors<span class="token punctuation">,</span> rpn_bbox_pred<span class="token punctuation">,</span> max_shape<span class="token operator">=</span>img_shape<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>5.进行统一的 NMS 操作，每张图片最终保留 <code>cfg.nms_post</code> 个预测框</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">nms_cfg <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'nms'</span><span class="token punctuation">,</span> iou_threshold<span class="token operator">=</span>cfg<span class="token punctuation">.</span>nms_thr<span class="token punctuation">)</span>dets<span class="token punctuation">,</span> keep <span class="token operator">=</span> batched_nms<span class="token punctuation">(</span>proposals<span class="token punctuation">,</span> scores<span class="token punctuation">,</span> ids<span class="token punctuation">,</span> nms_cfg<span class="token punctuation">)</span><span class="token keyword">return</span> dets<span class="token punctuation">[</span><span class="token punctuation">:</span>cfg<span class="token punctuation">.</span>nms_post<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>经过 RPN test 计算后每张图片可以提供最多 <code>nms_post</code> 个候选框，一般该值为 2000。</p><h3 id="2-9-RoI-Head"><a href="#2-9-RoI-Head" class="headerlink" title="2.9 RoI Head"></a>2.9 RoI Head</h3><p><strong>R-CNN 模块接收 RPN 输出的每张图片共 <code>nms_post</code> 个候选框，然后对这些候选框进一步 refine，输出包括区分具体类别和 bbox 回归</strong>。该模块网络构建方面虽然简单，但是也包括了 RPN 中涉及到的组件，例如 BBox Assigner、BBox Sampler、BBox Encoder Decoder、Loss 等等，除此之外，还包括一个额外的 RPN 到 R-CNN 数据转换模块：RoIAlign 或者 RoIPool, 下面详细描述。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">roi_head<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># 一次 refine head，另外对应的是级联结构</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'StandardRoIHead'</span><span class="token punctuation">,</span>    bbox_roi_extractor<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'SingleRoIExtractor'</span><span class="token punctuation">,</span>        roi_layer<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'RoIAlign'</span><span class="token punctuation">,</span> output_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> sampling_ratio<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        out_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>        featmap_strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    bbox_head<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token comment"># 2 个共享 FC 模块</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'Shared2FCBBoxHead'</span><span class="token punctuation">,</span>        <span class="token comment"># 输入通道数，相等于 FPN 输出通道</span>        in_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>        <span class="token comment"># 中间 FC 层节点个数</span>        fc_out_channels<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span>        <span class="token comment"># RoIAlign 或 RoIPool 输出的特征图大小</span>        roi_feat_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span>        <span class="token comment"># 类别个数</span>        num_classes<span class="token operator">=</span><span class="token number">80</span><span class="token punctuation">,</span>        <span class="token comment"># bbox 编解码策略，除了参数外和 RPN 相同，</span>        bbox_coder<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>            <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'DeltaXYWHBBoxCoder'</span><span class="token punctuation">,</span>            target_means<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            target_stds<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token comment"># 影响 bbox 分支的通道数，True 表示 4 通道输出，False 表示 4×num_classes 通道输出</span>        reg_class_agnostic<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>        <span class="token comment"># CE Loss</span>        loss_cls<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>            <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'CrossEntropyLoss'</span><span class="token punctuation">,</span> use_sigmoid<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token comment"># L1 Loss</span>        loss_bbox<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'L1Loss'</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>从配置可以看出，和 RPN 相比，除了额外的 SingleRoIExtractor 外，基本都是相同的。其训练和测试流程简要概况如下：</p><p><strong>(1) 公共部分</strong></p><ol><li>RPN 层输出每张图片最多 <code>nms_post</code> 个候选框，故 R-CNN 输入 shape 为 <code>(batch, nms_post, 4)</code>，4 表示 RoI 坐标</li><li>利用 RoI 重映射规则，将 <code>nms_post</code> 个候选框映射到 FPN 输出的不同特征图上，提取对应的特征图，然后利用插值思想将其变成指定的固定大小输出，输出 shape 为 <code>(batch, nms_post, 256, roi_feat_size, roi_feat_size)</code>，其中 256 是 FPN 层输出特征图通道大小，<code>roi_feat_size</code> 一般取 7。上述步骤即为 RoIAlign 或者 RoIPool 计算过程</li><li>将 <code>(batch, nms_post, 256, roi_feat_size, roi_feat_size)</code> 数据拉伸为 <code>(batch*nms_post, 256*roi_feat_size*roi_feat_size)</code>，转化为 FC 可以支持的格式, 然后应用两次共享卷积，输出 shape 为 <code>(batch*nms_post, 1024)</code></li><li>将 <code>(batch*nms_post, 1024)</code> 分成分类和回归分支，分类分支输出 <code>(batch*nms_post, num_classes+1)</code>, 回归分支输出 <code>(batch*nms_post, 4*num_class)</code></li></ol><p>第二步的映射规则是在 FPN 论文中提出。不知大家是否有疑问：假设某个 proposal 是由第 4 个 特征图层检测出来的，为啥该 proposal 不是直接去对应特征图层切割就行，还需要重新映射？原因是这些 proposal 是 RPN 测试阶段检测出来的，大部分 proposal 可能符合前面设定，但是也有很多不符合的，也就是说测试阶段上述一致性不一定满足，需要重新映射，公式如下：</p><p>$$k=\lfloor k_0 + log_2(\sqrt{wh}/224)\rfloor$$</p><p>上述公式中 k_0=4，通过公式可以算出 pk，具体是：</p><ul><li>wh&gt;=448x448，则分配给 p5</li><li>wh&lt;448x448 并且 wh&gt;=224x224，则分配给 p4</li><li>wh&lt;224x224 并且 wh&gt;=112x112，则分配给 p3</li><li>其余分配给 p2</li></ul><p>在 R-CNN 部分没有采用感受野最大的 p6 层。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">map_roi_levels</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> rois<span class="token punctuation">,</span> num_levels<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Map rois to corresponding feature levels by scales.    - scale &lt; finest_scale * 2: level 0    - finest_scale * 2 &lt;= scale &lt; finest_scale * 4: level 1    - finest_scale * 4 &lt;= scale &lt; finest_scale * 8: level 2     - scale &gt;= finest_scale * 8: level 3    """</span>    scale <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>        <span class="token punctuation">(</span>rois<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">-</span> rois<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>rois<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span> <span class="token operator">-</span> rois<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    target_lvls <span class="token operator">=</span> torch<span class="token punctuation">.</span>floor<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>log2<span class="token punctuation">(</span>scale <span class="token operator">/</span> self<span class="token punctuation">.</span>finest_scale <span class="token operator">+</span> <span class="token number">1e-6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    target_lvls <span class="token operator">=</span> target_lvls<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token builtin">min</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">max</span><span class="token operator">=</span>num_levels <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> target_lvls<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其中 <code>finest_scale=56，num_level=5</code>。</p><p>在基于候选框提取出对应的特征图后，再利用 RoIAlign 或者 RoIPool 进行统一输出大小，其计算过程在 Mask R-CNN 部分分析。</p><p>经过 RoIAlign 或者 RoIPool 后，所有候选框特征图的 shape 为 <code>(batch, nms_post, 256, roi_feat_size, roi_feat_size)</code>，将其拉伸后输入到 R-CNN 的 Head 模块中，具体来说主要是包括两层分类和回归共享全连接层 FC，最后是各自的输出头，其 forward 逻辑如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> self<span class="token punctuation">.</span>num_shared_fcs <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>    x <span class="token operator">=</span> x<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment"># 两层共享 FC</span>    <span class="token keyword">for</span> fc <span class="token keyword">in</span> self<span class="token punctuation">.</span>shared_fcs<span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>x_cls <span class="token operator">=</span> xx_reg <span class="token operator">=</span> x<span class="token comment"># 不共享的分类和回归分支输出</span>cls_score <span class="token operator">=</span> self<span class="token punctuation">.</span>fc_cls<span class="token punctuation">(</span>x_cls<span class="token punctuation">)</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>with_cls <span class="token keyword">else</span> <span class="token boolean">None</span>bbox_pred <span class="token operator">=</span> self<span class="token punctuation">.</span>fc_reg<span class="token punctuation">(</span>x_reg<span class="token punctuation">)</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>with_reg <span class="token keyword">else</span> <span class="token boolean">None</span><span class="token keyword">return</span> cls_score<span class="token punctuation">,</span> bbox_pred<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最终输出分类和回归预测结果。相比于目前主流的全卷积模型，Faster R-CNN 的 R-CNN 模块依然采用的是全连接模式。</p><p><strong>(2) 训练逻辑</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">rcnn<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    assigner<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token comment"># 和 RPN 一样，正负样本定义参数不同</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'MaxIoUAssigner'</span><span class="token punctuation">,</span>        pos_iou_thr<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>        neg_iou_thr<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>        min_pos_iou<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>        match_low_quality<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>        ignore_iof_thr<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    sampler<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token comment"># 和 RPN 一样，随机采样参数不同</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'RandomSampler'</span><span class="token punctuation">,</span>        num<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>        pos_fraction<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">,</span>        neg_pos_ub<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>        <span class="token comment"># True，RPN 中为 False</span>        add_gt_as_proposals<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>理论上，BBox Assigner 和 BBox Sampler 逻辑可以放置在 <em>(1) 公共部分</em> 后面，因为其任务是输入每张图片的 <code>nms_post</code> 个候选框以及标注的 gt bbox 信息，然后计算每个候选框样本的正负样本属性，最后再进行随机采样尽量保证样本平衡。<strong>R-CNN的候选框对应了 RPN 阶段的 anchor，只不过 RPN 中的 anchor 是预设密集的，而 R-CNN 面对的 anchor 是动态稀疏的，RPN 阶段基于 anchor 进行分类回归对应于 R-CNN 阶段基于候选框进行分类回归，思想是完全一致的，故 Faster R-CNN 类算法叫做 two-stage，因此可以简化为 one-stage + RoI 区域特征提取 + one-stage。</strong></p><p>实际上为了方便理解，BBox Assigner 和 BBox Sampler 逻辑是在 <em>(1) 公共部分</em> 的步骤 1 后运行的。需要特别注意的是配置参数和 RPN 不同：</p><ul><li><code>match_low_quality=False</code>。为了避免出现低质量匹配情况(因为 two-stage 算法性能核心在于 R-CNN，RPN 主要保证高召回率，R-CNN 保证高精度)，R-CNN 阶段禁用了允许低质量匹配设置</li><li>3 个 <code>iou_thr</code> 设置都是 0.5，不存在忽略样本，这个参数在 Cascade R-CNN 论文中有详细说明，影响较大</li><li><code>add_gt_as_proposals=True</code>。主要是克服刚开始 R-CNN 训练不稳定情况</li></ul><p>R-CNN 整体训练逻辑如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> self<span class="token punctuation">.</span>with_bbox <span class="token keyword">or</span> self<span class="token punctuation">.</span>with_mask<span class="token punctuation">:</span>    num_imgs <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>img_metas<span class="token punctuation">)</span>    sampling_results <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token comment"># 遍历每张图片，单独计算 BBox Assigner 和 BBox Sampler </span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_imgs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># proposal_list 是 RPN test 输出的候选框</span>        assign_result <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_assigner<span class="token punctuation">.</span>assign<span class="token punctuation">(</span>            proposal_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> gt_bboxes<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> gt_bboxes_ignore<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>            gt_labels<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment"># 随机采样</span>        sampling_result <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_sampler<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>            assign_result<span class="token punctuation">,</span>            proposal_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>            gt_bboxes<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>            gt_labels<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>            feats<span class="token operator">=</span><span class="token punctuation">[</span>lvl_feat<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token keyword">for</span> lvl_feat <span class="token keyword">in</span> x<span class="token punctuation">]</span><span class="token punctuation">)</span>        sampling_results<span class="token punctuation">.</span>append<span class="token punctuation">(</span>sampling_result<span class="token punctuation">)</span><span class="token comment"># 特征重映射+ RoI 区域特征提取+ 网络 forward + Loss 计算</span>losses <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># bbox head forward and loss</span><span class="token keyword">if</span> self<span class="token punctuation">.</span>with_bbox<span class="token punctuation">:</span>    bbox_results <span class="token operator">=</span> self<span class="token punctuation">.</span>_bbox_forward_train<span class="token punctuation">(</span>x<span class="token punctuation">,</span> sampling_results<span class="token punctuation">,</span>                                            gt_bboxes<span class="token punctuation">,</span> gt_labels<span class="token punctuation">,</span>                                            img_metas<span class="token punctuation">)</span>    losses<span class="token punctuation">.</span>update<span class="token punctuation">(</span>bbox_results<span class="token punctuation">[</span><span class="token string">'loss_bbox'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># mask head forward and loss</span><span class="token keyword">if</span> self<span class="token punctuation">.</span>with_mask<span class="token punctuation">:</span>    mask_results <span class="token operator">=</span> self<span class="token punctuation">.</span>_mask_forward_train<span class="token punctuation">(</span>x<span class="token punctuation">,</span> sampling_results<span class="token punctuation">,</span>                                            bbox_results<span class="token punctuation">[</span><span class="token string">'bbox_feats'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                            gt_masks<span class="token punctuation">,</span> img_metas<span class="token punctuation">)</span>    losses<span class="token punctuation">.</span>update<span class="token punctuation">(</span>mask_results<span class="token punctuation">[</span><span class="token string">'loss_mask'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">return</span> losses<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>_bbox_forward_train</code> 逻辑和 RPN 非常类似，只不过多了额外的 RoI 区域特征提取步骤：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_bbox_forward_train</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> sampling_results<span class="token punctuation">,</span> gt_bboxes<span class="token punctuation">,</span> gt_labels<span class="token punctuation">,</span>                        img_metas<span class="token punctuation">)</span><span class="token punctuation">:</span>    rois <span class="token operator">=</span> bbox2roi<span class="token punctuation">(</span><span class="token punctuation">[</span>res<span class="token punctuation">.</span>bboxes <span class="token keyword">for</span> res <span class="token keyword">in</span> sampling_results<span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token comment"># 特征重映射+ RoI 特征提取+ 网络 forward</span>    bbox_results <span class="token operator">=</span> self<span class="token punctuation">.</span>_bbox_forward<span class="token punctuation">(</span>x<span class="token punctuation">,</span> rois<span class="token punctuation">)</span>    <span class="token comment"># 计算每个样本对应的 target, bbox encoder 在内部进行</span>    bbox_targets <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">.</span>get_targets<span class="token punctuation">(</span>sampling_results<span class="token punctuation">,</span> gt_bboxes<span class="token punctuation">,</span>                                              gt_labels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>train_cfg<span class="token punctuation">)</span>    <span class="token comment"># 计算 loss</span>    loss_bbox <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>bbox_results<span class="token punctuation">[</span><span class="token string">'cls_score'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                    bbox_results<span class="token punctuation">[</span><span class="token string">'bbox_pred'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> rois<span class="token punctuation">,</span>                                    <span class="token operator">*</span>bbox_targets<span class="token punctuation">)</span>    bbox_results<span class="token punctuation">.</span>update<span class="token punctuation">(</span>loss_bbox<span class="token operator">=</span>loss_bbox<span class="token punctuation">)</span>    <span class="token keyword">return</span> bbox_results<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>_bbox_forward</code> 逻辑是 R-CNN 的重点：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_bbox_forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> rois<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 特征重映射+ RoI 区域特征提取，仅仅考虑前 num_inputs 个特征图</span>    bbox_feats <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_roi_extractor<span class="token punctuation">(</span>        x<span class="token punctuation">[</span><span class="token punctuation">:</span>self<span class="token punctuation">.</span>bbox_roi_extractor<span class="token punctuation">.</span>num_inputs<span class="token punctuation">]</span><span class="token punctuation">,</span> rois<span class="token punctuation">)</span>    <span class="token comment"># 共享模块</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>with_shared_head<span class="token punctuation">:</span>        bbox_feats <span class="token operator">=</span> self<span class="token punctuation">.</span>shared_head<span class="token punctuation">(</span>bbox_feats<span class="token punctuation">)</span>    <span class="token comment"># 独立分类和回归 head</span>    cls_score<span class="token punctuation">,</span> bbox_pred <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">(</span>bbox_feats<span class="token punctuation">)</span>    bbox_results <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>        cls_score<span class="token operator">=</span>cls_score<span class="token punctuation">,</span> bbox_pred<span class="token operator">=</span>bbox_pred<span class="token punctuation">,</span> bbox_feats<span class="token operator">=</span>bbox_feats<span class="token punctuation">)</span>    <span class="token keyword">return</span> bbox_results<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>(3) 测试逻辑</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">rcnn<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    score_thr<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">,</span>    nms<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'nms'</span><span class="token punctuation">,</span> iou_threshold<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    max_per_img<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>测试逻辑核心逻辑如下：</p><ul><li>公共逻辑部分输出 batch * nms_post 个候选框的分类和回归预测结果</li><li>将所有预测结果按照 batch 维度进行切分，然后依据单张图片进行后处理，后处理逻辑为：先解码并还原为原图尺度；然后利用 <code>score_thr</code> 去除低分值预测；然后进行 NMS；最后保留最多 <code>max_per_img</code> 个结果</li></ul><h2 id="3-Mask-R-CNN-代码详解"><a href="#3-Mask-R-CNN-代码详解" class="headerlink" title="3 Mask R-CNN 代码详解"></a>3 Mask R-CNN 代码详解</h2><p>Mask R-CNN 和 Faster R-CNN 的区别主要包括两个方面：</p><ul><li>R-CNN 中额外引入 Mask Head，从而可以实现实例分割任务</li><li>针对特征图与原始图像上的 RoI 不对准问题，提出了 RoIPool 的改进版本 RoIAlign</li></ul><h3 id="3-1-Mask-Head"><a href="#3-1-Mask-Head" class="headerlink" title="3.1 Mask Head"></a>3.1 Mask Head</h3><p><img src="https://pic2.zhimg.com/80/v2-5d406aac4497e77d37ae7c3fbfb35349_720w.jpg" alt="img"></p><p>上图为在 Faster R-CNN 和 FPN 基础上扩展得到 Mask R-CNN 模型，本文解读的是 FPN 扩展的 Mask R-CNN 模型，其简要描述为：</p><ul><li>bbox 分支和 mask 分支都有自己的 RoI 区域特征提取算子，其中由于 mask 任务要求更加精细的结果，所以 RoI 特征区域提取算子输出特征图比 bbox 分支大，一般设置为 14</li><li>bbox 分支是全连接结构，而 mask 分支是全卷积结构，输出 shape 为 <code>(batch* num_post,80,28,28)</code>，其中 80 表示类别，即输出的每个通道代表一个类别的 mask</li><li>由于 mask 分支目的是对前景物体进行分割，故该分支的输入仅仅包括 Bbox Assigner 加上 Bbox Sampler 后的正样本而已</li><li>在测试阶段，为了避免 bbox 和 mask 结果没有对齐，做法是<strong>先对 bbox 分支进行后处理，得到预测的 bbox，然后将 bbox 作为 proposal 输入到 mask 分支中进行 RoI 特征区域提取和 forward，输出和类别相关的 mask</strong>。为了得到二值分割图，还需要在 <code>test_cfg</code> 指定 <code>mask_thr_binary</code> 参数，一般设置为 0.5</li></ul><p>结合上面图示和配置就可以知道 Mask R-CNN 所有细节：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">roi_head<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># 和 Faster R-CNN 完全相同</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'StandardRoIHead'</span><span class="token punctuation">,</span>    bbox_roi_extractor<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'SingleRoIExtractor'</span><span class="token punctuation">,</span>        roi_layer<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'RoIAlign'</span><span class="token punctuation">,</span> output_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> sampling_ratio<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        out_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>        featmap_strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    bbox_head<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'Shared2FCBBoxHead'</span><span class="token punctuation">,</span>        in_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>        fc_out_channels<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span>        roi_feat_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span>        num_classes<span class="token operator">=</span><span class="token number">80</span><span class="token punctuation">,</span>        bbox_coder<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>            <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'DeltaXYWHBBoxCoder'</span><span class="token punctuation">,</span>            target_means<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            target_stds<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        reg_class_agnostic<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>        loss_cls<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>            <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'CrossEntropyLoss'</span><span class="token punctuation">,</span> use_sigmoid<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        loss_bbox<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'L1Loss'</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># mask 分支也有自己的 RoIAlign     </span>    mask_roi_extractor<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'SingleRoIExtractor'</span><span class="token punctuation">,</span>        <span class="token comment"># 除了 output_size 不同外，其他都相同</span>        roi_layer<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'RoIAlign'</span><span class="token punctuation">,</span> output_size<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">,</span> sampling_ratio<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        out_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>        featmap_strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 全卷积 head</span>    mask_head<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'FCNMaskHead'</span><span class="token punctuation">,</span>        <span class="token comment"># 总共 4 层卷积</span>        num_convs<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>        in_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>        <span class="token comment"># 中间卷积通道数</span>        conv_out_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>        <span class="token comment"># 输出类别</span>        num_classes<span class="token operator">=</span><span class="token number">80</span><span class="token punctuation">,</span>        <span class="token comment"># loss 采用 bce loss</span>        loss_mask<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>            <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'CrossEntropyLoss'</span><span class="token punctuation">,</span> use_mask<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-2-RoIAlign-和-RoIPool"><a href="#3-2-RoIAlign-和-RoIPool" class="headerlink" title="3.2 RoIAlign 和 RoIPool"></a>3.2 RoIAlign 和 RoIPool</h3><p>Mask R-CNN 中一个比较大的创新点就是提出了 RoIAlign 层，本小节先描述 RoIPool 层原理以及缺点，然后再分析 RoIAlign。</p><p><strong>(1) RoIPool</strong></p><p>首先需要明确其作用： <strong>将任意大小的特征图都池化为指定输出大小</strong>。示意图如下：</p><p><img src="https://pic1.zhimg.com/80/v2-f0b90b9532a88513dcf7cef347c10a00_720w.jpg" alt="img"></p><p>假设左上图为输入特征图，bbox 分支预测的坐标可能是浮点数，设置 RoIPool 输出 size 是 (2,2)</p><ul><li>首先将 bbox 预测值转化为整数，得到右上蓝框</li><li>将蓝框内 5x7 的特征图均匀切割为 2x2 的块，但是由于取整操作，实际上第一个块是 wh=(7//2,5//2)，第二个块是 wh=(7-7//2,5-5//2), 后面类似，从而得到左下图示</li><li>然后在每个小块内采用 MaxPool 提取最大值，从而得到右下角的 2x2 输出</li></ul><p>可以发现 RoIPool 存在两次取整操作，第一次是将 proposal 值变成整数，第二次是均匀切割时候。对于小物体的特征图而言，两次取整操作特征图会产生比较大的偏差，从而对 分割和定位精度有比较大的影响，在论文里作者把它总结为“不匹配问题”(mis-alignment)。</p><p><strong>(2) RoIAlign</strong> 为了解决这个问题，RoIAlign 取消两次整数化操作，保留了小数，每个小数位置都采用双线性插值方法获得坐标为浮点数的特征图上数值, 其可视化如下所示：</p><p><img src="https://pic2.zhimg.com/80/v2-fa489fcd45ed2027402ddb92505d3b19_720w.jpg" alt="img"></p><p>其对应的论文图示如下：</p><p><img src="https://pic3.zhimg.com/80/v2-8d5013acfb3e16270b29ef82ebf15e96_720w.jpg" alt="img"></p><p>假设黑色大框是要切割的 bbox，打算输出 size 为 2x2 输出，则先把黑色大 bbox 均匀切割为 4 个小 bbox，然后在每个小 bbox 内部均匀采样 4 个点(相当于每个小 bbox 内部再次均匀切割为 2x2 共 4 个小块，取每个小块的中心点即可)，首先对每个采样点利用双线性插值函数得到该浮点值处的值(插值的 4 个整数点是上下左右最近的 4 个点)，然后对 4 个采样点采样值取 max 操作得到该小 bbox 的最终值。采样个数 4 是超参，实验发现设置为 4 的时候速度和精度都是最合适的。</p><p>关于 RoIAlign 和 RoIPool 的源码实现，由于是采用 CUDA 编程，在后续系列中统一进行分析。</p><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h2><p>本文重点分析了主流的带有 FPN 模块的 Faster R-CNN 算法，对每个组件都进行了详细分析，在此基础上，对其扩展版本 Mask R-CNN 进行了详细描述。通过本系列第一篇 RetinaNet 和 本文第二篇 Faster R-CNN 和 Mask R-CNN的解读，希望大家可以了解：</p><ul><li><p>掌握 MMDetection 中涉及到的常用配置参数含义</p></li><li><p>掌握 MMDetection 框架中两个经典算法 RetinaNet 和 Faster R-CNN</p></li><li><p>对 one-stage 和 two-stage 算法有清晰的理解，例如能够理解 two-stage 实际上可以认为是 one-stage + RoI 区域特征提取 + one-stage，对于 Cascade R-CNN 这类算法可以认为是 one-stage + RoI 区域特征提取 + one-stage + RoI 区域特征提取 + one-stage</p></li><li><p>RetinaNet 和 Faster R-CNN 解读完后，大家阅读 MMDetection 中大部分算法都会相对容易2.2 MMDetection<br>  使用Pytorch构建一个新算法时，通常包含如下几步：</p><p>注册数据集：CustomDataset是MMDetection在原始的Dataset基础上的再次封装，其__getitem__()方法会根据训练和测试模式分别重定向到prepare_train_img()和prepare_test_img()函数。用户以继承CustomDataset类的方式构建自己的数据集时，需要重写load_annotations()和get_ann_info()函数，定义数据和标签的加载及遍历方式。完成数据集类的定义后，还需要使用DATASETS.register_module()进行模块注册。<br>注册模型：模型构建的方式和Pytorch类似，都是新建一个Module的子类然后重写forward()函数。唯一的区别在于MMDetection中需要继承BaseModule而不是Module，BaseModule是Module的子类，MMLab中的任何模型都必须继承此类。另外，MMDetection将一个完整的模型拆分为backbone、neck和head三部分进行管理，所以用户需要按照这种方式，将算法模型拆解成3个类，分别使用BACKBONES.register_module()、NECKS.register_module()和HEADS.register_module()完成模块注册。<br>构建配置文件：配置文件用于配置算法各个组件的运行参数，大体上可以包含四个部分：datasets、models、schedules和runtime。完成相应模块的定义和注册后，在配置文件中配置好相应的运行参数，然后MMDetection就会通过Registry类读取并解析配置文件，完成模块的实例化。另外，配置文件可以通过_base_字段实现继承功能，以提高代码复用率。<br>训练和验证：在完成各模块的代码实现、模块的注册、配置文件的编写后，就可以使用./tools/train.py和./tools/test.py对模型进行训练和验证，不需要用户编写额外的代码。<br>————————————————<br>版权声明：本文为CSDN博主「Maples丶丶」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/qq_16137569/article/details/121316235">https://blog.csdn.net/qq_16137569/article/details/121316235</a></p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>轻松掌握 MMDetection 中常用算法(一)：RetinaNet 及配置详解</title>
      <link href="/2022/04/05/mmdetection-xue-xi-bi-ji-3-gou-jian-retinanet-ji-pei-zhi/"/>
      <url>/2022/04/05/mmdetection-xue-xi-bi-ji-3-gou-jian-retinanet-ji-pei-zhi/</url>
      
        <content type="html"><![CDATA[<h1 id="轻松掌握-MMDetection-中常用算法-一-：RetinaNet-及配置详解"><a href="#轻松掌握-MMDetection-中常用算法-一-：RetinaNet-及配置详解" class="headerlink" title="轻松掌握 MMDetection 中常用算法(一)：RetinaNet 及配置详解"></a>轻松掌握 MMDetection 中常用算法(一)：RetinaNet 及配置详解</h1><h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0 前言"></a>0 前言</h2><p>在解读完<a href="https://zhuanlan.zhihu.com/p/337375549">轻松掌握 MMDetection 训练和测试流程</a>的相关系列文章后，相信大家对 MMDetection 框架训练和测试流程以及各个组件的内部抽象实现有了一定了解。本系列文章则从框架中已经实现的一些常用算法入手，通过对这些算法进行深度解析，使读者能够对 MMDetection 有进一步深入理解。本系列文章希望达到的目的是：</p><ul><li>通过<strong>对常用算法进行深度解析</strong>，使读者能够对该系列算法及其改进算法的实现有非常透彻的理解</li><li>通过对算法相<strong>关的配置文件解读和扩展</strong>，使读者能够更加熟练的自定义配置</li><li>通过对常用算法进行解读，使读者能够更加便捷快速的<strong>使用 MMDetection</strong></li></ul><p>作为开篇系列，第一个解读算法是 RetinaNet。或许大家会有所疑问：为什么第一个解读算法不是经典的 Faster R-CNN? 这是由于考虑到 Faster R-CNN 包括两个阶段，复杂度比较高，并且第一阶段可以认为是 one-stage 检测器。为了<strong>降低理解 Faster R-CNN 难度</strong>，我们<strong>先分析经典的 one-stage 算法 RetinaNet</strong>。在理解该算法基础上再去理解 Faster R-CNN，应该会更加容易，思路也会更加清晰。</p><p>本系列文章的重点是解读 MMDetection 中相关算法实现，对于其原理描述的比较简单，并且一些配置和参数都是以 MMDetection 默认参数为准，某些参数和设置不完全与论文相同。</p><p>在阅读本文前，请先阅读<a href="https://zhuanlan.zhihu.com/p/337375549">前置系列文章</a>。MMDetection 依然在快速发展，本文解读的版本是 V2.8。</p><p><strong>需要特别注意的是：</strong>由于本文是系列文章开篇，所涉及的内容不仅仅是 RetinaNet，还<strong>包括了配置文件里面每个参数的详细解读</strong>(这个非常关键)，在后续文章中如果出现重复配置就不再描述，故<strong>不管你对 RetinaNet 有多了解，如果你想进一步熟悉 MMDetection 参数配置及其含义，那么本文可能对你有帮助</strong>。</p><h2 id="1-RetinaNet-简要介绍"><a href="#1-RetinaNet-简要介绍" class="headerlink" title="1 RetinaNet 简要介绍"></a>1 RetinaNet 简要介绍</h2><p><strong>RetinaNet 来自 FAIR 论文：Focal Loss for Dense Object Detection</strong>，其简要概述为：深入分析了<strong>极度不平衡的正负（前景背景）样本比例</strong>导致 one-stage 检测器精度低于 two-stage 检测器，基于上述分析，提出了一种简单但是非常实用的 <strong>Focal Loss 焦点损失函数</strong>，并且 Loss 设计思想可以推广到其他领域，同时针对目标检测领域特定问题，设计了 <strong>RetinaNet 网络</strong>，结合 Focal Loss 使得 one-stage 检测器在精度上能够达到乃至超过 two-stage 检测器。</p><p>其简要网络结构图如下所示：</p><p><img src="https://pic3.zhimg.com/80/v2-35848a98ce3803dde4d7c533edbb1972_720w.jpg" alt="img"></p><p>总的来说，RetinaNet 有<strong>两个大创新</strong>：</p><ul><li>Focal Loss</li><li>RetinaNet 网络</li></ul><p><strong>Focal Loss 几乎已经成为 one-stage 算法的标配</strong>，而 RetinaNet 网络结构也是目前<strong>主流的目标检测网络结构</strong>，其变体不计其数。</p><h2 id="2-RetinaNet-代码详解"><a href="#2-RetinaNet-代码详解" class="headerlink" title="2 RetinaNet 代码详解"></a>2 RetinaNet 代码详解</h2><p>在<a href="https://zhuanlan.zhihu.com/p/337375549">轻松掌握 MMDetection 整体构建流程(一)</a>一文中分析了 MMDetection 中模型构建的基本组件，RetinaNet 涉及的组件包括： Backbone、Neck、Head、BBox Assigner、BBox Encoder Decoder、Loss 和 BBox PostProcess。下面按照顺序结合代码详细分析。</p><h3 id="2-1-Backbone"><a href="#2-1-Backbone" class="headerlink" title="2.1 Backbone"></a>2.1 Backbone</h3><p><img src="https://pic4.zhimg.com/80/v2-7face90299b486de63263311f200c58f_720w.jpg" alt="img"></p><p>标准的 RetinaNet 骨架网络<strong>采用的是 ResNet 系列</strong>。由于骨架本身没有限制，MMDetection 中目前提供的预训练权重所涉及的骨架网络包括：ResNet50-Caffe、ResNet50-Pytorch、ResNet101-Caffe、ResNet101-Pytorch、ResNeXt101，非常丰富。</p><p>为了读者好理解，先解释<strong>下配置文件名含义</strong>：</p><ul><li>retinanet 表示算法名称</li><li>r50 等表示骨架网络名</li><li>caffe 和 PyTorch 是指 Bottleneck 模块的区别，省略情况下表示是 PyTorch，后面会详细说明</li><li>fpn 表示 Neck 模块采用了 FPN 结构</li><li>mstrain 表示多尺度训练，一般对应的是 pipeline 中 <code>Resize</code> 类</li><li>1x 表示 1 倍数的 epoch 训练即 12 个 epoch，2x 则表示 24 个 epcoh</li><li>coco 表示在 COCO 数据集上训练</li></ul><p><strong>以 ResNet50 为例进行具体分析，骨架网络配置如下：</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 使用 pytorch 提供的在 imagenet 上面训练过的权重作为预训练权重</span>pretrained<span class="token operator">=</span><span class="token string">'torchvision://resnet50'</span><span class="token punctuation">,</span>backbone<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># 骨架网络类名</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'ResNet'</span><span class="token punctuation">,</span>    <span class="token comment"># 表示使用 ResNet50</span>    depth<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span>    <span class="token comment"># ResNet 系列包括 stem+ 4个 stage 输出</span>    num_stages<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    <span class="token comment"># 表示本模块输出的特征图索引，(0, 1, 2, 3),表示4个 stage 输出都需要，</span>    <span class="token comment"># 其 stride 为 (4,8,16,32)，channel 为 (256, 512, 1024, 2048)</span>    out_indices<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 表示固定 stem 加上第一个 stage 的权重，不进行训练</span>    frozen_stages<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>    <span class="token comment"># 所有的 BN 层的可学习参数都不需要梯度，也就不会进行参数更新</span>    norm_cfg<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'BN'</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># backbone 所有的 BN 层的均值和方差都直接采用全局预训练值，不进行更新</span>    norm_eval<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    <span class="token comment"># 默认采用 pytorch 模式</span>    style<span class="token operator">=</span><span class="token string">'pytorch'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看出，RetinaNet 算法采用了 ResNet50 作为 Backbone, 并且考虑到整个目标检测网络比较大，<strong>前面部分网络没有进行训练，BN 也不会进行参数更新</strong>。需要说明的是上述默认配置是经过前人工作和 OpenMMLab 在 COCO 数据集上<strong>不断实践的结果</strong>。推荐大家直接使用该配置模式，效果相对比较稳定。</p><h4 id="1-out-indices"><a href="#1-out-indices" class="headerlink" title="(1) out_indices"></a><strong>(1) out_indices</strong></h4><p><strong>ResNet 提出了骨架网络设计范式即 <code>stem+n stage+ cls head</code><strong>，对于 ResNet 而言，其实际 forward 流程是 <code>stem -&gt; 4 个 stage -&gt; 分类 head</code>，stem 的输出 stride 是 4，而 4 个 stage 的输出 stride 是 4,8,16,32，</strong>这 4 个输出就对应 <code>out_indices</code> 索引</strong>。例如如果你想要输出 stride=4 的特征图，那么你可以设置 <code>out_indices=(0,)</code>，如果你想要输出 stride=4 和 8 的特征图，那么你可以设置 <code>out_indices=(0, 1)</code>。</p><p>因为 RetinaNet 后面需要接 FPN，故需要输出 4 个尺度特征图，简要代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> i<span class="token punctuation">,</span> layer_name <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>res_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>    res_layer <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> layer_name<span class="token punctuation">)</span>    x <span class="token operator">=</span> res_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token comment"># 如果 i 在 self.out_indices 中才保留</span>    <span class="token keyword">if</span> i <span class="token keyword">in</span> self<span class="token punctuation">.</span>out_indices<span class="token punctuation">:</span>        outs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-frozen-stages"><a href="#2-frozen-stages" class="headerlink" title="(2) frozen_stages"></a><strong>(2) frozen_stages</strong></h4><p>该参数表示你想冻结前几个 stages 的权重，ResNet 结构包括 stem+4 stage</p><ul><li>frozen_stages=-1，表示全部可学习</li><li>frozen_stage=0，表示stem权重固定</li><li>frozen_stages=1，表示 stem 和第一个 stage 权重固定</li><li>frozen_stages=2，表示 stem 和前两个 stage 权重固定</li></ul><p>依次类推，具体代码为：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 固定权重，需要两个步骤：1. 设置 eval 模式；2. requires_grad=False</span><span class="token keyword">def</span> <span class="token function">_freeze_stages</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>frozen_stages <span class="token operator">&gt;=</span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token comment"># 固定 stem 权重</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>deep_stem<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>stem<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">for</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>stem<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>norm1<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">for</span> m <span class="token keyword">in</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">,</span> self<span class="token punctuation">.</span>norm1<span class="token punctuation">]</span><span class="token punctuation">:</span>                <span class="token keyword">for</span> param <span class="token keyword">in</span> m<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                    param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>    <span class="token comment"># 固定 stage 权重</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>frozen_stages <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        m <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f'layer</span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>        m<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> param <span class="token keyword">in</span> m<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>需要特别注意的是</strong>：上述函数不能仅仅在类初始化时候调用，因为在训练模式下，运行时候会调用 <code>model.train()</code> 导致 BN 层又进入 train 模式，最终 BN 没有被固定，故需要在 ResNet 中重写 train 方法，如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 这行代码会导致 BN 进入 train 模式</span>    <span class="token builtin">super</span><span class="token punctuation">(</span>ResNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>train<span class="token punctuation">(</span>mode<span class="token punctuation">)</span>    <span class="token comment"># 再次调用，固定 stem 和 前 n 个 stage 的 BN</span>    self<span class="token punctuation">.</span>_freeze_stages<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># 如果所有 BN 都采用全局均值和方差，则需要对整个网络的 BN 都开启 eval 模式</span>    <span class="token keyword">if</span> mode <span class="token keyword">and</span> self<span class="token punctuation">.</span>norm_eval<span class="token punctuation">:</span>        <span class="token keyword">for</span> m <span class="token keyword">in</span> self<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment"># trick: eval have effect on BatchNorm only</span>            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> _BatchNorm<span class="token punctuation">)</span><span class="token punctuation">:</span>                m<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果你自定义了骨架网络，想实现固定某一部分权重功能，你可以参考上述做法。</p><p><strong>(3) norm_cfg 和 norm_eval</strong></p><p><code>norm_cfg</code> 表示所采用的归一化算子，一般是 BN 或者 GN，而 <code>requires_grad</code> 表示该算子是否需要梯度，也就是是否进行参数更新，而布尔参数 <code>norm_eval</code> 是用于控制整个骨架网络的归一化算子是否需要变成 eval 模式。</p><p>RetinaNet 中用法是 <code>norm_cfg=dict(type='BN', requires_grad=True)</code>，表示通过 Registry 模式实例化 BN 类，并且设置为参数可学习。在 MMDetection 中会常看到通过字典配置方式来实例化某个类的做法， 底层是采用了装饰器模式进行构建，最大好处是扩展性极强，类和类之间的耦合度降低。</p><p><strong>(4) style</strong></p><p><code>style='caffe'</code> 和 <code>style='pytorch'</code> 的差别就在 <code>Bottleneck</code> 模块中</p><p><img src="https://pic3.zhimg.com/80/v2-d62671f250295573295414341010cb3e_720w.jpg" alt="img"></p><p><code>Bottleneck</code> 是标准的 1x1-3x3-1x1 结构，考虑 stride=2 下采样的场景，caffe 模式下，stride 参数放置在第一个 1x1 卷积上，而 Pyorch 模式下，stride 放在第二个 3x3 卷积上：</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">if self.style == 'pytorch':        self.conv1_stride = 1        self.conv2_stride = stride    else:        self.conv1_stride = stride        self.conv2_stride = 1<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>出现两种模式的原因是因为 ResNet 本身就有不同的实现，torchvision 的 resnet 和早期 release 的 resnet 版本不一样，使得目标检测框架在使用 Backbone 的时候有两种不同的配置，不过目前新网络都是采用 PyTorch 模式。</p><h3 id="2-2-Neck"><a href="#2-2-Neck" class="headerlink" title="2.2 Neck"></a>2.2 Neck</h3><p>Neck 模块即为 FPN，其简要结构如下所示：</p><p><img src="https://pic3.zhimg.com/80/v2-63d4cede659613ffd0886ddff49591ee_720w.jpg" alt="img"></p><p>MMDetection 中对应配置为：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">neck<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'FPN'</span><span class="token punctuation">,</span>    <span class="token comment"># ResNet 模块输出的4个尺度特征图通道数</span>    in_channels<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">2048</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token comment"># FPN 输出的每个尺度输出特征图通道</span>    out_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>    <span class="token comment"># 从输入多尺度特征图的第几个开始计算</span>    start_level<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>    <span class="token comment"># 额外输出层的特征图来源</span>    add_extra_convs<span class="token operator">=</span><span class="token string">'on_input'</span><span class="token punctuation">,</span>    <span class="token comment"># FPN 输出特征图个数</span>    num_outs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>前面说过 ResNet 输出 4 个不同尺度特征图 (c2,c3,c4,c5)，stride 分别是 (4,8,16,32)，通道数为 (256,512,1024,2048),通过配置文件我们可以知道：</p><ul><li><code>start_level=1</code> 说明虽然输入是 4 个特征图，但是实际上 FPN 中仅仅用了后面三个</li><li><code>num_outs=5</code> 说明 FPN 模块虽然是接收 3 个特征图，但是输出 5 个特征图</li><li><code>add_extra_convs='on_input'</code> 说明额外输出的 2 个特征图的来源是骨架网络输出，而不是 FPN 层本身输出又作为后面层的输入</li><li><code>out_channels=256</code> 说明了 5 个输出特征图的通道数都是 256</li></ul><p>下面对代码运行流程进行描述：</p><ol><li>将 c3、c4 和 c5 三个特征图全部经过各自 1x1 卷积进行通道变换得到 m3~m5，输出通道统一为 256</li><li>从 m5(特征图最小)开始，先进行 2 倍最近邻上采样，然后和 m4 进行 add 操作，得到新的 m4</li><li>将新 m4 进行 2 倍最近邻上采样，然后和 m3 进行 add 操作，得到新的 m3</li><li>对 m5 和新融合后的 m4、m3，都进行各自的 3x3 卷积，得到 3 个尺度的最终输出 P5～P3</li><li>将 c5 进行 3x3 且 stride=2 的卷积操作，得到 P6</li><li>将 P6 再一次进行 3x3 且 stride=2 的卷积操作，得到 P7</li></ol><p>P6 和 P7 目的是提供一个大感受野强语义的特征图，有利于大物体和超大物体检测。 在 RetinaNet 的 FPN 模块中只包括卷积，不包括 BN 和 ReLU。</p><p>总结：FPN 模块接收 c3, c4, c5 三个特征图，输出 P3-P7 五个特征图，通道数都是 256, stride 为 (8,16,32,64,128)，其中大 stride (特征图小)用于检测大物体，小 stride (特征图大)用于检测小物体。</p><h3 id="2-3-Head"><a href="#2-3-Head" class="headerlink" title="2.3 Head"></a>2.3 Head</h3><p>论文中作者认为 one-stage 算法 <strong>head 设计比较关键</strong>，对最终性能影响较大，相比于其余 one-stage 算法，RetinaNet 的 Head 模块比较重量级，<strong>输出头包括分类和检测两个分支</strong>，且每个分支都包括 4 个卷积层，不进行参数共享，<strong>分类 Head 输出通道是 <code>num_class*K</code><strong>，</strong>检测 head 输出通道是<code>4*K</code>, K 是 anchor 个数</strong>, 虽然每个 Head 的分类和回归分支权重不共享，但是 <strong>5 个输出特征图的 Head 模块权重是共享的</strong>。</p><p>其完整配置如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">bbox_head<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'RetinaHead'</span><span class="token punctuation">,</span>    <span class="token comment"># COCO 数据集类别个数</span>    num_classes<span class="token operator">=</span><span class="token number">80</span><span class="token punctuation">,</span>    <span class="token comment"># FPN 层输出特征图通道数</span>    in_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>    <span class="token comment"># 每个分支堆叠4层卷积</span>    stacked_convs<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    <span class="token comment"># 中间特征图通道数</span>    feat_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>    <span class="token comment"># 后面分析</span>    anchor_generator<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'AnchorGenerator'</span><span class="token punctuation">,</span>        octave_base_scale<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>        scales_per_octave<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>        ratios<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 后面分析</span>    bbox_coder<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'DeltaXYWHBBoxCoder'</span><span class="token punctuation">,</span>        target_means<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        target_stds<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 后面分析</span>    loss_cls<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'FocalLoss'</span><span class="token punctuation">,</span>        use_sigmoid<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>        gamma<span class="token operator">=</span><span class="token number">2.0</span><span class="token punctuation">,</span>        alpha<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">,</span>        loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 后面分析</span>    loss_bbox<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'L1Loss'</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Head 模块比较简单，网络构建代码如下所示:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">_init_layers</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># stacked_convs=4</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>stacked_convs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 构建4个中间卷积层，分类和回归分支不共享权重</span>        chn <span class="token operator">=</span> self<span class="token punctuation">.</span>in_channels <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">else</span> self<span class="token punctuation">.</span>feat_channels        self<span class="token punctuation">.</span>cls_convs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>            ConvModule<span class="token punctuation">(</span>                chn<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>feat_channels<span class="token punctuation">,</span>                <span class="token number">3</span><span class="token punctuation">,</span>                <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>reg_convs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>            ConvModule<span class="token punctuation">(</span>                chn<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>feat_channels<span class="token punctuation">,</span>                <span class="token number">3</span><span class="token punctuation">,</span>                <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment"># 构建最终输出层</span>    self<span class="token punctuation">.</span>retina_cls <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>        self<span class="token punctuation">.</span>feat_channels<span class="token punctuation">,</span>        self<span class="token punctuation">.</span>num_anchors <span class="token operator">*</span> self<span class="token punctuation">.</span>cls_out_channels<span class="token punctuation">,</span>        <span class="token number">3</span><span class="token punctuation">,</span>        padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>retina_reg <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>        self<span class="token punctuation">.</span>feat_channels<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_anchors <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>单张特征图的 forward 流程为:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># x是 p3-p7 中的某个特征图</span>cls_feat <span class="token operator">=</span> xreg_feat <span class="token operator">=</span> x<span class="token comment"># 4层不共享参数卷积</span><span class="token keyword">for</span> cls_conv <span class="token keyword">in</span> self<span class="token punctuation">.</span>cls_convs<span class="token punctuation">:</span>     cls_feat <span class="token operator">=</span> cls_conv<span class="token punctuation">(</span>cls_feat<span class="token punctuation">)</span><span class="token keyword">for</span> reg_conv <span class="token keyword">in</span> self<span class="token punctuation">.</span>reg_convs<span class="token punctuation">:</span>     reg_feat <span class="token operator">=</span> reg_conv<span class="token punctuation">(</span>reg_feat<span class="token punctuation">)</span><span class="token comment"># 输出特征图</span>cls_score <span class="token operator">=</span> self<span class="token punctuation">.</span>retina_cls<span class="token punctuation">(</span>cls_feat<span class="token punctuation">)</span>bbox_pred <span class="token operator">=</span> self<span class="token punctuation">.</span>retina_reg<span class="token punctuation">(</span>reg_feat<span class="token punctuation">)</span><span class="token keyword">return</span> cls_score<span class="token punctuation">,</span> bbox_pred<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>5 个输出 Head 共享所有分类或者回归分支的卷积权重，经过 Head 模块的前向流程输出一共是 5*2 个特征图。</p><h3 id="2-4-BBox-Assigner"><a href="#2-4-BBox-Assigner" class="headerlink" title="2.4 BBox Assigner"></a>2.4 BBox Assigner</h3><h3 id="2-4-1-AnchorGenerator"><a href="#2-4-1-AnchorGenerator" class="headerlink" title="2.4.1 AnchorGenerator"></a>2.4.1 AnchorGenerator</h3><p>RetinaNet 属于 Anchor-based 算法，在运行 bbox 属性分配前<strong>需要得到每个输出特征图位置的 anchor 列表</strong>，故在分析 BBox Assigner 前，需要先详细说明下 anchor 生成过程，其对应配置如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">anchor_generator<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'AnchorGenerator'</span><span class="token punctuation">,</span>    <span class="token comment"># 特征图 anchor 的 base scale, 值越大，所有 anchor 的尺度都会变大</span>    octave_base_scale<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    <span class="token comment"># 每个特征图有3个尺度，2**0, 2**(1/3), 2**(2/3)</span>    scales_per_octave<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>    <span class="token comment"># 每个特征图有3个高宽比例</span>    ratios<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token comment"># 特征图对应的 stride，必须特征图 stride 一致，不可以随意更改</span>    strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>从上面配置可以看出：RetinaNet 一共 5 个输出特征图，每个特征图上有 3 种尺度和 3 种宽高比，每个位置一共 9 个 anchor，并且通过 <code>octave_base_scale</code> 参数来控制全局 anchor 的 base scales ，如果自定义数据集中普遍都是大物体或者小物体，则可能修改更改 <code>octave_base_scale</code> 参数。</p><p><strong>为了方便理解，可以写个简单脚本可视化下指定特征图位置的 anchor 情况。</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> mmcv<span class="token punctuation">.</span>visualization <span class="token keyword">import</span> imshow_bboxes<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">from</span> mmdet<span class="token punctuation">.</span>core <span class="token keyword">import</span> build_anchor_generator<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    anchor_generator_cfg <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>        <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'AnchorGenerator'</span><span class="token punctuation">,</span>        octave_base_scale<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>        scales_per_octave<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>        ratios<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        strides<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    anchor_generator <span class="token operator">=</span> build_anchor_generator<span class="token punctuation">(</span>anchor_generator_cfg<span class="token punctuation">)</span>    <span class="token comment"># 输出原图尺度上 anchor 坐标 xyxy 左上角格式</span>    <span class="token comment"># base_anchors 长度为5，表示5个输出特征图，不同的特征图尺度相差的只是 strides</span>    <span class="token comment"># 故我们取 strides=8 的位置 anchor 可视化即可</span>    base_anchors <span class="token operator">=</span> anchor_generator<span class="token punctuation">.</span>base_anchors<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    h <span class="token operator">=</span> <span class="token number">100</span>    w <span class="token operator">=</span> <span class="token number">160</span>    img <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">[</span>h<span class="token punctuation">,</span> w<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">255</span>    base_anchors<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+=</span> w <span class="token operator">//</span> <span class="token number">2</span>    base_anchors<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+=</span> h <span class="token operator">//</span> <span class="token number">2</span>    colors <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'green'</span><span class="token punctuation">,</span> <span class="token string">'red'</span><span class="token punctuation">,</span> <span class="token string">'blue'</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        base_anchor <span class="token operator">=</span> base_anchors<span class="token punctuation">[</span>i<span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>        imshow_bboxes<span class="token punctuation">(</span>img<span class="token punctuation">,</span> base_anchor<span class="token punctuation">,</span> show<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> colors<span class="token operator">=</span>colors<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">)</span>    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>结果如下所示：</p><p><img src="https://pic3.zhimg.com/80/v2-6d3d2c417ced4976933b3c4c7ec7e64a_720w.jpg" alt="img"></p><p>相同颜色表示在该特征图中基本尺度是相同的，只是宽高比不一样而已。</p><p>在对 AnchorGenerator 有基本认识后，下面对其实现源码进行分析：</p><p><strong>(1) 先对单个位置 (0,0) 生成 base anchors</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">w <span class="token operator">=</span> base_sizeh <span class="token operator">=</span> base_size<span class="token comment"># 计算高宽比例</span>h_ratios <span class="token operator">=</span> torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>ratios<span class="token punctuation">)</span>w_ratios <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> h_ratios<span class="token comment"># base_size 乘上宽高比例乘上尺度，就可以得到 n 个 anchor 的原图尺度wh值</span>ws <span class="token operator">=</span> <span class="token punctuation">(</span>w <span class="token operator">*</span> w_ratios<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token operator">*</span> scales<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>hs <span class="token operator">=</span> <span class="token punctuation">(</span>h <span class="token operator">*</span> h_ratios<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token operator">*</span> scales<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment"># 得到 x1y1x2y2 格式的 base_anchor 坐标值</span>base_anchors <span class="token operator">=</span> <span class="token punctuation">[</span>    x_center <span class="token operator">-</span> <span class="token number">0.5</span> <span class="token operator">*</span> ws<span class="token punctuation">,</span> y_center <span class="token operator">-</span> <span class="token number">0.5</span> <span class="token operator">*</span> hs<span class="token punctuation">,</span> x_center <span class="token operator">+</span> <span class="token number">0.5</span> <span class="token operator">*</span> ws<span class="token punctuation">,</span>    y_center <span class="token operator">+</span> <span class="token number">0.5</span> <span class="token operator">*</span> hs<span class="token punctuation">]</span><span class="token comment"># 堆叠起来即可</span>base_anchors <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>base_anchors<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>(2) 利用输入特征图尺寸加上 base anchors，得到每个特征图位置的对于原图的 anchors</strong></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">feat_h<span class="token punctuation">,</span> feat_w <span class="token operator">=</span> featmap_size<span class="token comment"># 遍历特征图上所有位置，并且乘上 stride，从而变成原图坐标</span>shift_x <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> feat_w<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span> <span class="token operator">*</span> stride<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>shift_y <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> feat_h<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span> <span class="token operator">*</span> stride<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>shift_xx<span class="token punctuation">,</span> shift_yy <span class="token operator">=</span> self<span class="token punctuation">.</span>_meshgrid<span class="token punctuation">(</span>shift_x<span class="token punctuation">,</span> shift_y<span class="token punctuation">)</span>shifts <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>shift_xx<span class="token punctuation">,</span> shift_yy<span class="token punctuation">,</span> shift_xx<span class="token punctuation">,</span> shift_yy<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>shifts <span class="token operator">=</span> shifts<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>base_anchors<span class="token punctuation">)</span><span class="token comment"># (0,0) 位置的 base_anchor，假设原图上坐标 shifts，即可得到特征图上面每个点映射到原图坐标上的 anchor</span>all_anchors <span class="token operator">=</span> base_anchors<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">+</span> shifts<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>all_anchors <span class="token operator">=</span> all_anchors<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token keyword">return</span> all_anchors<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>简单来说就是：假设一共 m 个输出特征图</p><ul><li>遍历 m 个输出特征图，在每个特征图的 (0,0) 或者说原图的 (0,0) 坐标位置生成 <code>base_anchors</code>，注意 <code>base_anchors</code> 不是特征图尺度，而是原图尺度</li><li>遍历 m 个输出特征图中每个特征图上每个坐标点，将其映射到原图坐标上</li><li>原图坐标点加上 <code>base_anchors</code>，就可以得到特征图每个位置的对应到原图尺度的 anchor 列表，anchor 列表长度为 m</li></ul><h3 id="2-4-2-BBox-Assigner"><a href="#2-4-2-BBox-Assigner" class="headerlink" title="2.4.2 BBox Assigner"></a>2.4.2 BBox Assigner</h3><p>计算得到输出特征图上面每个点对应的原图 anchor 坐标后，就可以和 gt 信息计算每个 anchor 的正负样本属性，对应配置如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">assigner<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># 最大 IoU 原则分配器</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'MaxIoUAssigner'</span><span class="token punctuation">,</span>    <span class="token comment"># 正样本阈值</span>    pos_iou_thr<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>    <span class="token comment"># 负样本阈值</span>    neg_iou_thr<span class="token operator">=</span><span class="token number">0.4</span><span class="token punctuation">,</span>    <span class="token comment"># 正样本阈值下限</span>    min_pos_iou<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>    <span class="token comment"># 忽略 bboes 的阈值，-1表示不忽略</span>    ignore_iof_thr<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>仅从上面的描述可能比较难理解参数含义，通过下面的流程分析就比较容易理解每个参数含义了。MaxIoUAssigner 操作包括 4 个步骤：</p><p><strong>(1) 初始化所有 anchor 为忽略样本</strong></p><p>假设所有输出特征的所有 anchor 总数一共 n 个，对应某张图片中 gt bbox 个数为 m，首先初始化长度为 n 的 <code>assigned_gt_inds</code>，全部赋值为 -1，表示当前全部设置为忽略样本</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 1. assign -1 by default</span>assigned_gt_inds <span class="token operator">=</span> overlaps<span class="token punctuation">.</span>new_full<span class="token punctuation">(</span><span class="token punctuation">(</span>num_bboxes<span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">,</span>                                     <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>                                     dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><strong>(2) 计算背景样本</strong></p><p>将每个 anchor 和所有 gt bbox 计算 iou，找出最大 iou，如果该 iou 小于 <code>neg_iou_thr</code> 或者在背景样本阈值范围内，则该 anchor 对应索引位置的 <code>assigned_gt_inds</code> 设置为 0，表示是负样本(背景样本)</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">max_overlaps<span class="token punctuation">,</span> argmax_overlaps <span class="token operator">=</span> overlaps<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>gt_max_overlaps<span class="token punctuation">,</span> gt_argmax_overlaps <span class="token operator">=</span> overlaps<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment"># 2. assign negative: below</span><span class="token comment"># the negative inds are set to be 0</span><span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>neg_iou_thr<span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    assigned_gt_inds<span class="token punctuation">[</span><span class="token punctuation">(</span>max_overlaps <span class="token operator">&gt;=</span> <span class="token number">0</span><span class="token punctuation">)</span>                     <span class="token operator">&amp;</span> <span class="token punctuation">(</span>max_overlaps <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>neg_iou_thr<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>neg_iou_thr<span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">assert</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>neg_iou_thr<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span>    <span class="token comment"># 可以设置一个范围</span>    assigned_gt_inds<span class="token punctuation">[</span><span class="token punctuation">(</span>max_overlaps <span class="token operator">&gt;=</span> self<span class="token punctuation">.</span>neg_iou_thr<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>                     <span class="token operator">&amp;</span> <span class="token punctuation">(</span>max_overlaps <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>neg_iou_thr<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>(3) 计算高质量正样本</strong></p><p>将每个 anchor 和所有 gt bbox 计算 iou，找出最大 iou，如果其最大 iou 大于等于 <code>pos_iou_thr</code>，则设置该 anchor 对应所有的 <code>assigned_gt_inds</code> 设置为当前匹配 gt bbox 的编号 +1(后面会减掉 1)，表示该 anchor 负责预测该 gt bbox，且是高质量 anchor。之所以要加 1，是为了区分背景样本(背景样本的 <code>assigned_gt_inds</code> 值为 0)</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 3. assign positive: above positive IoU threshold</span>pos_inds <span class="token operator">=</span> max_overlaps <span class="token operator">&gt;=</span> self<span class="token punctuation">.</span>pos_iou_thrassigned_gt_inds<span class="token punctuation">[</span>pos_inds<span class="token punctuation">]</span> <span class="token operator">=</span> argmax_overlaps<span class="token punctuation">[</span>pos_inds<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>(4) 适当增加更多正样本</strong></p><p>在第三步计算高质量正样本中可能会出现某些 gt bbox 没有分配给任何一个 anchor (由于 iou 低于 <code>pos_iou_thr</code>)，导致该 gt bbox 不被认为是前景物体，此时可以通过 <code>self.match_low_quality=True</code> 配置进行补充正样本。</p><p>对于每个 gt bbox 需要找出和其最大 iou 的 anchor 索引，如果其 iou 大于 <code>min_pos_iou</code>，则将该 anchor 对应索引的 <code>assigned_gt_inds</code> 设置为正样本，表示该 anchor 负责预测对应的 gt bbox。通过本步骤，可以最大程度保证每个 gt bbox 都有相应的 anchor 负责预测，<strong>但是如果其最大 iou 值还是小于 <code>min_pos_iou</code>，则依然不被认为是前景物体</strong>。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> self<span class="token punctuation">.</span>match_low_quality<span class="token punctuation">:</span>    <span class="token comment"># Low-quality matching will overwirte the assigned_gt_inds assigned</span>    <span class="token comment"># in Step 3. Thus, the assigned gt might not be the best one for</span>    <span class="token comment"># prediction.</span>    <span class="token comment"># For example, if bbox A has 0.9 and 0.8 iou with GT bbox 1 &amp; 2,</span>    <span class="token comment"># bbox 1 will be assigned as the best target for bbox A in step 3.</span>    <span class="token comment"># However, if GT bbox 2's gt_argmax_overlaps = A, bbox A's</span>    <span class="token comment"># assigned_gt_inds will be overwritten to be bbox B.</span>    <span class="token comment"># This might be the reason that it is not used in ROI Heads.</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_gts<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> gt_max_overlaps<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">&gt;=</span> self<span class="token punctuation">.</span>min_pos_iou<span class="token punctuation">:</span>            <span class="token keyword">if</span> self<span class="token punctuation">.</span>gt_max_assign_all<span class="token punctuation">:</span>                <span class="token comment">#如果有多个相同最高 iou 的 anchor 和该 gt bbox 对应，则一并赋值</span>                max_iou_inds <span class="token operator">=</span> overlaps<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">==</span> gt_max_overlaps<span class="token punctuation">[</span>i<span class="token punctuation">]</span>                <span class="token comment"># 同样需要加1</span>                assigned_gt_inds<span class="token punctuation">[</span>max_iou_inds<span class="token punctuation">]</span> <span class="token operator">=</span> i <span class="token operator">+</span> <span class="token number">1</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                assigned_gt_inds<span class="token punctuation">[</span>gt_argmax_overlaps<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> i <span class="token operator">+</span> <span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>从这一步可以看出，3 和 4 有部分 anchor 重复分配了，即当某个 gt bbox 和 anchor 的最大 iou 大于等于 <code>pos_iou_thr</code>，那肯定大于 <code>min_pos_iou</code>，此时 3 和 4 步骤分配的同一个 anchor，并且从上面注释可以看出本步骤可能会引入低质量 anchor，是否需要开启本步骤需要根据不同算法来确定。</p><p>再次回到 RetinaNet 的 bbox assigner 配置：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">assigner<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># 最大 IoU 原则分配器</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'MaxIoUAssigner'</span><span class="token punctuation">,</span>    <span class="token comment"># 正样本阈值</span>    pos_iou_thr<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>    <span class="token comment"># 负样本阈值</span>    neg_iou_thr<span class="token operator">=</span><span class="token number">0.4</span><span class="token punctuation">,</span>    <span class="token comment"># 正样本阈值下限</span>    min_pos_iou<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>    <span class="token comment"># 忽略 bbox 的阈值，-1表示不忽略</span>    ignore_iof_thr<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>此时可以可以得到如下总结：</p><ul><li>如果 anchor 和所有 gt bbox 的最大 iou 值小于 0.4，那么该 anchor 就是背景样本</li><li>如果 anchor 和所有 gt bbox 的最大 iou 值大于等于 0.5，那么该 anchor 就是高质量正样本</li><li>如果 gt bbox 和所有 anchor 的最大 iou 值大于等于 0(可以看出每个 gt bbox 都一定有至少一个 anchor 匹配)，那么该 gt bbox 所对应的 anchor 也是正样本</li><li>其余样本全部为忽略样本即 anchor 和所有 gt bbox 的最大 iou 值处于 [0.4,0.5) 区间的 anchor 为忽略样本，不计算 loss</li></ul><h3 id="2-5-BBox-Encoder-Decoder"><a href="#2-5-BBox-Encoder-Decoder" class="headerlink" title="2.5 BBox Encoder Decoder"></a>2.5 BBox Encoder Decoder</h3><p>在 anchor-based 算法中，为了利用 anchor 信息进行更快更好的收敛，一般会对 head 输出的 bbox 分支 4 个值进行编解码操作，作用有两个：</p><ol><li>更好的平衡分类和回归分支 loss，以及平衡 bbox 四个预测值的 loss</li><li>训练过程中引入 anchor 信息，加快收敛</li></ol><p>RetinaNet 采用的编解码函数是主流的 DeltaXYWHBBoxCoder，其配置如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">bbox_coder<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'DeltaXYWHBBoxCoder'</span><span class="token punctuation">,</span>    target_means<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">,</span> <span class="token number">.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    target_stds<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><code>target_means</code> 和 <code>target_stds</code> 相当于对 bbox 回归的 4 个 <code>txtytwth</code> 进行变换。在不考虑 <code>target_means</code> 和 <code>target_stds</code> 情况下，其编码公式如下：</p><p><img src="https://www.zhihu.com/equation?tex=t%5E*_x=(x%5E*-x_a)/w_a,t%5E*_y=(y%5E*-x_a)/h_a" alt="[公式]"></p><p><img src="https://www.zhihu.com/equation?tex=t%5E*_w=log(w%5E*/w_a),t%5E*_h=log(h%5E*/h_a)" alt="[公式]"></p><p><img src="https://www.zhihu.com/equation?tex=x%5E*,y%5E*" alt="[公式]"> 是 gt bbox 的中心 xy 坐标， <img src="https://www.zhihu.com/equation?tex=w%5E*,h%5E*" alt="[公式]"> 是 gt bbox 的 wh 值， <img src="https://www.zhihu.com/equation?tex=x%5Ea,y%5Ea" alt="[公式]"> 是 anchor 的中心 xy 坐标， <img src="https://www.zhihu.com/equation?tex=w%5Ea,h%5Ea" alt="[公式]"> 是 anchor 的 wh 值， <img src="https://www.zhihu.com/equation?tex=t%5E*" alt="[公式]"> 是 bbox 分支输出的 4 个值对应 targets。可以看出 <img src="https://www.zhihu.com/equation?tex=t_x,t_y" alt="[公式]"> 预测值表示 gt bbox 中心相对于 anchor 中心点的偏移，并且通过除以 anchor 的 wh 进行归一化；而 <img src="https://www.zhihu.com/equation?tex=t_w,t_h" alt="[公式]"> 预测值表示 gt bbox 的 wh 除以 anchor 的 wh，然后取 log 非线性变换即可。</p><p>考虑编码过程 <code>target_means</code> 和 <code>target_stds</code> 情况下，核心代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">dx <span class="token operator">=</span> <span class="token punctuation">(</span>gx <span class="token operator">-</span> px<span class="token punctuation">)</span> <span class="token operator">/</span> pwdy <span class="token operator">=</span> <span class="token punctuation">(</span>gy <span class="token operator">-</span> py<span class="token punctuation">)</span> <span class="token operator">/</span> phdw <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>gw <span class="token operator">/</span> pw<span class="token punctuation">)</span>dh <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>gh <span class="token operator">/</span> ph<span class="token punctuation">)</span>deltas <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>dx<span class="token punctuation">,</span> dy<span class="token punctuation">,</span> dw<span class="token punctuation">,</span> dh<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment"># 最后减掉均值，处于标准差</span>means <span class="token operator">=</span> deltas<span class="token punctuation">.</span>new_tensor<span class="token punctuation">(</span>means<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>stds <span class="token operator">=</span> deltas<span class="token punctuation">.</span>new_tensor<span class="token punctuation">(</span>stds<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>deltas <span class="token operator">=</span> deltas<span class="token punctuation">.</span>sub_<span class="token punctuation">(</span>means<span class="token punctuation">)</span><span class="token punctuation">.</span>div_<span class="token punctuation">(</span>stds<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>解码过程是编码过程的反向，比较容易理解，其核心代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 先乘上 std，加上 mean</span>means <span class="token operator">=</span> deltas<span class="token punctuation">.</span>new_tensor<span class="token punctuation">(</span>means<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> deltas<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">)</span>stds <span class="token operator">=</span> deltas<span class="token punctuation">.</span>new_tensor<span class="token punctuation">(</span>stds<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> deltas<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">)</span>denorm_deltas <span class="token operator">=</span> deltas <span class="token operator">*</span> stds <span class="token operator">+</span> meansdx <span class="token operator">=</span> denorm_deltas<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span>dy <span class="token operator">=</span> denorm_deltas<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span>dw <span class="token operator">=</span> denorm_deltas<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span>dh <span class="token operator">=</span> denorm_deltas<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token comment"># wh 解码</span>gw <span class="token operator">=</span> pw <span class="token operator">*</span> dw<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">)</span>gh <span class="token operator">=</span> ph <span class="token operator">*</span> dh<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 中心点 xy 解码</span>gx <span class="token operator">=</span> px <span class="token operator">+</span> pw <span class="token operator">*</span> dxgy <span class="token operator">=</span> py <span class="token operator">+</span> ph <span class="token operator">*</span> dy<span class="token comment"># 得到 x1y1x2y2 的 gt bbox 预测坐标</span>x1 <span class="token operator">=</span> gx <span class="token operator">-</span> gw <span class="token operator">*</span> <span class="token number">0.5</span>y1 <span class="token operator">=</span> gy <span class="token operator">-</span> gh <span class="token operator">*</span> <span class="token number">0.5</span>x2 <span class="token operator">=</span> gx <span class="token operator">+</span> gw <span class="token operator">*</span> <span class="token number">0.5</span>y2 <span class="token operator">=</span> gy <span class="token operator">+</span> gh <span class="token operator">*</span> <span class="token number">0.5</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-6-Loss"><a href="#2-6-Loss" class="headerlink" title="2.6 Loss"></a>2.6 Loss</h3><p>前面说过 RetinaNet 一个非常大的亮点就是提出了 Focal Loss。</p><p><img src="https://pic4.zhimg.com/80/v2-7bf35b65e528d7ea226281211beef7ef_720w.jpg" alt="img"></p><p>Focal Loss 属于 CE Loss 的动态加权版本，其可以根据样本的难易程度(预测值和 label 的差距可以反映)对每个样本单独加权，易学习样本权重比较低，难样本权重比较高。因为在前面的 bbox assigner 环节，大部分样本都是背景易学习样本，虽然其本身 loss 比较小，但是由于数目众多最终会主导梯度，从而得到次优模型，而 Focal Loss 通过指数效应把大量易学习样本的权重大大降低，从而避免上述问题。</p><p>完整的 Focal Loss 为： <img src="https://www.zhihu.com/equation?tex=loss_%7Bfocal%7D+=-%5Calpha_t(1-p_t)%5E%5Cgamma+log(p_t)" alt="[公式]"></p><p><img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> 属于正负样本的加权参数，值越大，正样本的权重越大， <img src="https://www.zhihu.com/equation?tex=%5Cbeta" alt="[公式]"> 有focal效应，可以控制难易样本权重，值越大，对分类错误样本梯度越大(难样本权重大)，focal 效应越大，这个参数非常关键。</p><p>代码实现方面也比较简单，MMDetection 提供了 py 和 cuda 版本，py 版本如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">pred_sigmoid <span class="token operator">=</span> pred<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># one-hot 格式</span>target <span class="token operator">=</span> target<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>pred<span class="token punctuation">)</span>pt <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> pred_sigmoid<span class="token punctuation">)</span> <span class="token operator">*</span> target <span class="token operator">+</span> pred_sigmoid <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> target<span class="token punctuation">)</span>focal_weight <span class="token operator">=</span> <span class="token punctuation">(</span>alpha <span class="token operator">*</span> target <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> alpha<span class="token punctuation">)</span> <span class="token operator">*</span>            <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> target<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> pt<span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span>gamma<span class="token punctuation">)</span>loss <span class="token operator">=</span> F<span class="token punctuation">.</span>binary_cross_entropy_with_logits<span class="token punctuation">(</span>        pred<span class="token punctuation">,</span> target<span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span> <span class="token operator">*</span> focal_weightloss <span class="token operator">=</span> weight_reduce_loss<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> weight<span class="token punctuation">,</span> reduction<span class="token punctuation">,</span> avg_factor<span class="token punctuation">)</span><span class="token keyword">return</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>RetinaNet 的完整 loss 配置如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 分类 loss</span>loss_cls<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'FocalLoss'</span><span class="token punctuation">,</span>    use_sigmoid<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    gamma<span class="token operator">=</span><span class="token number">2.0</span><span class="token punctuation">,</span>    alpha<span class="token operator">=</span><span class="token number">0.25</span><span class="token punctuation">,</span>    loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token comment"># 回归 loss</span>loss_bbox<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'L1Loss'</span><span class="token punctuation">,</span> loss_weight<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-7-测试流程"><a href="#2-7-测试流程" class="headerlink" title="2.7 测试流程"></a>2.7 测试流程</h3><p>对应配置如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">test_cfg <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token comment"># nms 前每个输出层最多保留1000个预测框</span>    nms_pre<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span>    <span class="token comment"># 过滤掉的最小 bbox 尺寸</span>    min_bbox_size<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>    <span class="token comment"># 分值阈值</span>    score_thr<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">,</span>    <span class="token comment"># nms 方法和 nms 阈值</span>    nms<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'nms'</span><span class="token punctuation">,</span> iou_threshold<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># 最终输出的每张图片最多 bbox 个数</span>    max_per_img<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>测试阶段流程如下：</p><p>(1) 对 5 个 head 输出特征图结果进行遍历，先按照预测分值排序，保留前 nms_pre 个预测结果</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> nms_pre <span class="token operator">&gt;</span> <span class="token number">0</span> <span class="token keyword">and</span> scores<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">&gt;</span> nms_pre<span class="token punctuation">:</span>    <span class="token comment"># Get maximum scores for foreground classes.</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_sigmoid_cls<span class="token punctuation">:</span>        max_scores<span class="token punctuation">,</span> _ <span class="token operator">=</span> scores<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token comment"># remind that we set FG labels to [0, num_class-1]</span>        <span class="token comment"># since mmdet v2.0</span>        <span class="token comment"># BG cat_id: num_class</span>        max_scores<span class="token punctuation">,</span> _ <span class="token operator">=</span> scores<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    _<span class="token punctuation">,</span> topk_inds <span class="token operator">=</span> max_scores<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>nms_pre<span class="token punctuation">)</span>    anchors <span class="token operator">=</span> anchors<span class="token punctuation">[</span>topk_inds<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>    bbox_pred <span class="token operator">=</span> bbox_pred<span class="token punctuation">[</span>topk_inds<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>    scores <span class="token operator">=</span> scores<span class="token punctuation">[</span>topk_inds<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>(2) 对剩下的 bbox 进行解码</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">bboxes <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_coder<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>    anchors<span class="token punctuation">,</span> bbox_pred<span class="token punctuation">,</span> max_shape<span class="token operator">=</span>img_shape<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>(3) 还原到原图尺度</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">mlvl_bboxes <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>mlvl_bboxes<span class="token punctuation">)</span><span class="token keyword">if</span> rescale<span class="token punctuation">:</span>    mlvl_bboxes <span class="token operator">/=</span> mlvl_bboxes<span class="token punctuation">.</span>new_tensor<span class="token punctuation">(</span>scale_factor<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>(4) 用 <code>score_thr</code> 阈值对所有结果进行过滤，然后将保留框进行 nms，最终输出框最大为 <code>max_per_img</code> 个</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">valid_mask <span class="token operator">=</span> scores <span class="token operator">&gt;</span> score_thrinds <span class="token operator">=</span> valid_mask<span class="token punctuation">.</span>nonzero<span class="token punctuation">(</span>as_tuple<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>bboxes<span class="token punctuation">,</span> scores<span class="token punctuation">,</span> labels <span class="token operator">=</span> bboxes<span class="token punctuation">[</span>inds<span class="token punctuation">]</span><span class="token punctuation">,</span> scores<span class="token punctuation">[</span>inds<span class="token punctuation">]</span><span class="token punctuation">,</span> labels<span class="token punctuation">[</span>inds<span class="token punctuation">]</span>dets<span class="token punctuation">,</span> keep <span class="token operator">=</span> batched_nms<span class="token punctuation">(</span>bboxes<span class="token punctuation">,</span> scores<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> nms_cfg<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h2><p>本文结合源码深入详细的分析了 MMDetection 中的 RetinaNet 模型，不仅如此，本文还将所涉及到的所有配置参数都进行了仔细分析，希望读者通过阅读本文可以了解到：</p><ul><li>RetinaNet 算法的整个实现过程和细节</li><li>MMDetection 中算法的配置参数具体含义和用法</li><li>对 MMDetection 有更加清晰深入的理解</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>轻松掌握 MMDetection 整体构建流程(二)</title>
      <link href="/2022/04/05/mmdetection-xue-xi-bi-ji-2-zheng-ti-gou-jian-liu-cheng-dai-ma/"/>
      <url>/2022/04/05/mmdetection-xue-xi-bi-ji-2-zheng-ti-gou-jian-liu-cheng-dai-ma/</url>
      
        <content type="html"><![CDATA[<h1 id="轻松掌握-MMDetection-整体构建流程-二"><a href="#轻松掌握-MMDetection-整体构建流程-二" class="headerlink" title="轻松掌握 MMDetection 整体构建流程(二)"></a>轻松掌握 MMDetection 整体构建流程(二)</h1><h2 id="0-内容概览"><a href="#0-内容概览" class="headerlink" title="0 内容概览"></a>0 内容概览</h2><p>上一节中，重点分析了 MMDetection 框架中 Model 整体构建流程，但仅对 Model 算法组件方面进行深入分析，并未涉及整个框架训练和测试流程。</p><p>本文核心内容是<strong>按照抽象到具体方式，从多个层次进行训练和测试流程深入解析</strong>，<strong>从最抽象层讲起，到最后核心代码实现</strong>，帮助理解 MMDetection 开源框架整体构建细节。</p><h2 id="1-第一层整体抽象"><a href="#1-第一层整体抽象" class="headerlink" title="1 第一层整体抽象"></a>1 第一层整体抽象</h2><p><img src="https://pic2.zhimg.com/80/v2-2463639f7e39afd273fdeccbfa530d49_720w.jpg" alt="img"></p><ul><li>上图为 MMDetection 框架整体训练和测试抽象流程图。按照数据流过程，<strong>训练流程可以简单总结为</strong>：</li></ul><ol><li>给定任何一个数据集，首先需要<strong>构建 Dataset 类</strong>，用于迭代输出数据。</li><li>在迭代输出数据的时候需要通过数据 Pipeline <strong>对数据进行各种处理</strong>，最典型的处理流是训练中的<strong>数据增强</strong>操作，测试中的数据预处理等等。</li><li><strong>通过 Sampler 采样器可以控制 Dataset 输出的数据顺序</strong>，最常用的是随机采样器 RandomSampler。由于 Dataset 中输出的图片大小不一样，<strong>为了尽可能减少后续组成 batch 时 pad 的像素个数，MMDetection 引入了分组采样器 GroupSampler</strong> 和 DistributedGroupSampler，相当于在 RandomSampler 基础上额外新增了根据图片宽高比进行 group 功能。</li><li><strong>将 Sampler 和 Dataset 都输入给 DataLoader</strong>，然后通过 DataLoader 输出已组成 batch 的数据，作为 Model 的输入。</li><li>对于任何一个 Model，<strong>为了方便处理数据流以及分布式需求</strong>，MMDetection 引入了两个 Model 的上层封装：<strong>单机版本 MMDataParallel</strong>、<strong>分布式（单机多卡或多机多卡）版本 MMDistributedDataParallel</strong>。</li><li>Model 运行后会输出 <strong>loss 以及其他一些信息，会通过 logger 进行保存或者可视化</strong>。</li><li>为了更好地解耦， 方便地获取各个组件之间依赖和灵活扩展，MMDetection <strong>引入了 Runner 类进行全生命周期管理</strong>，并且通过 <strong>Hook 方便的获取</strong>、修改和拦截任何生命周期数据流，扩展非常便捷。</li></ol><ul><li>而<strong>测试流程</strong>就比较简单了，直接对 DataLoader 输出的数据进行前向推理即可，还原到最终原图尺度过程也是在 Model 中完成。</li></ul><p>以上就是 MMDetection 框架整体训练和测试抽象流程，上图不仅仅反映了训练和测试数据流，而且还包括了模块和模块之间的调用关系。<strong>对于训练而言，最核心部分应该是 Runner，理解了 Runner 的运行流程，也就理解了整个 MMDetection 数据流。</strong></p><h2 id="2-第二层模块抽象"><a href="#2-第二层模块抽象" class="headerlink" title="2 第二层模块抽象"></a>2 第二层模块抽象</h2><p>在总体把握了整个 MMDetection 框架训练和测试流程后，下个层次是每个模块内部抽象流程，主要包括 <strong>Pipeline、DataParallel、Model、Runner 和 Hooks。</strong></p><h3 id="2-1-Pipeline"><a href="#2-1-Pipeline" class="headerlink" title="2.1 Pipeline"></a>2.1 Pipeline</h3><p>Pipeline 实际上<strong>由一系列按照插入顺序运行的数据处理模块组成</strong>，每个模块完成某个特定功能，例如 Resize，因为其流式顺序运行特性，故叫做 Pipeline。</p><p><img src="https://pic3.zhimg.com/80/v2-d7eb7e24335613da3da22da4ea93e132_720w.jpg" alt="img"></p><p>上图是一个非常典型的训练流程 Pipeline，<strong>每个类都接收字典输入，输出也是字典，顺序执行</strong>，其中绿色表示该类运行后新增字段，橙色表示对该字段可能会进行修改。如果进一步细分的话，不同算法的 Pipeline 都可以划分为如下部分：</p><ul><li><strong>图片和标签加载</strong>，通常用的类是 LoadImageFromFile 和 LoadAnnotations</li><li><strong>数据前处理</strong>，例如统一 Resize</li><li><strong>数据增强</strong>，典型的例如各种图片几何变换等，这部分是训练流程特有，测试阶段一般不采用(多尺度测试采用其他实现方式)</li><li><strong>数据收集</strong>，例如 Collect</li></ul><p>在 MMDetection 框架中，图片和标签加载和数据后处理流程一般是固定的，<strong>用户主要可能修改的是数据增强步骤</strong>，目前已经接入了第三方增强库 Albumentations，可以按照示例代码轻松构建属于你自己的数据增强 Pipeline。</p><p><strong>在构建自己的 Pipeline 时候一定要仔细检查你修改或者新增的字典 key 和 value，因为一旦你错误地覆盖或者修改原先字典里面的内容，代码也可能不会报错，如果出现 bug，则比较难排查</strong>。</p><h3 id="2-2-DataParallel-和-Model"><a href="#2-2-DataParallel-和-Model" class="headerlink" title="2.2 DataParallel 和 Model"></a>2.2 DataParallel 和 Model</h3><p><strong>在 MMDetection 中 DataLoader 输出的内容不是 pytorch 能处理的标准格式</strong>，还包括了 DataContainer 对象，该对象的作用是包装不同类型的对象使之能按需组成 batch。在目标检测中，每张图片 gt bbox 个数是不一样的，如果想组成 batch tensor，要么你设置最大长度，要么你自己想办法组成 batch。而考虑到内存和效率，<strong>MMDetection 通过引入 DataContainer</strong> 模块来解决上述问题，但是随之带来的问题是 pytorch 无法解析 DataContainer 对象，故需要在 MMDetection 中自行处理。</p><p>解决办法其实非常多，<strong>MMDetection 选择了一种比较优雅的实现方式：MMDataParallel 和 MMDistributedDataParallel。</strong>具体来说，这两个类相比 PyTorch 自带的 DataParallel 和 DistributedDataParallel 区别是：</p><ul><li>可以处理 DataContainer 对象</li><li>额外实现了 <code>train_step()</code> 和 <code>val_step()</code> 两个函数，可以被 Runner 调用</li></ul><p>关于这两个类的具体实现后面会描述。</p><p>而 Model 部分内容就是第一篇解读文章所讲的，具体如下：</p><p><img src="https://pic4.zhimg.com/80/v2-0c8f69636320fb40d8a8cd994296bf87_720w.jpg" alt="img"></p><h3 id="2-3-Runner-和-Hooks"><a href="#2-3-Runner-和-Hooks" class="headerlink" title="2.3 Runner 和 Hooks"></a>2.3 Runner 和 Hooks</h3><p>对于任何一个目标检测算法，都需要包括优化器、学习率设置、权重保存等等组件才能构成完整训练流程，而这些组件是通用的。为了方便 OpenMMLab 体系下的所有框架复用，<strong>在 MMCV 框架中引入了 Runner 类来统一管理训练和验证流程</strong>，并且<strong>通过 Hooks 机制以一种非常灵活、解耦的方式来实现丰富扩展功能</strong>。</p><p>关于 Runner 和 Hooks 详细解读会发布在 MMCV 系列解读文章中，简单来说 <strong>Runner 封装了 OpenMMLab 体系下各个框架的训练和验证详细流程</strong>，其负责管理训练和验证过程中的整个生命周期，<strong>通过预定义回调函数，用户可以插入定制化 Hook ，从而实现各种各样的需求</strong>。下面列出了在 MMDetection 几个非常重要的 hook 以及其作用的生命周期：</p><p><img src="https://pic4.zhimg.com/80/v2-5d614997aa85e1b841457094b7bc0cbb_720w.jpg" alt="img"></p><p>例如 CheckpointHook 在每个训练 epoch 完成后会被调用，从而实现保存权重功能。用户也可以将自己定制实现的 Hook 采用上述方式绘制，对理解整个流程或许有帮助。</p><h2 id="3-第三层代码抽象"><a href="#3-第三层代码抽象" class="headerlink" title="3 第三层代码抽象"></a>3 第三层代码抽象</h2><p>前面两层抽象分析流程，基本上把整个 MMDetection 的训练和测试流程分析完了，下面从具体代码层面进行抽象分析。</p><h3 id="3-1-训练和测试整体代码抽象流程"><a href="#3-1-训练和测试整体代码抽象流程" class="headerlink" title="3.1 训练和测试整体代码抽象流程"></a>3.1 训练和测试整体代码抽象流程</h3><p><img src="https://pic4.zhimg.com/80/v2-b03d43ed4b3dc4c02e68712e57023cff_720w.jpg" alt="img"></p><p>上图为训练和验证的和具体代码相关的整体抽象流程，对应到代码上，其核心代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#=================== tools/train.py ==================</span><span class="token comment"># 1.初始化配置</span>cfg <span class="token operator">=</span> Config<span class="token punctuation">.</span>fromfile<span class="token punctuation">(</span>args<span class="token punctuation">.</span>config<span class="token punctuation">)</span><span class="token comment"># 2.判断是否为分布式训练模式</span><span class="token comment"># 3.初始化 logger</span>logger <span class="token operator">=</span> get_root_logger<span class="token punctuation">(</span>log_file<span class="token operator">=</span>log_file<span class="token punctuation">,</span> log_level<span class="token operator">=</span>cfg<span class="token punctuation">.</span>log_level<span class="token punctuation">)</span><span class="token comment"># 4.收集运行环境并且打印，方便排查硬件和软件相关问题</span>env_info_dict <span class="token operator">=</span> collect_env<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 5.初始化 model</span>model <span class="token operator">=</span> build_detector<span class="token punctuation">(</span>cfg<span class="token punctuation">.</span>model<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token comment"># 6.初始化 datasets</span><span class="token comment">#=================== mmdet/apis/train.py ==================</span><span class="token comment"># 1.初始化 data_loaders ，内部会初始化 GroupSampler</span>data_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token comment"># 2.基于是否使用分布式训练，初始化对应的 DataParallel</span><span class="token keyword">if</span> distributed<span class="token punctuation">:</span>  model <span class="token operator">=</span> MMDistributedDataParallel<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>  model <span class="token operator">=</span> MMDataParallel<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token comment"># 3.初始化 runner</span>runner <span class="token operator">=</span> EpochBasedRunner<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token comment"># 4.注册必备 hook</span>runner<span class="token punctuation">.</span>register_training_hooks<span class="token punctuation">(</span>cfg<span class="token punctuation">.</span>lr_config<span class="token punctuation">,</span> optimizer_config<span class="token punctuation">,</span>                               cfg<span class="token punctuation">.</span>checkpoint_config<span class="token punctuation">,</span> cfg<span class="token punctuation">.</span>log_config<span class="token punctuation">,</span>                               cfg<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'momentum_config'</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 5.如果需要 val，则还需要注册 EvalHook           </span>runner<span class="token punctuation">.</span>register_hook<span class="token punctuation">(</span>eval_hook<span class="token punctuation">(</span>val_dataloader<span class="token punctuation">,</span> <span class="token operator">**</span>eval_cfg<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 6.注册用户自定义 hook</span>runner<span class="token punctuation">.</span>register_hook<span class="token punctuation">(</span>hook<span class="token punctuation">,</span> priority<span class="token operator">=</span>priority<span class="token punctuation">)</span><span class="token comment"># 7.权重恢复和加载</span><span class="token keyword">if</span> cfg<span class="token punctuation">.</span>resume_from<span class="token punctuation">:</span>    runner<span class="token punctuation">.</span>resume<span class="token punctuation">(</span>cfg<span class="token punctuation">.</span>resume_from<span class="token punctuation">)</span><span class="token keyword">elif</span> cfg<span class="token punctuation">.</span>load_from<span class="token punctuation">:</span>    runner<span class="token punctuation">.</span>load_checkpoint<span class="token punctuation">(</span>cfg<span class="token punctuation">.</span>load_from<span class="token punctuation">)</span><span class="token comment"># 8.运行，开始训练</span>runner<span class="token punctuation">.</span>run<span class="token punctuation">(</span>data_loaders<span class="token punctuation">,</span> cfg<span class="token punctuation">.</span>workflow<span class="token punctuation">,</span> cfg<span class="token punctuation">.</span>total_epochs<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面的流程比较简单，<strong>一般大家比较难以理解的是 <code>runner.run</code> 内部逻辑</strong>，下小节进行详细分析，而对于测试逻辑由于比较简单，就不详细描述了，<strong>简单来说测试流程下不需要 runner，直接加载训练好的权重，然后进行 model 推理</strong>即可。</p><h2 id="3-2-Runner-训练和验证代码抽象"><a href="#3-2-Runner-训练和验证代码抽象" class="headerlink" title="3.2 Runner 训练和验证代码抽象"></a>3.2 Runner 训练和验证代码抽象</h2><p>runner 对象内部的 run 方式是一个通用方法，可以运行任何 workflow，目前常用的主要是 train 和 val。</p><ul><li>当配置为：workflow = [(‘train’, 1)]，表示仅仅进行 train workflow，也就是迭代训练</li><li>当配置为：workflow = [(‘train’, n),(‘val’, 1)]，表示先进行 n 个 epoch 的训练，然后再进行1个 epoch 的验证，然后循环往复,如果写成 [(‘val’, 1),(‘train’, n)] 表示先进行验证，然后才开始训练</li></ul><p>当进入对应的 workflow，则会调用 runner 里面的 train() 或者 val()，表示进行一次 epoch 迭代。其代码也非常简单，如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data_loader<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>mode <span class="token operator">=</span> <span class="token string">'train'</span>    self<span class="token punctuation">.</span>data_loader <span class="token operator">=</span> data_loader    self<span class="token punctuation">.</span>call_hook<span class="token punctuation">(</span><span class="token string">'before_train_epoch'</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> data_batch <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>call_hook<span class="token punctuation">(</span><span class="token string">'before_train_iter'</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>run_iter<span class="token punctuation">(</span>data_batch<span class="token punctuation">,</span> train_mode<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>call_hook<span class="token punctuation">(</span><span class="token string">'after_train_iter'</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>call_hook<span class="token punctuation">(</span><span class="token string">'after_train_epoch'</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">val</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data_loader<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    self<span class="token punctuation">.</span>model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>mode <span class="token operator">=</span> <span class="token string">'val'</span>    self<span class="token punctuation">.</span>data_loader <span class="token operator">=</span> data_loader    self<span class="token punctuation">.</span>call_hook<span class="token punctuation">(</span><span class="token string">'before_val_epoch'</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> data_batch <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>call_hook<span class="token punctuation">(</span><span class="token string">'before_val_iter'</span><span class="token punctuation">)</span>        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>run_iter<span class="token punctuation">(</span>data_batch<span class="token punctuation">,</span> train_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>call_hook<span class="token punctuation">(</span><span class="token string">'after_val_iter'</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>call_hook<span class="token punctuation">(</span><span class="token string">'after_val_epoch'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>核心函数实际上是 self.run_iter()，如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">run_iter</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data_batch<span class="token punctuation">,</span> train_mode<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> train_mode<span class="token punctuation">:</span>        <span class="token comment"># 对于每次迭代，最终是调用如下函数</span>        outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>train_step<span class="token punctuation">(</span>data_batch<span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token comment"># 对于每次迭代，最终是调用如下函数</span>        outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>val_step<span class="token punctuation">(</span>data_batch<span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> <span class="token string">'log_vars'</span> <span class="token keyword">in</span> outputs<span class="token punctuation">:</span>        self<span class="token punctuation">.</span>log_buffer<span class="token punctuation">.</span>update<span class="token punctuation">(</span>outputs<span class="token punctuation">[</span><span class="token string">'log_vars'</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>outputs <span class="token operator">=</span> outputs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上述 self.call_hook() 表示在不同生命周期调用所有已经注册进去的 hook，而字符串参数表示对应的生命周期。以 OptimizerHook 为例，其执行反向传播、梯度裁剪和参数更新等核心训练功能：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token decorator annotation punctuation">@HOOKS<span class="token punctuation">.</span>register_module</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">OptimizerHook</span><span class="token punctuation">(</span>Hook<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> grad_clip<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>grad_clip <span class="token operator">=</span> grad_clip    <span class="token keyword">def</span> <span class="token function">after_train_iter</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> runner<span class="token punctuation">)</span><span class="token punctuation">:</span>        runner<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        runner<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>grad_clip <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            grad_norm <span class="token operator">=</span> self<span class="token punctuation">.</span>clip_grads<span class="token punctuation">(</span>runner<span class="token punctuation">.</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        runner<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以发现 OptimizerHook 注册到的生命周期是 after_train_iter，故在每次 train() 里面运行到 <code>self.call_hook('after_val_iter')</code> 时候就会被调用，其他 hook 也是同样运行逻辑。</p><h3 id="3-3-Model-训练和测试代码抽象"><a href="#3-3-Model-训练和测试代码抽象" class="headerlink" title="3.3 Model 训练和测试代码抽象"></a>3.3 Model 训练和测试代码抽象</h3><p>前面说个，训练和验证的时候实际上调用了 model 内部的 <code>train_step</code> 和 <code>val_step</code> 函数，<strong>理解了两个函数调用流程就理解了 MMDetection 训练和测试流程</strong>。</p><p>注意，由于 model 对象会被 DataParallel 类包裹，故实际上上此时的 model，是指的 MMDataParallel 或者 MMDistributedDataParallel。以非分布式 train_step 流程为例，其内部完成调用流程图示如下：</p><p><img src="https://pic4.zhimg.com/80/v2-0d17b53f68286931803bf9d1dca10467_720w.jpg" alt="img"></p><h3 id="3-3-1-train-或者-val-流程"><a href="#3-3-1-train-或者-val-流程" class="headerlink" title="3.3.1 train 或者 val 流程"></a>3.3.1 train 或者 val 流程</h3><h4 id="1-调用-runner-中的-train-step-或者-val-step"><a href="#1-调用-runner-中的-train-step-或者-val-step" class="headerlink" title="(1) 调用 runner 中的 train_step 或者 val_step"></a><strong>(1) 调用 runner 中的 <code>train_step</code> 或者 <code>val_step</code></strong></h4><p>在 runner 中调用 <code>train_step</code> 或者 <code>val_step</code>，代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#=================== mmcv/runner/epoch_based_runner.py ==================</span><span class="token keyword">if</span> train_mode<span class="token punctuation">:</span>    outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>train_step<span class="token punctuation">(</span>data_batch<span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>    outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>val_step<span class="token punctuation">(</span>data_batch<span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>实际上，首先会调用 DataParallel 中的 <code>train_step</code> 或者 <code>val_step</code> ，其具体调用流程为：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 非分布式训练</span><span class="token comment">#=================== mmcv/parallel/data_parallel.py/MMDataParallel ==================</span><span class="token keyword">def</span> <span class="token function">train_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>inputs<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>device_ids<span class="token punctuation">:</span>        inputs<span class="token punctuation">,</span> kwargs <span class="token operator">=</span> self<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> kwargs<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment"># 此时才是调用 model 本身的 train_step</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>train_step<span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>    <span class="token comment"># 单 gpu 模式</span>    inputs<span class="token punctuation">,</span> kwargs <span class="token operator">=</span> self<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> kwargs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>device_ids<span class="token punctuation">)</span>    <span class="token comment"># 此时才是调用 model 本身的 train_step</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>train_step<span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># val_step 也是的一样逻辑</span><span class="token keyword">def</span> <span class="token function">val_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>inputs<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    inputs<span class="token punctuation">,</span> kwargs <span class="token operator">=</span> self<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> kwargs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>device_ids<span class="token punctuation">)</span>    <span class="token comment"># 此时才是调用 model 本身的 val_step</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>val_step<span class="token punctuation">(</span><span class="token operator">*</span>inputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以发现，在调用 model 本身的 train_step 前，需要额外调用 <strong>scatter 函数</strong>，前面说过该函数的作用是<strong>处理 DataContainer 格式数据，使其能够组成 batch</strong>，否则程序会报错。</p><p>如果是分布式训练，则调用的实际上是 <code>mmcv/parallel/distributed.py/MMDistributedDataParallel</code>，最终调用的依然是 model 本身的 <code>train_step</code> 或者 <code>val_step</code>。</p><h4 id="2-调用-model-中的-train-step-或者-val-step"><a href="#2-调用-model-中的-train-step-或者-val-step" class="headerlink" title="(2) 调用 model 中的 train_step 或者 val_step"></a><strong>(2) 调用 model 中的 <code>train_step</code> 或者 <code>val_step</code></strong></h4><p>其核心代码如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#=================== mmdet/models/detectors/base.py/BaseDetector ==================</span><span class="token keyword">def</span> <span class="token function">train_step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data<span class="token punctuation">,</span> optimizer<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 调用本类自身的 forward 方法</span>    losses <span class="token operator">=</span> self<span class="token punctuation">(</span><span class="token operator">**</span>data<span class="token punctuation">)</span>    <span class="token comment"># 解析 loss</span>    loss<span class="token punctuation">,</span> log_vars <span class="token operator">=</span> self<span class="token punctuation">.</span>_parse_losses<span class="token punctuation">(</span>losses<span class="token punctuation">)</span>    <span class="token comment"># 返回字典对象</span>    outputs <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>        loss<span class="token operator">=</span>loss<span class="token punctuation">,</span> log_vars<span class="token operator">=</span>log_vars<span class="token punctuation">,</span> num_samples<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token string">'img_metas'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> outputs<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">,</span> img_metas<span class="token punctuation">,</span> return_loss<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> return_loss<span class="token punctuation">:</span>        <span class="token comment"># 训练模式</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>forward_train<span class="token punctuation">(</span>img<span class="token punctuation">,</span> img_metas<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token comment"># 测试模式</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>forward_test<span class="token punctuation">(</span>img<span class="token punctuation">,</span> img_metas<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>forward_train</code> 和 <code>forward_test</code> 需要在不同的算法子类中实现，输出是 Loss 或者 预测结果。</p><p><strong>(3) 调用子类中的 <code>forward_train</code> 方法</strong></p><p>目前提供了两个具体子类，<code>TwoStageDetector</code> 和 <code>SingleStageDetector</code> ，用于实现 two-stage 和 single-stage 算法。</p><p>对于 <code>TwoStageDetector</code> 而言，其核心逻辑是：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#============= mmdet/models/detectors/two_stage.py/TwoStageDetector ============</span><span class="token keyword">def</span> <span class="token function">forward_train</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 先进行 backbone+neck 的特征提取</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>extract_feat<span class="token punctuation">(</span>img<span class="token punctuation">)</span>    losses <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># RPN forward and loss</span>    <span class="token keyword">if</span> self<span class="token punctuation">.</span>with_rpn<span class="token punctuation">:</span>        <span class="token comment"># 训练 RPN</span>        proposal_cfg <span class="token operator">=</span> self<span class="token punctuation">.</span>train_cfg<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'rpn_proposal'</span><span class="token punctuation">,</span>                                          self<span class="token punctuation">.</span>test_cfg<span class="token punctuation">.</span>rpn<span class="token punctuation">)</span>        <span class="token comment"># 主要是调用 rpn_head 内部的 forward_train 方法</span>        rpn_losses<span class="token punctuation">,</span> proposal_list <span class="token operator">=</span> self<span class="token punctuation">.</span>rpn_head<span class="token punctuation">.</span>forward_train<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>        losses<span class="token punctuation">.</span>update<span class="token punctuation">(</span>rpn_losses<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        proposal_list <span class="token operator">=</span> proposals    <span class="token comment"># 第二阶段，主要是调用 roi_head 内部的 forward_train 方法</span>    roi_losses <span class="token operator">=</span> self<span class="token punctuation">.</span>roi_head<span class="token punctuation">.</span>forward_train<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    losses<span class="token punctuation">.</span>update<span class="token punctuation">(</span>roi_losses<span class="token punctuation">)</span>    <span class="token keyword">return</span> losses<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对于 <code>SingleStageDetector</code> 而言，其核心逻辑是：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#============= mmdet/models/detectors/single_stage.py/SingleStageDetector ============</span><span class="token keyword">def</span> <span class="token function">forward_train</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token builtin">super</span><span class="token punctuation">(</span>SingleStageDetector<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>forward_train<span class="token punctuation">(</span>img<span class="token punctuation">,</span> img_metas<span class="token punctuation">)</span>    <span class="token comment"># 先进行 backbone+neck 的特征提取</span>    x <span class="token operator">=</span> self<span class="token punctuation">.</span>extract_feat<span class="token punctuation">(</span>img<span class="token punctuation">)</span>    <span class="token comment"># 主要是调用 bbox_head 内部的 forward_train 方法</span>    losses <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">.</span>forward_train<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> losses<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果再往里分析，那就到各个 Head 模块的训练环节了，这部分内容请读者自行分析，应该不难。</p><h3 id="3-3-2-test-流程"><a href="#3-3-2-test-流程" class="headerlink" title="3.3.2 test 流程"></a>3.3.2 test 流程</h3><p>由于没有 runner 对象，测试流程简单很多，下面简要概述：</p><ol><li>调用 MMDataParallel 或 MMDistributedDataParallel 中的 <code>forward</code> 方法</li><li>调用 base.py 中的 <code>forward</code> 方法</li><li>调用 base.py 中的 <code>self.forward_test</code> 方法</li><li>如果是单尺度测试，则会调用 TwoStageDetector 或 SingleStageDetector 中的 <code>simple_test</code> 方法，如果是多尺度测试，则调用 <code>aug_test</code> 方法</li><li>最终调用的是每个具体算法 Head 模块的 <code>simple_test</code> 或者 <code>aug_test</code> 方法</li></ol><h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h2><p>本文基于第一篇解读文章，详细地从三个层面全面解读了 MMDetection 框架，希望读者读完本文，能够对 MMDetection 框架设计思想、组件间关系和整体代码实现流程了然于心。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>轻松掌握 MMDetection 整体构建流程(一)</title>
      <link href="/2022/04/02/mmdetection-xue-xi-bi-ji-1-zheng-ti-gou-jian-liu-cheng/"/>
      <url>/2022/04/02/mmdetection-xue-xi-bi-ji-1-zheng-ti-gou-jian-liu-cheng/</url>
      
        <content type="html"><![CDATA[<h1 id="轻松掌握-MMDetection-整体构建流程-一"><a href="#轻松掌握-MMDetection-整体构建流程-一" class="headerlink" title="轻松掌握 MMDetection 整体构建流程(一)"></a>轻松掌握 MMDetection 整体构建流程(一)</h1><h2 id="0-涉及内容"><a href="#0-涉及内容" class="headerlink" title="0 涉及内容"></a>0 涉及内容</h2><ul><li>MMDetection整体构建流程与思想</li><li>目标检测算法核心组件划分</li><li>目标检测核心组件功能</li></ul><h2 id="1-目标检测算法抽象流程"><a href="#1-目标检测算法抽象流程" class="headerlink" title="1 目标检测算法抽象流程"></a>1 目标检测算法抽象流程</h2><p>目前目标检测算法大致归纳如下：</p><p><img src="https://pic1.zhimg.com/80/v2-23f3f33d5ed5792e7ad55e559a6798fc_720w.jpg" alt="img"></p><p>目标检测算法可以按照 3 个维度划分：</p><ul><li><p><strong>按照 stage 个数划分</strong>，常规是 one-stage 和 two-stage，但是实际上界限不是特别清晰，例如带 refine 阶段的算法 RepPoints，实际上可以认为是1.5 stage 算法，而 Cascade R-CNN 可以认为是多阶段算法，为了简单，上面图示没有划分如此细致。</p></li><li><p><strong>按照是否需要预定义 anchor 划分</strong>，常规是 anchor-based 和 anchor-free，当然也有些算法是两者混合的。</p></li><li><p><strong>按照是否采用了 transformer 结构划分</strong>，目前基于 transformer 结构的目标检测算法发展迅速，也引起了极大的关注，所以这里特意增加了这个类别的划分</p></li></ul><h2 id="2-MMDetection整体构建流程与思想"><a href="#2-MMDetection整体构建流程与思想" class="headerlink" title="2 MMDetection整体构建流程与思想"></a>2 MMDetection整体构建流程与思想</h2><p>基于目前代码实现，所有目标检测算法都按照以下流程进行划分：<img src="https://pic1.zhimg.com/80/v2-7ecc8e5e19c59a3e6682c5e3cdc34918_720w.jpg" alt="img"></p><p>上述流程对应 MMDetection 代码构建流程，理解每个组件的作用不仅仅对阅读算法源码有帮助，而且还能够快速理解新提出算法对应的改进部分。下面对每个模块进行详细解读。</p><h3 id="2-1-训练核心组件"><a href="#2-1-训练核心组件" class="headerlink" title="2.1 训练核心组件"></a>2.1 训练核心组件</h3><p>训练部分一般包括 9 个核心组件，总体流程是：</p><ol><li>任何一个 batch 的图片先输入到 <strong>backbone 中进行特征提取</strong>，典型的骨干网络是 <strong>ResNet</strong>。</li><li>输出的单尺度或者多尺度特征图输入到 <strong>neck 模块中进行特征融合或者增强</strong>，典型的 neck 是 <strong>FPN</strong>。</li><li>上述多尺度特征最终输入到 <strong>head 部分</strong>，一般都会包括<strong>分类和回归</strong>分支输出。</li><li>在整个<strong>网络构建阶段</strong>都可以引入一些即插即用<strong>增强算子</strong>来增加提取提取能力，典型的例如 <strong>SPP、DCN</strong> 等等。</li><li>目标检测 <strong>head 输出一般是特征图</strong>，对于分类任务存在严重的正负样本不平衡，可以通过正负样本<strong>属性分配和采样</strong>控制。</li><li>为了方便收敛和平衡多分支，一般都会<strong>对 gt bbox 进行编码</strong>。</li><li>最后一步是<strong>计算分类和回归 loss</strong>，进行训练。</li><li>在训练过程中也包括非常多的 <strong>trick</strong>，例如优化器选择等，参数调节也非常关键</li></ol><p>注意上述 9 个组件不是每个算法都需要的，下面详细分析。</p><h4 id="2-1-1-Backbone"><a href="#2-1-1-Backbone" class="headerlink" title="2.1.1 Backbone"></a>2.1.1 Backbone</h4><p><strong>backbone 作用主要是特征提取</strong>，最常用的是 ResNet 系列、ResNetV1d 系列和 Res2Net 系列。</p><p><img src="https://pic2.zhimg.com/80/v2-cdee2bd9f289d650ddbcbd748c4be0f9_720w.jpg" alt="img"></p><p>目前 MMDetection 中已经集成了大部分骨架网络，具体见文件：<code>mmdet/models/backbones</code>，V2.7 已经实现的骨架如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'RegNet'</span><span class="token punctuation">,</span> <span class="token string">'ResNet'</span><span class="token punctuation">,</span> <span class="token string">'ResNetV1d'</span><span class="token punctuation">,</span> <span class="token string">'ResNeXt'</span><span class="token punctuation">,</span> <span class="token string">'SSDVGG'</span><span class="token punctuation">,</span> <span class="token string">'HRNet'</span><span class="token punctuation">,</span> <span class="token string">'Res2Net'</span><span class="token punctuation">,</span>    <span class="token string">'HourglassNet'</span><span class="token punctuation">,</span> <span class="token string">'DetectoRS_ResNet'</span><span class="token punctuation">,</span> <span class="token string">'DetectoRS_ResNeXt'</span><span class="token punctuation">,</span> <span class="token string">'Darknet'</span><span class="token punctuation">,</span>    <span class="token string">'ResNeSt'</span><span class="token punctuation">,</span> <span class="token string">'TridentResNet'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果需要对骨架进行扩展，可以继承上述网络，然后通过<strong>注册器机制</strong>注册使用。一个典型用法为：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 骨架的预训练权重路径</span>pretrained<span class="token operator">=</span><span class="token string">'torchvision://resnet50'</span><span class="token punctuation">,</span>backbone<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'ResNet'</span><span class="token punctuation">,</span> <span class="token comment"># 骨架类名，后面的参数都是该类的初始化参数</span>    depth<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span>    num_stages<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    out_indices<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    frozen_stages<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>    norm_cfg<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'BN'</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>     norm_eval<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    style<span class="token operator">=</span><span class="token string">'pytorch'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>通过 MMCV 中的注册器机制，你可以通过 dict 形式的配置来实例化任何已经注册的类</strong>，非常方便和灵活。</p><h4 id="2-1-2-Neck"><a href="#2-1-2-Neck" class="headerlink" title="2.1.2 Neck"></a>2.1.2 Neck</h4><p>neck 可以认为<strong>是 backbone 和 head 的连接层</strong>，主要负责对 backbone 的特征进行高效融合和增强，能够对输入的单尺度或者多尺度<strong>特征进行融合、增强输出</strong>等。</p><p><img src="https://pic1.zhimg.com/80/v2-f0975c00a32fa03a80860f9c09234bbc_720w.jpg" alt="img"></p><p>具体见文件：<code>mmdet/models/necks</code>，V2.7 已经实现的 neck 如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'FPN'</span><span class="token punctuation">,</span> <span class="token string">'BFP'</span><span class="token punctuation">,</span> <span class="token string">'ChannelMapper'</span><span class="token punctuation">,</span> <span class="token string">'HRFPN'</span><span class="token punctuation">,</span> <span class="token string">'NASFPN'</span><span class="token punctuation">,</span> <span class="token string">'FPN_CARAFE'</span><span class="token punctuation">,</span> <span class="token string">'PAFPN'</span><span class="token punctuation">,</span>    <span class="token string">'NASFCOS_FPN'</span><span class="token punctuation">,</span> <span class="token string">'RFP'</span><span class="token punctuation">,</span> <span class="token string">'YOLOV3Neck'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>最常用的应该是 FPN，一个典型用法是：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">neck<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'FPN'</span><span class="token punctuation">,</span>    in_channels<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">2048</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token comment"># 骨架多尺度特征图输出通道</span>    out_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token comment"># 增强后通道输出</span>    num_outs<span class="token operator">=</span><span class="token number">5</span><span class="token comment"># 输出num_outs个多尺度特征图</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-1-3-Head"><a href="#2-1-3-Head" class="headerlink" title="2.1.3 Head"></a>2.1.3 Head</h4><p>目标检测算法<strong>输出一般包括分类和框坐标回归</strong>两个分支，不同算法 head 模块复杂程度不一样，灵活度比较高。在网络构建方面，<strong>理解目标检测算法主要是要理解 head 模块</strong>。几乎<strong>每个算法都包括一个独立的 head</strong>。</p><p><img src="https://pic2.zhimg.com/80/v2-fdd9a6232e62c75b143153dab8ba9bc1_720w.jpg" alt="img"></p><p>MMDetection 中 head 模块又划分为 <strong>two-stage 所需的 RoIHead</strong> 和 <strong>one-stage 所需的 DenseHead</strong>，也就是说所有的 one-stage 算法的 head 模块都在<code>mmdet/models/dense_heads</code>中，而 two-stage 算法还包括额外的<code>mmdet/models/roi_heads</code>。</p><p>目前 V2.7 中已经实现的 dense_heads 包括：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'AnchorFreeHead'</span><span class="token punctuation">,</span> <span class="token string">'AnchorHead'</span><span class="token punctuation">,</span> <span class="token string">'GuidedAnchorHead'</span><span class="token punctuation">,</span> <span class="token string">'FeatureAdaption'</span><span class="token punctuation">,</span>    <span class="token string">'RPNHead'</span><span class="token punctuation">,</span> <span class="token string">'GARPNHead'</span><span class="token punctuation">,</span> <span class="token string">'RetinaHead'</span><span class="token punctuation">,</span> <span class="token string">'RetinaSepBNHead'</span><span class="token punctuation">,</span> <span class="token string">'GARetinaHead'</span><span class="token punctuation">,</span>    <span class="token string">'SSDHead'</span><span class="token punctuation">,</span> <span class="token string">'FCOSHead'</span><span class="token punctuation">,</span> <span class="token string">'RepPointsHead'</span><span class="token punctuation">,</span> <span class="token string">'FoveaHead'</span><span class="token punctuation">,</span>    <span class="token string">'FreeAnchorRetinaHead'</span><span class="token punctuation">,</span> <span class="token string">'ATSSHead'</span><span class="token punctuation">,</span> <span class="token string">'FSAFHead'</span><span class="token punctuation">,</span> <span class="token string">'NASFCOSHead'</span><span class="token punctuation">,</span>    <span class="token string">'PISARetinaHead'</span><span class="token punctuation">,</span> <span class="token string">'PISASSDHead'</span><span class="token punctuation">,</span> <span class="token string">'GFLHead'</span><span class="token punctuation">,</span> <span class="token string">'CornerHead'</span><span class="token punctuation">,</span> <span class="token string">'YOLACTHead'</span><span class="token punctuation">,</span>    <span class="token string">'YOLACTSegmHead'</span><span class="token punctuation">,</span> <span class="token string">'YOLACTProtonet'</span><span class="token punctuation">,</span> <span class="token string">'YOLOV3Head'</span><span class="token punctuation">,</span> <span class="token string">'PAAHead'</span><span class="token punctuation">,</span>    <span class="token string">'SABLRetinaHead'</span><span class="token punctuation">,</span> <span class="token string">'CentripetalHead'</span><span class="token punctuation">,</span> <span class="token string">'VFNetHead'</span><span class="token punctuation">,</span> <span class="token string">'TransformerHead'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>而 roi_heads 比较杂，就不列出了。</p><p>需要注意的是：<strong>two-stage 或者 mutli-stage 算法，会额外包括一个区域提取器 roi extractor，用于将不同大小的 RoI 特征图统一成相同大小</strong>。</p><p>虽然 head 部分的网络构建比较简单，但是由于正负样本属性定义、正负样本采样和 bbox 编解码模块都在 head 模块中进行组合调用，故 MMDetection <strong>中最复杂的模块就是 head</strong>。在最后的整体流程部分会对该模块进行详细分析。</p><h4 id="2-1-4-Enhance"><a href="#2-1-4-Enhance" class="headerlink" title="2.1.4 Enhance"></a>2.1.4 Enhance</h4><p>enhance 是<strong>即插即用、能够对特征进行增强的模块</strong>，其具体代码可以<strong>通过 dict 形式注册到 backbone、neck 和 head 中</strong>，非常方便(目前还不完善)。常用的 enhance 模块是 <strong>SPP、ASPP、RFB、Dropout、Dropblock、DCN 和各种注意力模块</strong> SeNet、Non_Local、CBA 等。目前 MMDetection 中部分模块支持 enhance 的接入，例如 ResNet 骨架中的 plugins，这个部分的解读放在具体算法模块中讲解。</p><p><img src="https://pic3.zhimg.com/80/v2-65a706efe224f0b7ffc7f4fd7a65f2ca_720w.jpg" alt="img"></p><h4 id="2-1-5-BBox-Assigner"><a href="#2-1-5-BBox-Assigner" class="headerlink" title="2.1.5 BBox Assigner"></a>2.1.5 BBox Assigner</h4><p><strong>正负样本属性分配模块</strong>作用是<strong>进行正负样本定义或者正负样本分配</strong>（可能也包括忽略样本定义），正样本就是常说的前景样本（可以是任何类别），负样本就是背景样本。因为目标检测是一个同时进行分类和回归的问题，对于分类场景必然需要确定正负样本，否则无法训练。该模块至关重要，不同的正负样本分配策略会带来显著的性能差异，目前大部分目标检测算法都会对这个部分进行改进，至关重要。一些典型的分配策略如下：</p><p><img src="https://pic3.zhimg.com/80/v2-12bae70e2ea2e4afb05d0d8d3f38ca56_720w.jpg" alt="img"></p><p>对应的代码在<code>mmdet/core/bbox/assigners</code>中，V2.7 主要包括：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'BaseAssigner'</span><span class="token punctuation">,</span> <span class="token string">'MaxIoUAssigner'</span><span class="token punctuation">,</span> <span class="token string">'ApproxMaxIoUAssigner'</span><span class="token punctuation">,</span>     <span class="token string">'PointAssigner'</span><span class="token punctuation">,</span> <span class="token string">'ATSSAssigner'</span><span class="token punctuation">,</span> <span class="token string">'CenterRegionAssigner'</span><span class="token punctuation">,</span> <span class="token string">'GridAssigner'</span><span class="token punctuation">,</span>    <span class="token string">'HungarianAssigner'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-1-7-BBox-Encoder"><a href="#2-1-7-BBox-Encoder" class="headerlink" title="2.1.7 BBox Encoder"></a>2.1.7 BBox Encoder</h4><p>为了<strong>更好的收敛和平衡多个 loss</strong>，具体解决办法非常多，而 bbox 编解码策略也算其中一个，bbox <strong>编码阶段</strong>对应的是<strong>对正样本的 gt bbox 采用某种编码变换</strong>（反操作就是 bbox 解码），最简单的编码是对 gt bbox 除以图片宽高进行归一化以平衡分类和回归分支，一些典型的编解码策略如下：</p><p><img src="https://pic4.zhimg.com/80/v2-1f8d5e5e45886423df474d168452f50b_720w.jpg" alt="img"></p><p>对应的代码在<code>mmdet/core/bbox/coder</code>中，V2.7 主要包括：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'BaseBBoxCoder'</span><span class="token punctuation">,</span> <span class="token string">'PseudoBBoxCoder'</span><span class="token punctuation">,</span> <span class="token string">'DeltaXYWHBBoxCoder'</span><span class="token punctuation">,</span>    <span class="token string">'LegacyDeltaXYWHBBoxCoder'</span><span class="token punctuation">,</span> <span class="token string">'TBLRBBoxCoder'</span><span class="token punctuation">,</span> <span class="token string">'YOLOBBoxCoder'</span><span class="token punctuation">,</span>    <span class="token string">'BucketingBBoxCoder'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-1-8-Loss"><a href="#2-1-8-Loss" class="headerlink" title="2.1.8 Loss"></a>2.1.8 Loss</h4><p>Loss 通常都分为<strong>分类和回归 loss</strong>，其<strong>对网络 head 输出的预测值和 bbox encoder 得到的 targets 进行梯度下降</strong>迭代训练。</p><p>loss 的设计也是各大算法重点改进对象，常用的 loss 如下：</p><p><img src="https://pic4.zhimg.com/80/v2-686b0b9ac6a82f9945ae454d18783227_720w.jpg" alt="img"></p><p>对应的代码在<code>mmdet/models/losses</code>中，V2.7 主要包括：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'cross_entropy'</span><span class="token punctuation">,</span> <span class="token string">'binary_cross_entropy'</span><span class="token punctuation">,</span>    <span class="token string">'mask_cross_entropy'</span><span class="token punctuation">,</span> <span class="token string">'CrossEntropyLoss'</span><span class="token punctuation">,</span> <span class="token string">'sigmoid_focal_loss'</span><span class="token punctuation">,</span>    <span class="token string">'FocalLoss'</span><span class="token punctuation">,</span> <span class="token string">'smooth_l1_loss'</span><span class="token punctuation">,</span> <span class="token string">'SmoothL1Loss'</span><span class="token punctuation">,</span> <span class="token string">'balanced_l1_loss'</span><span class="token punctuation">,</span>    <span class="token string">'BalancedL1Loss'</span><span class="token punctuation">,</span> <span class="token string">'mse_loss'</span><span class="token punctuation">,</span> <span class="token string">'MSELoss'</span><span class="token punctuation">,</span> <span class="token string">'iou_loss'</span><span class="token punctuation">,</span> <span class="token string">'bounded_iou_loss'</span><span class="token punctuation">,</span>    <span class="token string">'IoULoss'</span><span class="token punctuation">,</span> <span class="token string">'BoundedIoULoss'</span><span class="token punctuation">,</span> <span class="token string">'GIoULoss'</span><span class="token punctuation">,</span> <span class="token string">'DIoULoss'</span><span class="token punctuation">,</span> <span class="token string">'CIoULoss'</span><span class="token punctuation">,</span> <span class="token string">'GHMC'</span><span class="token punctuation">,</span>    <span class="token string">'GHMR'</span><span class="token punctuation">,</span> <span class="token string">'reduce_loss'</span><span class="token punctuation">,</span> <span class="token string">'weight_reduce_loss'</span><span class="token punctuation">,</span> <span class="token string">'weighted_loss'</span><span class="token punctuation">,</span> <span class="token string">'L1Loss'</span><span class="token punctuation">,</span>    <span class="token string">'l1_loss'</span><span class="token punctuation">,</span> <span class="token string">'isr_p'</span><span class="token punctuation">,</span> <span class="token string">'carl_loss'</span><span class="token punctuation">,</span> <span class="token string">'AssociativeEmbeddingLoss'</span><span class="token punctuation">,</span>    <span class="token string">'GaussianFocalLoss'</span><span class="token punctuation">,</span> <span class="token string">'QualityFocalLoss'</span><span class="token punctuation">,</span> <span class="token string">'DistributionFocalLoss'</span><span class="token punctuation">,</span>    <span class="token string">'VarifocalLoss'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看出 MMDetection 中已经实现了非常多的 loss，可以直接使用。</p><h4 id="2-1-9-Training-tricks"><a href="#2-1-9-Training-tricks" class="headerlink" title="2.1.9 Training tricks"></a>2.1.9 Training tricks</h4><p>训练技巧非常多，常说的调参很大一部分工作都是在设置这部分超参。这部分内容比较杂乱，很难做到完全统一，目前主流的 tricks 如下所示:</p><p><img src="https://pic3.zhimg.com/80/v2-569a12b6d4a20f8619a27b48d5b2fa42_720w.jpg" alt="img"></p><p>MMDetection 目前这部分还会继续完善。</p><h3 id="2-2-测试核心组件"><a href="#2-2-测试核心组件" class="headerlink" title="2.2 测试核心组件"></a>2.2 测试核心组件</h3><p>测试核心组件和训练非常类似，但是简单很多，除了必备的网络构建部分外( <strong>backbone、neck、head 和 enhance</strong> )，不需要正负样本定义、正负样本采样和 loss 计算三个最难的部分，但是其<strong>额外需要一个 bbox 后处理模块</strong>和<strong>测试 trick</strong>。</p><h4 id="2-2-1-BBox-Decoder"><a href="#2-2-1-BBox-Decoder" class="headerlink" title="2.2.1 BBox Decoder"></a>2.2.1 BBox Decoder</h4><p><strong>训练时候进行了编码，那么对应的测试环节需要进行解码。</strong>根据编码的不同，解码也是不同的。举个简单例子：假设训练时候对宽高是直接除以图片宽高进行归一化的，那么解码过程也仅仅需要乘以图片宽高即可。其代码和 bbox encoder 放在一起，在<code>mmdet/core/bbox/coder</code>中。</p><h4 id="2-2-2-BBox-PostProcess"><a href="#2-2-2-BBox-PostProcess" class="headerlink" title="2.2.2 BBox PostProcess"></a>2.2.2 BBox PostProcess</h4><p>在<strong>得到原图尺度 bbox 后，由于可能会出现重叠 bbox 现象</strong>，故一般都需要进行后处理，最常用的后处理就是<strong>非极大值抑制以及其变种</strong>。</p><p>其对应的文件在<code>mmdet/core/post_processing</code>中，V2.7 主要包括：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'multiclass_nms'</span><span class="token punctuation">,</span> <span class="token string">'merge_aug_proposals'</span><span class="token punctuation">,</span> <span class="token string">'merge_aug_bboxes'</span><span class="token punctuation">,</span>    <span class="token string">'merge_aug_scores'</span><span class="token punctuation">,</span> <span class="token string">'merge_aug_masks'</span><span class="token punctuation">,</span> <span class="token string">'fast_nms'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-2-3-Testing-tricks"><a href="#2-2-3-Testing-tricks" class="headerlink" title="2.2.3 Testing tricks"></a>2.2.3 Testing tricks</h4><p>为了提高检测性能，测试阶段也会采用 trick。这个阶段的 tricks 也非常多，难以完全统一，最典型的是<strong>多尺度测试</strong>以及<strong>各种模型集成手段</strong>，典型配置如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'MultiScaleFlipAug'</span><span class="token punctuation">,</span>    img_scale<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1333</span><span class="token punctuation">,</span> <span class="token number">800</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    flip<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    transforms<span class="token operator">=</span><span class="token punctuation">[</span>        <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'Resize'</span><span class="token punctuation">,</span> keep_ratio<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'RandomFlip'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'Normalize'</span><span class="token punctuation">,</span> <span class="token operator">**</span>img_norm_cfg<span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'Pad'</span><span class="token punctuation">,</span> size_divisor<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'ImageToTensor'</span><span class="token punctuation">,</span> keys<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'img'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'Collect'</span><span class="token punctuation">,</span> keys<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'img'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://pic3.zhimg.com/80/v2-16e307727f0c3e941ec72c21f214b982_720w.jpg" alt="img"></p><h3 id="2-3-训练测试算法流程"><a href="#2-3-训练测试算法流程" class="headerlink" title="2.3 训练测试算法流程"></a>2.3 训练测试算法流程</h3><p>在分析完每个训练流程的各个核心组件后，为了方便大家理解整个算法构建，下面分析 MMDetection 是<strong>如何组合各个组件进行训练的</strong>，这里<strong>以 one-stage 检测器为例</strong>，two-stage 也比较类似。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SingleStageDetector</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 构建backbone、neck和head</span>        self<span class="token punctuation">.</span>backbone <span class="token operator">=</span> build_backbone<span class="token punctuation">(</span>backbone<span class="token punctuation">)</span>        <span class="token keyword">if</span> neck <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>neck <span class="token operator">=</span> build_neck<span class="token punctuation">(</span>neck<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bbox_head <span class="token operator">=</span> build_head<span class="token punctuation">(</span>bbox_head<span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward_train</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token comment"># 先运行backbone+neck进行特征提取</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>extract_feat<span class="token punctuation">(</span>img<span class="token punctuation">)</span>        <span class="token comment"># 对head进行forward train，输出loss</span>        losses <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">.</span>forward_train<span class="token punctuation">(</span>            x<span class="token punctuation">,</span>             img_metas<span class="token punctuation">,</span>             gt_bboxes<span class="token punctuation">,</span>                                        gt_labels<span class="token punctuation">,</span>             gt_bboxes_ignore        <span class="token punctuation">)</span>        <span class="token keyword">return</span> losses  <span class="token keyword">def</span> <span class="token function">simple_test</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 先运行backbone+neck进行特征提取</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>extract_feat<span class="token punctuation">(</span>img<span class="token punctuation">)</span>        <span class="token comment"># head输出预测特征图</span>        outs <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment"># bbox解码和还原</span>        bbox_list <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">.</span>get_bboxes<span class="token punctuation">(</span>            <span class="token operator">*</span>outs<span class="token punctuation">,</span>             img_metas<span class="token punctuation">,</span>             rescale<span class="token operator">=</span>rescale        <span class="token punctuation">)</span>        <span class="token comment"># 重组结果返回</span>        bbox_results <span class="token operator">=</span> <span class="token punctuation">[</span>            bbox2result<span class="token punctuation">(</span>                det_bboxes<span class="token punctuation">,</span>                 det_labels<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">.</span>num_classes            <span class="token punctuation">)</span>            <span class="token keyword">for</span> det_bboxes<span class="token punctuation">,</span> det_labels <span class="token keyword">in</span> bbox_list        <span class="token punctuation">]</span>        <span class="token keyword">return</span> bbox_results<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>以上就是整个检测器算法训练和测试最简逻辑，可以发现训练部分最核心的就是<code>bbox_head.forward_train</code>，测试部分最核心的是<code>bbox_head.get_bboxes</code>，下面单独简要分析。</p><h4 id="2-3-1-bbox-head-forward-train"><a href="#2-3-1-bbox-head-forward-train" class="headerlink" title="2.3.1 bbox_head.forward_train"></a>2.3.1 bbox_head.forward_train</h4><p>forward_train 是通用函数，如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward_train</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 调用每个head自身的forward方法</span>    outs <span class="token operator">=</span> self<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token keyword">if</span> gt_labels <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>        loss_inputs <span class="token operator">=</span> outs <span class="token operator">+</span> <span class="token punctuation">(</span>gt_bboxes<span class="token punctuation">,</span> img_metas<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        loss_inputs <span class="token operator">=</span> outs <span class="token operator">+</span> <span class="token punctuation">(</span>gt_bboxes<span class="token punctuation">,</span> gt_labels<span class="token punctuation">,</span> img_metas<span class="token punctuation">)</span>    <span class="token comment"># 计算每个head自身的loss方法</span>    losses <span class="token operator">=</span> self<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>        <span class="token operator">*</span>loss_inputs<span class="token punctuation">,</span>         gt_bboxes_ignore<span class="token operator">=</span>gt_bboxes_ignore    <span class="token punctuation">)</span>    <span class="token comment"># 返回</span>    <span class="token keyword">return</span> losses<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对于不同的 head，虽然 forward 内容不一样，但是依然可以抽象为： <code>outs = self(x)</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> feats<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token comment"># 多尺度特征图，一个一个迭代进行forward_single</span>   <span class="token keyword">return</span> multi_apply<span class="token punctuation">(</span>self<span class="token punctuation">.</span>forward_single<span class="token punctuation">,</span> feats<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">forward_single</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token comment"># 运行各个head独特的head forward方法，得到预测图</span>   <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>   <span class="token keyword">return</span> cls_score<span class="token punctuation">,</span> bbox_pred<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>而对于不同的 head，其 loss 计算部分也比较复杂，可以简单抽象为：<code>losses = self.loss(...)</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">loss</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 1 生成anchor-base需要的anchor或者anchor-free需要的points</span>    <span class="token comment"># 2 利用gt bbox对特征图或者anchor计算其正负和忽略样本属性</span>    <span class="token comment"># 3 进行正负样本采样</span>    <span class="token comment"># 4 对gt bbox进行bbox编码</span>    <span class="token comment"># 5 loss计算，并返回</span>    <span class="token keyword">return</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>loss_cls<span class="token operator">=</span>losses_cls<span class="token punctuation">,</span> loss_bbox<span class="token operator">=</span>losses_bbox<span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-3-2-bbox-head-get-bboxes"><a href="#2-3-2-bbox-head-get-bboxes" class="headerlink" title="2.3.2 bbox_head.get_bboxes"></a>2.3.2 bbox_head.get_bboxes</h4><p>get_bboxes函数更加简单</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_bboxes</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token comment"># 1 生成anchor-base需要的anchor或者anchor-free需要的points</span>   <span class="token comment"># 2 遍历每个输出层，遍历batch内部的每张图片，对每张图片先提取指定个数的预测结果，缓解后面后处理压力；对保留的位置进行bbox解码和还原到原图尺度</span>   <span class="token comment"># 3 统一nms后处理</span>   <span class="token keyword">return</span> det_bboxes<span class="token punctuation">,</span> det_labels<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h2><p>本文重点分析了一个目标检测器是如何通过多个核心组件堆叠而成，不涉及具体代码，大家只需要总体把握即可，其中最应该了解的是：<strong>任何一个目标检测算法都可以分成 n 个核心组件，组件和组件之间是隔离的，方便复用和设计</strong>。当面对一个新算法时候我们可以先分析其主要是改进了哪几个核心组件，然后就可以高效的掌握该算法。</p><p>另外还有一些重要的模块没有分析，特别是 dataset、dataloader 和分布式训练相关的检测代码，由于篇幅限制就不介绍了，如有需要欢迎在评论区留言。</p><p>再次欢迎大家使用 MMDetection，也非常欢迎社区贡献！</p><p>最后附上总图：</p><p><img src="https://pic3.zhimg.com/80/v2-c4e6229a1fd42692d090108481be34a6_720w.jpg" alt="img"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>智能视频感知与分析工作室招新考核题目</title>
      <link href="/2022/03/18/zhao-xin-kao-he/"/>
      <url>/2022/03/18/zhao-xin-kao-he/</url>
      
        <content type="html"><![CDATA[<h1 id="智能视频感知与分析工作室招新考核题目"><a href="#智能视频感知与分析工作室招新考核题目" class="headerlink" title="智能视频感知与分析工作室招新考核题目"></a>智能视频感知与分析工作室招新考核题目</h1><h2 id="时间：2022-x2F-03-x2F-19-2022-x2F-03-x2F-26"><a href="#时间：2022-x2F-03-x2F-19-2022-x2F-03-x2F-26" class="headerlink" title="时间：2022/03/19 ~ 2022/03/26"></a>时间：2022/03/19 ~ 2022/03/26</h2><p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20220318222607061.png" alt="image-20220318222607061"></p><hr><h2 id="技术类（算法设计组、轻量化及产品设计组）"><a href="#技术类（算法设计组、轻量化及产品设计组）" class="headerlink" title="技术类（算法设计组、轻量化及产品设计组）"></a>技术类（算法设计组、轻量化及产品设计组）</h2><p><strong>组队题目要求：</strong></p><ul><li><strong>自由组队，每队<code>1-3</code>人，分工明确；</strong></li><li><strong>由题目清单选择题目，亦可自选题目；</strong></li></ul><p><strong>项目规范要求：</strong></p><ul><li>需要程序实现，制作PPT并展示（5-10分钟展示时间）；</li><li>上传至<code>Github</code>并提交链接（如自选题目涉及自有知识产权，可不提供）；</li><li>提交实验结果录屏演示；</li><li>工程结构规范、注释丰富；</li><li>必须包含<code>Readme.md</code>说明文件；</li><li>语言不限；</li><li>如能将实现过程进行记录汇总为Blog可作为加分项。</li></ul><p><strong>题目清单：</strong></p><ol><li>基于U-Net的图像分割；包括：搭建U-Net网络结构、实验数据处理、网络的训练与测试。（<a href="https://drive.google.com/file/d/1OQBErLlZ-xNGz5PsoOaJwjG9E-N4dcbE/view?usp=sharing">数据集链接</a>）提示：很多博客都有类似讲解，复现难度不大，数据集很小（30训练+30测试），CPU也可训练。不要求准确率，自主编写代码实现即可。</li><li>基于OpenCV的实时人脸识别（可直接调用OpenCV人脸检测器，但所使用原理方法需要阐述清楚）；</li><li>基于Dlib的人脸识别（可直接调用，阐述其与OpenCV人脸检测器方法的原理及实现区别）；</li><li>基于Yolo的图像目标检测技术（不要求训练，只需要利用官方给定权重预测图像即可）；</li><li>LeNet手写体字符识别；</li><li>基于OpenMP、MPI、CUDA或OneAPI（四选一、或结合）的卷积操作加速（C\C++实现可优先考虑）；</li><li>基于嵌入式设备（例如有树莓派等，可做）目标检测或图片分类；</li><li>机械设计制造、实体模型设计、3D打印等（可自选的相关内容）；</li><li>复现任一较新的深度学习方法（可自选的相关内容）。</li></ol><h2 id="非技术类（策划管理组及UI交互设计组）"><a href="#非技术类（策划管理组及UI交互设计组）" class="headerlink" title="非技术类（策划管理组及UI交互设计组）"></a>非技术类（策划管理组及UI交互设计组）</h2><p><strong>要求：</strong></p><ul><li><strong>自由组队，每队<code>1-2</code>人；</strong></li><li><strong>从以下题目任选其一，可撰写策划案、制作PPT或图形化界面，亦可自选主题；</strong></li><li><strong>制作并提交项目计划书(提交PPT、或设计草图等)。</strong></li></ul><p><strong>题目清单</strong></p><ol><li><p><code>策划</code>调研国内外智能监控视频分析系统的发展趋势，分析各系统不足与优势，根据调研结果，制作不少于10页的PPT（项目计划书格式），拟定未来基本研究方向（要求：提交PPT或其他策划资料）。</p></li><li><p><code>策划</code>调研校园暴力事件识别系统的潜在应用场景，深入剖析行业痛点需求，给出分析思考，结合相应技术进行分析更佳，提交PPT及其他材料；</p></li><li><p><code>策划</code>策划结合人工智能技术，剖析身边技术方案、产品等方面的需求，给出整体性的策划方法，明确需求、市场、技术背景等要素。</p></li><li><p><code>GUI</code>复现QT-GUI项目PyDracula，进行自定义美化编辑，并在窗口内实现：读取本地视频进行播放，在窗口可控制视频的播放与暂停。<a href="https://github.com/Wanderson-Magalhaes/Modern_GUI_PyDracula_PySide6_or_PyQt6">项目地址</a>（要求：提交UI运行视频与代码github链接）</p></li><li><p><code>设计</code>独立设计的任一作品（例如图片、海报等，自有版权的记得加水印等措施），阐明设计思想，设计理念；</p></li></ol><hr><h2 id="组队及报名地址"><a href="#组队及报名地址" class="headerlink" title="组队及报名地址"></a>组队及报名地址</h2><h3 id="智能视频感知与分析工作室招新考核登记表"><a href="#智能视频感知与分析工作室招新考核登记表" class="headerlink" title="智能视频感知与分析工作室招新考核登记表"></a><a href="https://docs.qq.com/sheet/DWGxweHlkVldZQ1RE">智能视频感知与分析工作室招新考核登记表</a></h3>]]></content>
      
      
      
        <tags>
            
            <tag> 考核题目 </tag>
            
            <tag> 智能视频感知与分析工作室 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/03/15/hello-world/"/>
      <url>/2022/03/15/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span>$ hexo server$ hexo server$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ZerotTier内网穿透+VSCode配置远程开发+tmux基础操作</title>
      <link href="/2021/11/11/zerotier-nei-wang-chuan-tou-vscode-pei-zhi-yuan-cheng-kai-fa-tmux-ji-chu-cao-zuo/"/>
      <url>/2021/11/11/zerotier-nei-wang-chuan-tou-vscode-pei-zhi-yuan-cheng-kai-fa-tmux-ji-chu-cao-zuo/</url>
      
        <content type="html"><![CDATA[<h1 id="ZerotTier内网穿透-VSCode配置远程开发-tmux基础操作"><a href="#ZerotTier内网穿透-VSCode配置远程开发-tmux基础操作" class="headerlink" title="ZerotTier内网穿透+VSCode配置远程开发+tmux基础操作"></a>ZerotTier内网穿透+VSCode配置远程开发+tmux基础操作</h1><ul><li>平时在开发深度学习等相关项目时，往往需要大型服务器或工作站的支持，远程开发时使用向日葵等远程桌面软件往往不是那么明知，多人使用冲突、限速卡顿，代码体验极其**。</li><li>那么有没有更好的解决方案呢？单就小型团队而言，如果能将所有散布在各个实验室的机器使用内网穿透统一接入同一局域网，便可方便的使用ssh连接，搭配<code>vscode</code>的<code>ssh-remote</code>插件与<code>tmux</code>进行终端复用，便可实现本地无感的远程开发。</li></ul><h2 id="1-ZeroTier配置内网穿透"><a href="#1-ZeroTier配置内网穿透" class="headerlink" title="1. ZeroTier配置内网穿透"></a>1. ZeroTier配置内网穿透</h2><h3 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1 简介"></a>1.1 简介</h3><p><a href="https://www.zerotier.com/">ZeroTier </a>作为一款非常简单易用的内网穿透工具，<strong>不需要复杂配置</strong>，就能实现虚拟局域网的组建，让你可以在外也能连回实验室的NAS、服务器获取数据、远程开发。</p><h3 id="1-2-费用"><a href="#1-2-费用" class="headerlink" title="1.2 费用"></a>1.2 费用</h3><p>免费网络限制 100 台设备，超过了就要付费。100 台对于个人或者小团队使用来说都足够了。</p><h3 id="1-3-支持平台"><a href="#1-3-支持平台" class="headerlink" title="1.3 支持平台"></a>1.3 <a href="https://www.zerotier.com/download/">支持平台</a></h3><p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024133645939.png" alt="image-20211024133645939"></p><h3 id="1-4-使用步骤"><a href="#1-4-使用步骤" class="headerlink" title="1.4 使用步骤"></a>1.4 使用步骤</h3><ul><li>说明：如已有Network ID，直接执行步骤3，安装<a href="https://www.zerotier.com/download/">客户端</a>，加入Network ID即可</li></ul><ol><li><p>注册ZeroTier ID：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024141003246.png" alt="image-20211024141003246" style="zoom:50%;"></li><li><p>创建私有局域网，得到Network ID与子网地址：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024135727789.png" alt="image-20211024135727789" style="zoom:80%;"></li><li><p>安装<a href="https://www.zerotier.com/download/">客户端</a>，<strong>加入Network ID</strong></p><ul><li>windows下<code>ipconfig</code>，ubuntu下<code>ifconfig</code>出现ZeroTier的网段后说明连接成功（也可直接ping其他ip验证）：</li></ul><p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024140430169.png" alt="image-20211024140430169"></p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024143314079.png" alt="image-20211024143314079" style="zoom: 80%;"></li><li><p>管理：</p><ul><li>可以选择子网地址：</li></ul><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024135944372.png" alt="image-20211024135944372" style="zoom: 67%;"><ul><li><p>查看连接客户端，第三列机器即为所有机器的局域网IP，接下来的步骤即shh该ip地址：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024140138785.png" alt="image-20211024140138785" style="zoom: 50%;"></li></ul></li></ol><hr><h2 id="2-VSCode配置Remote-SSH插件"><a href="#2-VSCode配置Remote-SSH插件" class="headerlink" title="2. VSCode配置Remote-SSH插件"></a>2. VSCode配置Remote-SSH插件</h2><h3 id="2-1-安装OpenSSH"><a href="#2-1-安装OpenSSH" class="headerlink" title="2.1 安装OpenSSH"></a>2.1 安装OpenSSH</h3><ul><li><p>Remote-SSH插件是基于SHH的，所以首先要确保本机和远程服务器都安装好了OpenSHH</p></li><li><p>Ubuntu：</p><p>ubuntu默认并没有安装ssh服务，如果通过ssh链接ubuntu，需要自己手动安装ssh-server。判断是否安装ssh服务，可以通过如下命令进行：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">ssh</span> localhost   ssh: connect to <span class="token function">host</span> localhost port <span class="token number">22</span>: Connection refused   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>若出现上述情况，表示还没有安装，可通过以下命令安装：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> openssh-server<span class="token function">sudo</span> /etc/init.d/ssh start<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>安装启动后，可以通过如下命令查看服务是否正确启动：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">ps</span> -e<span class="token operator">|</span><span class="token function">grep</span> <span class="token function">ssh</span><span class="token number">6212</span> ?        00:00:00 sshd  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>如上表示安装成功</p></li><li><p>Windows：</p><p>Win10现在已经支持OpenSSH，可在设置-&gt;应用-&gt;可选功能中查看：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024143529860.png" alt="image-20211024143529860" style="zoom:67%;"><p>若未安装，直接使用添加功能-&gt;搜索SSH安装即可，其他安装方式可参考<a href="https://www.jianshu.com/p/f8ba3e51d60e">Windows安装OpenSSH支持SSH - 简书 (jianshu.com)</a></p></li><li><p>Mac OS：</p><p>Mac OS X系统已经默认安装了SSH，但是SSH服务并未启用，启用SSH服务的方法：</p><p>系统偏好设置-&gt;共享-&gt;勾选“远程登陆”：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024143911895.png" alt="image-20211024143911895" style="zoom:67%;"></li><li><p>验证ssh：</p><p>使用：<code>ssh &lt;username&gt;@&lt;ip&gt;</code>连接任意主机，输入密码连接成功即可：</p><p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024144324785.png" alt="image-20211024144324785"></p></li></ul><h3 id="2-2-配置Remote-SSH插件"><a href="#2-2-配置Remote-SSH插件" class="headerlink" title="2.2 配置Remote-SSH插件"></a>2.2 配置Remote-SSH插件</h3><ul><li><p>安装Remote-SSH：</p><p>在vscode的拓展商店中搜索Remote-SSH进行安装，安装完成后左侧会出现以下按钮：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024145412604.png" alt="image-20211024145412604" style="zoom:50%;"></li><li><p>配置config文件：</p><p>进入该拓展，点击SSH TARGETS上面的设置按钮，选择所要配置的ssh config文件（一般为第一个）：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024145620858.png" alt="image-20211024145620858" style="zoom:67%;"><p>配置远程服务器的名称、ip与用户名：</p><ul><li><p><code>Host</code>: 主机的自定义显示名，可以随便起</p></li><li><p><code>HostName</code>: 登录远程主机的内网IP，即1.4中主机内网穿透后得到的虚拟IP</p></li><li><p><code>User</code>: 登录远程主机的用户名</p></li><li><p><code>Port</code>: 用于登录远程主机的端口（可选）</p></li><li><p><code>IdentityFile</code>: 本地的id_rsa的路径（用于免密登陆的私钥）（多人使用不推荐配置私钥免密）（可选）</p></li></ul></li></ul><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024150620523.png" alt="image-20211024150620523" style="zoom:80%;"><ul><li>远程连接测试：</li></ul><p>​    配置完成后，该窗口下会出现所配置的主机，可以在新窗口下进行连接：</p><p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024151155738.png" alt="image-20211024151155738"></p><p>（第一次连接需要选择服务器操作系统）-&gt; 输入密码-&gt;等待服务器安装vscode远程端-&gt;打开远程项目文件夹后即可开始使用，所有的使用均和本地使用无任何差异：    </p><p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024151750787.png" alt="image-20211024151750787"></p><hr><h2 id="3-使用Tmux保证会话持续运行"><a href="#3-使用Tmux保证会话持续运行" class="headerlink" title="3.  使用Tmux保证会话持续运行"></a>3.  使用Tmux保证会话持续运行</h2><h3 id="3-1-Tmux简介"><a href="#3-1-Tmux简介" class="headerlink" title="3.1 Tmux简介"></a>3.1 Tmux简介</h3><ul><li><strong>目的：****避免训练过程中因为本地Terminal关闭后服务器上的进程也被关闭。</strong></li></ul><h4 id="为什么需要终端复用？"><a href="#为什么需要终端复用？" class="headerlink" title="为什么需要终端复用？"></a><a href="https://www.ruanyifeng.com/blog/2019/10/tmux.html">为什么需要终端复用</a>？</h4><blockquote><p>命令行的典型使用方式是，打开一个终端窗口（terminal window，以下简称”窗口”），在里面输入命令。<strong>用户与计算机的这种临时的交互，称为一次”会话”（session）</strong> 。</p></blockquote><blockquote><p>会话的一个重要特点是，窗口与其中启动的进程是连在一起的。打开窗口，会话开始；关闭窗口，会话结束，会话内部的进程也会随之终止，不管有没有运行完。</p></blockquote><blockquote><p>一个典型的例子就是，SSH 登录远程计算机，打开一个远程窗口执行命令。这时，网络突然断线，再次登录的时候，是找不回上一次执行的命令的。因为上一次 SSH 会话已经终止了，里面的进程也随之消失了。</p><p>为了解决这个问题，会话与窗口可以”解绑”：窗口关闭时，会话并不终止，而是继续运行，等到以后需要的时候，再让会话”绑定”其他窗口。</p></blockquote><h4 id="Tmux-终端复用的作用？"><a href="#Tmux-终端复用的作用？" class="headerlink" title="Tmux 终端复用的作用？"></a>Tmux 终端复用的作用？</h4><p>（1）它允许在单个窗口中，同时访问多个会话。这对于同时运行多个命令行程序很有用。</p><p>（2） 它可以让新窗口”接入”已经存在的会话。</p><p>（3）它允许每个会话有多个连接窗口，因此可以多人实时共享会话。</p><p>（4）它还支持窗口任意的垂直和水平拆分。</p><h3 id="3-2-Tmux的安装"><a href="#3-2-Tmux的安装" class="headerlink" title="3.2 Tmux的安装"></a>3.2 Tmux的安装</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># Ubuntu 或 Debian</span>$ <span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> tmux<span class="token comment"># CentOS 或 Fedora</span>$ <span class="token function">sudo</span> yum <span class="token function">install</span> tmux<span class="token comment"># Mac</span>$ brew <span class="token function">install</span> tmux<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-3-Tmux使用常用命令"><a href="#3-3-Tmux使用常用命令" class="headerlink" title="3.3 Tmux使用常用命令"></a>3.3 Tmux使用常用命令</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ tmux new -s <span class="token operator">&lt;</span>窗口名<span class="token operator">&gt;</span> <span class="token comment"># 打开新窗口</span>ctrl+b d  <span class="token comment"># 分离窗口</span>$ tmux info  <span class="token comment"># 列出当前所有 Tmux 会话信息</span>$ tmux attach -t <span class="token operator">&lt;</span>窗口名<span class="token operator">&gt;</span> <span class="token comment"># 连接窗口</span>ctrl+b %<span class="token comment"># 分割窗口</span>ctrl+b s <span class="token comment"># 切换窗口</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h2 id="4-远程文件拷贝"><a href="#4-远程文件拷贝" class="headerlink" title="4. 远程文件拷贝"></a>4. 远程文件拷贝</h2><ul><li><p>直接使用以下命令即可：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ <span class="token function">scp</span> <span class="token operator">&lt;</span>本地文件路径<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>远程主机用户名<span class="token operator">&gt;</span>@<span class="token operator">&lt;</span>远程主机ip<span class="token operator">&gt;</span>:<span class="token operator">&lt;</span>要拷贝到的远程主机路径<span class="token operator">&gt;</span><span class="token comment">#或</span>$ <span class="token function">scp</span> <span class="token operator">&lt;</span>远程主机用户名<span class="token operator">&gt;</span>@<span class="token operator">&lt;</span>远程主机ip<span class="token operator">&gt;</span>:<span class="token operator">&lt;</span>要拷贝的远程主机文件路径<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>拷贝到的本地文件路径<span class="token operator">&gt;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>例如:</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ <span class="token function">scp</span> .<span class="token punctuation">\</span>labels.zip hp3090@192.168.192.164:/media/hp3090/HDD-2T/renjunjie/WSOL_RS/dataset/C45V2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 内网穿透 </tag>
            
            <tag> tmux </tag>
            
            <tag> 远程开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker使用基本说明</title>
      <link href="/2021/04/17/docker-shi-yong-ji-ben-jiao-cheng/"/>
      <url>/2021/04/17/docker-shi-yong-ji-ben-jiao-cheng/</url>
      
        <content type="html"><![CDATA[<h1 id="docker使用基本说明"><a href="#docker使用基本说明" class="headerlink" title="docker使用基本说明"></a>docker使用基本说明</h1><h2 id="1-拉取镜像"><a href="#1-拉取镜像" class="headerlink" title="1. 拉取镜像"></a>1. 拉取镜像</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token function">sudo</span> <span class="token function">docker</span> pull xdurjj/207base:v1<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="2-从镜像启动容器"><a href="#2-从镜像启动容器" class="headerlink" title="2. 从镜像启动容器"></a>2. 从镜像启动容器</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token function">sudo</span> <span class="token function">docker</span> run --gpus all -it --name<span class="token operator">=</span><span class="token string">"rjj_cpp"</span> -v /media/hp3090/HDD-2T/RJJ_FOR_C++:/home xdurjj/207base:v1 /bin/bash<span class="token comment"># --gpus all 开启所有显卡支持</span><span class="token comment"># -it -i(iterative) 打开标准输入 -d(daemons) 启动之后挂起,类似于后台进程 -t(tty) 分配一个伪终端</span><span class="token comment"># --name 容器名</span><span class="token comment"># -v 指定挂载路径，宿主机目录:镜像内挂载路径</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-暂时退出容器"><a href="#3-暂时退出容器" class="headerlink" title="3. 暂时退出容器"></a>3. 暂时退出容器</h2><p>docker退出而不结束容器：CTRL+P+Q，vscode环境下需要替换掉默认的ctrl+q与ctrl+p的快捷键</p><h2 id="4-容器打包为镜像"><a href="#4-容器打包为镜像" class="headerlink" title="4. 容器打包为镜像"></a>4. 容器打包为镜像</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token function">sudo</span> <span class="token function">docker</span> commit <span class="token punctuation">[</span>选项<span class="token punctuation">]</span> <span class="token punctuation">[</span>容器ID或容器名<span class="token punctuation">]</span>  <span class="token punctuation">[</span>仓库名:标签<span class="token punctuation">]</span><span class="token comment"># 例：</span><span class="token function">sudo</span> <span class="token function">docker</span> commit -a <span class="token string">'jjren'</span> -m <span class="token string">'安装了mmdet'</span> 9adeb5943045  xdurjj/207base:v1<span class="token comment"># -a:修改人</span><span class="token comment"># -m:备注</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="5-列出所有镜像与容器"><a href="#5-列出所有镜像与容器" class="headerlink" title="5. 列出所有镜像与容器"></a>5. 列出所有镜像与容器</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token comment"># 容器</span><span class="token function">sudo</span> <span class="token function">docker</span> <span class="token function">ps</span> -a<span class="token comment"># 镜像</span><span class="token function">sudo</span> <span class="token function">docker</span> images<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="6-保存镜像至本地-tar"><a href="#6-保存镜像至本地-tar" class="headerlink" title="6. 保存镜像至本地.tar"></a>6. 保存镜像至本地.tar</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token comment"># 保存</span><span class="token function">sudo</span> <span class="token function">docker</span> save 0fdaf3gg4jgd <span class="token operator">&gt;</span> ./my_docker_image.tar<span class="token comment"># 加载</span><span class="token function">sudo</span> <span class="token function">docker</span> load <span class="token operator">&lt;</span> ./my_docker_image.tar<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="7-docker支持图形化界面"><a href="#7-docker支持图形化界面" class="headerlink" title="7. docker支持图形化界面"></a>7. docker支持图形化界面</h2><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell"><span class="token function">docker</span> run -it --name jjren_gui -v /etc/localtime:/etc/localtime:ro --net<span class="token operator">=</span>host -e <span class="token assign-left variable"><span class="token environment constant">DISPLAY</span></span><span class="token operator">=</span>:0 -v <span class="token environment constant">$HOME</span>/.Xauthority:/root/.Xauthority -v /run/media/dell3080/软件/jjren:/home/ xdurjj/207base:v1 /bin/bash<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><hr><h2 id="关于机器"><a href="#关于机器" class="headerlink" title="关于机器"></a>关于机器</h2><ol><li>zerotier连接</li><li>vscode配置远程ssh</li><li>vscode安装远程container</li><li>连接至容器后正常开发<ul><li>文件位置</li><li>容器内mmdet位置</li></ul></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>遥感常用数据集介绍</title>
      <link href="/2020/12/07/yao-gan-shu-ju-ji-zheng-li/"/>
      <url>/2020/12/07/yao-gan-shu-ju-ji-zheng-li/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>【转载】：<a href="https://aistudio.baidu.com/aistudio/projectdetail/882508">遥感影像数据集汇总 - 飞桨AI Studio - 人工智能学习实训社区 (baidu.com)</a><br>在使用深度学习处理遥感影像的过程中，经常在数据集上遇到各种问题：</p><p> 找不到针对自己任务的数据集<br> 数据集网址404或龟速下载<br> 数据集在空间分辨率、图像尺寸等方面不符合需求</p><p>现在，<strong>遥感影像数据集汇总</strong> 项目可以助你解决上述问题。<br>该项目是一个遥感影像领域常用的深度学习数据集的汇总，包括数据集<strong>基本信息</strong>，并附上数据集<strong>源地址</strong>与<strong>Aistudi备份链接（包括详细的类别信息，提供高速下载）</strong>。</p><p>目前本项目共收录</p><ul><li><p>图像分类数据集27个（整理完结）；</p></li><li><p>目标检测数据集31+个（整理完结）；</p></li><li><p>图像分割数据集36+个（整理完结）；</p></li><li><p>变化数据集14个；</p></li><li><p>高光谱分类3个（整理完结）；</p></li><li><p>高光谱检测2个（整理完结）；</p></li><li><p>高光谱分割8个（整理完结）；</p></li><li><p>多标签分类2个（整理完结）；</p></li><li><p>视频跟踪1个（整理完结）；</p></li></ul><h1 id="遥感影像场景分类"><a href="#遥感影像场景分类" class="headerlink" title="遥感影像场景分类"></a>遥感影像场景分类</h1><p><strong>收集网络中开源的、关于遥感影像 场景分类的深度学习数据集</strong></p><p>main ref </p><blockquote><p><a href="https://captain-whu.github.io/DiRS/">https://captain-whu.github.io/DiRS/</a><br><a href="https://mp.weixin.qq.com/s/kThlSqItIwuCFTIfC_-5lQ">https://mp.weixin.qq.com/s/kThlSqItIwuCFTIfC_-5lQ</a></p></blockquote><h2 id="Moving-and-Stationary-TargetAcquisition-and-Recognition-MSTAR-8class"><a href="#Moving-and-Stationary-TargetAcquisition-and-Recognition-MSTAR-8class" class="headerlink" title="Moving and Stationary TargetAcquisition and Recognition (MSTAR) 8class"></a>Moving and Stationary TargetAcquisition and Recognition (MSTAR) 8class</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">368 * 368 * 3</td><td>8</td><td>9466</td><td>0.3m</td><td>STARLOS SAR</td><td>1996</td><td>Defense Advanced Research Projects Agency and the Air Force Research Laboratory</td></tr></tbody></table><ul><li>源地址：<a href="https://www.kaggle.com/atreyamajumdar/mstar-dataset-8-classes">https://www.kaggle.com/atreyamajumdar/mstar-dataset-8-classes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/78849">https://aistudio.baidu.com/aistudio/datasetdetail/78849</a></li></ul><h2 id="UCMerced-LandUse"><a href="#UCMerced-LandUse" class="headerlink" title="UCMerced_LandUse"></a>UCMerced_LandUse</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>21</td><td>21 * 100 = 2100</td><td>1 foot</td><td>USGS</td><td>2010</td><td>加利福尼亚大学</td></tr></tbody></table><ul><li>源地址：<a href="http://weegee.vision.ucmerced.edu/datasets/landuse.html">http://weegee.vision.ucmerced.edu/datasets/landuse.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/51628">https://aistudio.baidu.com/aistudio/datasetdetail/51628</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1ryrLb5HnCw0Uxzbpva9Vnw">https://pan.baidu.com/s/1ryrLb5HnCw0Uxzbpva9Vnw</a>  提取码：n71j</li></ul><h2 id="WHU-RS19"><a href="#WHU-RS19" class="headerlink" title="WHU-RS19"></a>WHU-RS19</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">600 * 600 * 3</td><td>19</td><td>19 * 50± = 1005</td><td>最高0.5 m</td><td>GoogleEarth</td><td>2012</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="http://captain.whu.edu.cn/repository.html">http://captain.whu.edu.cn/repository.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/51733">https://aistudio.baidu.com/aistudio/datasetdetail/51733</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1-w68vcKpj0NYlOltEWdi6w">https://pan.baidu.com/s/1-w68vcKpj0NYlOltEWdi6w</a>  提取码：gpjd</li></ul><h2 id="RSSCN7"><a href="#RSSCN7" class="headerlink" title="RSSCN7"></a>RSSCN7</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">400 * 400 * 3</td><td>7</td><td>7 * 400 = 2800</td><td>未知</td><td>GoogleEarth</td><td>2015</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://sites.google.com/site/qinzoucn/documents">https://sites.google.com/site/qinzoucn/documents</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52117">https://aistudio.baidu.com/aistudio/datasetdetail/52117</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1wS3-TgykUS5svytVsJnU-Q">https://pan.baidu.com/s/1wS3-TgykUS5svytVsJnU-Q</a>  提取码：ppy1</li></ul><h2 id="RS-C11"><a href="#RS-C11" class="headerlink" title="RS_C11"></a>RS_C11</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>11</td><td>10 * 100± = 1232</td><td>约0.2m</td><td>GoogleEarth</td><td>2016</td><td>中科院</td></tr></tbody></table><ul><li>源地址：<a href="https://www.researchgate.net/publication/271647282_RS_C11_Database/comments">https://www.researchgate.net/publication/271647282_RS_C11_Database/comments</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52227">https://aistudio.baidu.com/aistudio/datasetdetail/52227</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1sOIaya_CibyZhx2RTd3C_Q">https://pan.baidu.com/s/1sOIaya_CibyZhx2RTd3C_Q</a>  提取码：hdnr</li></ul><h2 id="NWPU-RESISC45"><a href="#NWPU-RESISC45" class="headerlink" title="NWPU-RESISC45"></a>NWPU-RESISC45</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>45</td><td>45 * 700 = 31500</td><td>0.2~30m</td><td>GoogleEarth</td><td>2016</td><td>西北工业大学</td></tr></tbody></table><ul><li>源地址：<a href="http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.html">http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/51873">https://aistudio.baidu.com/aistudio/datasetdetail/51873</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1gkvrsiN4r3CpAl3k-UfIkA">https://pan.baidu.com/s/1gkvrsiN4r3CpAl3k-UfIkA</a>  提取码：3n0g</li></ul><h2 id="AID"><a href="#AID" class="headerlink" title="AID"></a>AID</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">600 * 600 * 3</td><td>30</td><td>30 * 300± = 10000</td><td>0.5~8m</td><td>GoogleEarth</td><td>2017</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://captain-whu.github.io/AID/">https://captain-whu.github.io/AID/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52025">https://aistudio.baidu.com/aistudio/datasetdetail/52025</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1dWMqt6y8Xe7g2gjpygOALQ">https://pan.baidu.com/s/1dWMqt6y8Xe7g2gjpygOALQ</a>  提取码：svzw</li></ul><h2 id="GID"><a href="#GID" class="headerlink" title="GID"></a>GID</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">56 * 56 * 4<br>56 * 56 * 3</td><td>15</td><td>15 * 2000 = 30000</td><td>未知</td><td>高分2</td><td>2018</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://x-ytong.github.io/project/GID.html">https://x-ytong.github.io/project/GID.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55324">https://aistudio.baidu.com/aistudio/datasetdetail/55324</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1Xux-3nKTbu1v3Mp5tdyM7A">https://pan.baidu.com/s/1Xux-3nKTbu1v3Mp5tdyM7A</a>  提取码：d3qs</li></ul><h2 id="RSD46-WHU"><a href="#RSD46-WHU" class="headerlink" title="RSD46-WHU"></a>RSD46-WHU</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>46</td><td>117000</td><td>0.5~2m</td><td>GoogleEarth、天地图</td><td>2017</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/RSIA-LIESMARS-WHU/RSD46-WHU">https://github.com/RSIA-LIESMARS-WHU/RSD46-WHU</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52359">https://aistudio.baidu.com/aistudio/datasetdetail/52359</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1WNR8wV86jl_g04JX7bCpNQ">https://pan.baidu.com/s/1WNR8wV86jl_g04JX7bCpNQ</a>  提取码：gn2a</li></ul><h2 id="PatternNet"><a href="#PatternNet" class="headerlink" title="PatternNet"></a>PatternNet</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>38</td><td>38 * 800 = 30400</td><td>0.062~4.693m</td><td>GoogleMap</td><td>2018</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://sites.google.com/view/zhouwx/dataset#h.p_Tgef10WTuEFr">https://sites.google.com/view/zhouwx/dataset#h.p_Tgef10WTuEFr</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52411">https://aistudio.baidu.com/aistudio/datasetdetail/52411</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1_1KgQFi7QaUp8p6dz3TbWw">https://pan.baidu.com/s/1_1KgQFi7QaUp8p6dz3TbWw</a>  提取码：j2y2</li></ul><h2 id="AIRS"><a href="#AIRS" class="headerlink" title="AIRS"></a>AIRS</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">10000 * 10000 * 3</td><td>1</td><td>857 train，94 val， 96 test</td><td>0.075m</td><td>LINZ Data Service</td><td>2018</td><td>University of Tokyo等</td></tr></tbody></table><ul><li>源地址：<a href="https://www.airs-dataset.com/">https://www.airs-dataset.com/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/74274">https://aistudio.baidu.com/aistudio/datasetdetail/74274</a></li></ul><h2 id="Satellite-Images-of-Hurricane-Damage"><a href="#Satellite-Images-of-Hurricane-Damage" class="headerlink" title="Satellite Images of Hurricane Damage"></a>Satellite Images of Hurricane Damage</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">128 * 128 * 3</td><td>2</td><td>23000</td><td>未知</td><td>2018</td><td>University of Washington等</td></tr></tbody></table><ul><li>源地址：<a href="https://www.kaggle.com/kmader/satellite-images-of-hurricane-damage">https://www.kaggle.com/kmader/satellite-images-of-hurricane-damage</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/88155">https://aistudio.baidu.com/aistudio/datasetdetail/88155</a></li></ul><h2 id="How-to-make-high-resolution-remote-sensing-image-dataset"><a href="#How-to-make-high-resolution-remote-sensing-image-dataset" class="headerlink" title="How-to-make-high-resolution-remote-sensing-image-dataset"></a>How-to-make-high-resolution-remote-sensing-image-dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>数据源</th><th>发布时间</th><th>空间分辨率</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>5</td><td>533</td><td>谷歌地球</td><td>2018</td><td>0.075m</td></tr></tbody></table><ul><li>源地址：<a href="https://blog.csdn.net/u012193416/article/details/79472533">https://blog.csdn.net/u012193416/article/details/79472533</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/88597">https://aistudio.baidu.com/aistudio/datasetdetail/88597</a></li></ul><h2 id="OPTIMAL-31"><a href="#OPTIMAL-31" class="headerlink" title="OPTIMAL-31"></a>OPTIMAL-31</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>31</td><td>31* 60 = 1860</td><td>未知</td><td>GoogleEarth</td><td>2019</td><td>西北工业大学</td></tr></tbody></table><ul><li>源地址：<a href="http://crabwq.github.io/">http://crabwq.github.io/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/51798">https://aistudio.baidu.com/aistudio/datasetdetail/51798</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1ETHM44DuZYpKkUTqTxs4-w">https://pan.baidu.com/s/1ETHM44DuZYpKkUTqTxs4-w</a>  提取码：1dyd</li></ul><h2 id="WiDS-Datathon-2019"><a href="#WiDS-Datathon-2019" class="headerlink" title="WiDS Datathon 2019"></a>WiDS Datathon 2019</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>1</td><td>11000train</td><td>Planet</td><td>2019</td><td>Stanford</td><td>3m</td></tr></tbody></table><ul><li>源地址：<a href="https://www.kaggle.com/c/widsdatathon2019/data">https://www.kaggle.com/c/widsdatathon2019/data</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/76927">https://aistudio.baidu.com/aistudio/datasetdetail/76927</a></li></ul><h2 id="Continual-Learning-Benchmark-for-Remote-Sensing-Image-Scene-Classification-CLRS"><a href="#Continual-Learning-Benchmark-for-Remote-Sensing-Image-Scene-Classification-CLRS" class="headerlink" title="Continual Learning Benchmark for Remote Sensing Image Scene Classification (CLRS)"></a>Continual Learning Benchmark for Remote Sensing Image Scene Classification (CLRS)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>25</td><td>15,000</td><td>Google Earth, Bing Map, Google Map, and Tianditu</td><td>2020</td><td>中南大学</td><td>0.26 m to 8.85 m</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/lehaifeng/CLRS">https://github.com/lehaifeng/CLRS</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/76417">https://aistudio.baidu.com/aistudio/datasetdetail/76417</a></li></ul><h2 id="SenseEarth-Classify"><a href="#SenseEarth-Classify" class="headerlink" title="SenseEarth Classify"></a>SenseEarth Classify</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">100 * 100 ~ 12655 * 12655 * 3</td><td>28</td><td>~70000</td><td>未知</td><td>2020</td><td>商汤科技</td><td>0.2~153m</td></tr></tbody></table><ul><li>源地址：<a href="https://rs.sensetime.com/competition/index.html#/info">https://rs.sensetime.com/competition/index.html#/info</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52728">https://aistudio.baidu.com/aistudio/datasetdetail/52728</a></li></ul><h2 id="Multi-View-Datasets，CV-BrCT"><a href="#Multi-View-Datasets，CV-BrCT" class="headerlink" title="Multi-View Datasets，CV-BrCT"></a>Multi-View Datasets，CV-BrCT</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500 * 475 * 3</td><td>8</td><td>24171 * 2</td><td>航空影像及地面街景影像</td><td>2020</td><td>Federal University of Minas Gerais</td></tr></tbody></table><ul><li>源地址：<a href="http://www.patreo.dcc.ufmg.br/2020/07/22/multi-view-datasets/">http://www.patreo.dcc.ufmg.br/2020/07/22/multi-view-datasets/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/58013">https://aistudio.baidu.com/aistudio/datasetdetail/58013</a></li></ul><h2 id="Multi-View-Datasets，AiRound"><a href="#Multi-View-Datasets，AiRound" class="headerlink" title="Multi-View Datasets，AiRound"></a>Multi-View Datasets，AiRound</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500 * 475 * 3</td><td>11</td><td>3495 * 4</td><td>Sentinel-2等多源数据</td><td>2020</td><td>Federal University of Minas Gerais</td></tr></tbody></table><ul><li>源地址：<a href="http://www.patreo.dcc.ufmg.br/2020/07/22/multi-view-datasets/">http://www.patreo.dcc.ufmg.br/2020/07/22/multi-view-datasets/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/58760">https://aistudio.baidu.com/aistudio/datasetdetail/58760</a></li></ul><h2 id="SIRI-WHU"><a href="#SIRI-WHU" class="headerlink" title="SIRI-WHU"></a>SIRI-WHU</h2><h3 id="SIRI-WHU：google"><a href="#SIRI-WHU：google" class="headerlink" title="SIRI-WHU：google"></a>SIRI-WHU：google</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">200 * 200 * 3</td><td>12</td><td>12 * 200 = 2400</td><td>2m</td><td>GoogleEarth</td><td>2016</td><td>武汉大学</td></tr></tbody></table><h3 id="SIRI-WHU：USGS"><a href="#SIRI-WHU：USGS" class="headerlink" title="SIRI-WHU：USGS"></a>SIRI-WHU：USGS</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">10000 * 9000 * 3</td><td>4</td><td>1</td><td>2 foot</td><td>USGS</td><td>2016</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="http://www.lmars.whu.edu.cn/prof_web/zhongyanfei/e-code.html">http://www.lmars.whu.edu.cn/prof_web/zhongyanfei/e-code.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/51921">https://aistudio.baidu.com/aistudio/datasetdetail/51921</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/14C4Km92FekHy2dWsrXLckA">https://pan.baidu.com/s/14C4Km92FekHy2dWsrXLckA</a>  提取码：6259</li></ul><h2 id="RSI-CB"><a href="#RSI-CB" class="headerlink" title="RSI-CB"></a>RSI-CB</h2><h3 id="RSI-CB128"><a href="#RSI-CB128" class="headerlink" title="RSI-CB128"></a>RSI-CB128</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>45</td><td>36000</td><td>0.3–3m</td><td>多源</td><td>2017</td><td>中南大学</td></tr></tbody></table><h3 id="RSI-CB256"><a href="#RSI-CB256" class="headerlink" title="RSI-CB256"></a>RSI-CB256</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>35</td><td>24000</td><td>0.3–3m</td><td>多源</td><td>2017</td><td>中南大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/lehaifeng/RSI-CB">https://github.com/lehaifeng/RSI-CB</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52487">https://aistudio.baidu.com/aistudio/datasetdetail/52487</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1f77k5g8mhsSzxPantyxemA">https://pan.baidu.com/s/1f77k5g8mhsSzxPantyxemA</a>  提取码：246r</li></ul><h2 id="SAT"><a href="#SAT" class="headerlink" title="SAT"></a>SAT</h2><h3 id="SAT-4"><a href="#SAT-4" class="headerlink" title="SAT-4"></a>SAT-4</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>备注</th></tr></thead><tbody><tr><td align="left">28 * 28 * 4</td><td>4</td><td>500,000</td><td>1-6m</td><td>NAIP dataset</td><td>2015</td><td>路易斯安那州立大学与NASA</td><td>采用MATLAB的.mat数据存储格式</td></tr></tbody></table><h3 id="SAT-6"><a href="#SAT-6" class="headerlink" title="SAT-6"></a>SAT-6</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>备注</th></tr></thead><tbody><tr><td align="left">28 * 28 * 4</td><td>6</td><td>405,000</td><td>1-6m</td><td>NAIP dataset</td><td>2015</td><td>路易斯安那州立大学与NASA</td><td>采用MATLAB的.mat数据存储格式</td></tr></tbody></table><ul><li>源地址：<a href="http://csc.lsu.edu/~saikat/deepsat/">http://csc.lsu.edu/~saikat/deepsat/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52534">https://aistudio.baidu.com/aistudio/datasetdetail/52534</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1LKCAPAz-DVGTkjQ1DMS0_Q">https://pan.baidu.com/s/1LKCAPAz-DVGTkjQ1DMS0_Q</a>  提取码：12du</li></ul><h2 id="V-RSIR"><a href="#V-RSIR" class="headerlink" title="V-RSIR"></a>V-RSIR</h2><h3 id="rs-VArcGIS"><a href="#rs-VArcGIS" class="headerlink" title="rs VArcGIS"></a>rs VArcGIS</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>38</td><td>59071</td><td>0.07 ~ 19.11m</td><td>ArcGIS World Imagery</td><td>2020</td><td>中南大学</td></tr></tbody></table><h3 id="rs-VBing"><a href="#rs-VBing" class="headerlink" title="rs VBing"></a>rs VBing</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>38</td><td>58944</td><td>0.07 ~ 38.22m</td><td>Bing World Imagery</td><td>2020</td><td>中南大学</td></tr></tbody></table><h3 id="rs-VGoogle"><a href="#rs-VGoogle" class="headerlink" title="rs VGoogle"></a>rs VGoogle</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>38</td><td>59404</td><td>0.075 ~ 9.555m</td><td>VGoogle</td><td>2020</td><td>中南大学</td></tr></tbody></table><ul><li>源地址：<br><a href="https://aistudio.baidu.com/aistudio/datasetdetail/47630">https://aistudio.baidu.com/aistudio/datasetdetail/47630</a><br><a href="https://aistudio.baidu.com/aistudio/datasetdetail/47638">https://aistudio.baidu.com/aistudio/datasetdetail/47638</a><br><a href="https://aistudio.baidu.com/aistudio/datasetdetail/47619">https://aistudio.baidu.com/aistudio/datasetdetail/47619</a></li></ul><h2 id="rs-sensetime"><a href="#rs-sensetime" class="headerlink" title="rs sensetime"></a>rs sensetime</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">100 * 100 * 3 ~ <br>12655 * 12655 * 3</td><td>50</td><td>60040</td><td>0.2~153m</td><td>未知</td><td>2020</td><td>商汤科技SenseEarth</td></tr></tbody></table><ul><li>源地址：<a href="https://rs.sensetime.com/competition/index.html#/data">https://rs.sensetime.com/competition/index.html#/data</a></li></ul><h2 id="rscup"><a href="#rscup" class="headerlink" title="rscup"></a>rscup</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>45</td><td>197,121</td><td>未知</td><td>2019</td><td>rscup组委会</td></tr></tbody></table><ul><li>源地址：<a href="http://rscup.bjxintong.com.cn/#/theme/1">http://rscup.bjxintong.com.cn/#/theme/1</a></li></ul><h2 id="Planet-Understanding-the-Amazon-from-Space"><a href="#Planet-Understanding-the-Amazon-from-Space" class="headerlink" title="Planet: Understanding the Amazon from Space"></a>Planet: Understanding the Amazon from Space</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>备注</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3<br>256 * 256 * 4</td><td>17</td><td>40480</td><td>3m</td><td>plant传感器</td><td>2017</td><td>Planet、SCCON公司</td><td>多标签</td></tr></tbody></table><ul><li>源地址：<a href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data">https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data</a></li></ul><h2 id="brazilian-coffee-scenes"><a href="#brazilian-coffee-scenes" class="headerlink" title="brazilian coffee scenes"></a>brazilian coffee scenes</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">64 * 64 * 3</td><td>2</td><td>2,876</td><td>未知</td><td>SPOT传感器</td><td>2015</td><td>米纳斯联邦大学</td></tr></tbody></table><ul><li>源地址：<a href="http://patreo.dcc.ufmg.br/2017/11/12/brazilian-coffee-scenes-dataset">http://patreo.dcc.ufmg.br/2017/11/12/brazilian-coffee-scenes-dataset</a></li></ul><h1 id="遥感影像目标检测"><a href="#遥感影像目标检测" class="headerlink" title="遥感影像目标检测"></a>遥感影像目标检测</h1><p><strong>收集网络中开源的、关于遥感影像 目标检测的深度学习数据集</strong></p><p>main ref </p><blockquote><p><a href="https://captain-whu.github.io/DiRS/">https://captain-whu.github.io/DiRS/</a><br><a href="https://mp.weixin.qq.com/s/QGIFeriC3s0xvok-TVhmPQ">https://mp.weixin.qq.com/s/QGIFeriC3s0xvok-TVhmPQ</a><br><a href="https://zhuanlan.zhihu.com/p/113579163">https://zhuanlan.zhihu.com/p/113579163</a></p></blockquote><h2 id="TAS"><a href="#TAS" class="headerlink" title="TAS"></a>TAS</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">792 * 636 * 3</td><td>1</td><td>30</td><td>1,319</td><td>HBB</td><td>Google Earth</td><td>2008</td><td>斯坦福大学</td></tr></tbody></table><ul><li>源地址：<a href="http://ai.stanford.edu/~gaheitz/Research/TAS/">http://ai.stanford.edu/~gaheitz/Research/TAS/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53806">https://aistudio.baidu.com/aistudio/datasetdetail/53806</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1jGUThjNj3GsU0EIAQlQaiw%20">https://pan.baidu.com/s/1jGUThjNj3GsU0EIAQlQaiw</a> 提取码：t7lo</li></ul><h2 id="OIRDS"><a href="#OIRDS" class="headerlink" title="OIRDS"></a>OIRDS</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256<del>640 * 256</del>640 * 3</td><td>5</td><td>900</td><td>1800</td><td>约0.15m</td><td>OBB</td><td>USGS、DARPA、VIVID</td><td>2009</td><td>雷神公司等</td></tr></tbody></table><ul><li>源地址：<a href="https://sourceforge.net/projects/oirds/">https://sourceforge.net/projects/oirds/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53461">https://aistudio.baidu.com/aistudio/datasetdetail/53461</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/173yxlf279R8EESlT7mxbPw">https://pan.baidu.com/s/173yxlf279R8EESlT7mxbPw</a>  提取码：k8di</li></ul><h2 id="SZTAKI-INRIA-Building-Detection-Benchmark"><a href="#SZTAKI-INRIA-Building-Detection-Benchmark" class="headerlink" title="SZTAKI-INRIA Building Detection Benchmark"></a>SZTAKI-INRIA Building Detection Benchmark</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">700± * 700± * 3</td><td>1</td><td>9</td><td>665</td><td>OBB</td><td>未知</td><td>2012</td><td>MTA SZTAKI</td></tr></tbody></table><ul><li>源地址：<a href="http://web.eee.sztaki.hu/remotesensing/building_benchmark.html">http://web.eee.sztaki.hu/remotesensing/building_benchmark.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/77674">https://aistudio.baidu.com/aistudio/datasetdetail/77674</a></li></ul><h2 id="UCAS-AOD"><a href="#UCAS-AOD" class="headerlink" title="UCAS_AOD"></a>UCAS_AOD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1000± * 1000± * 3</td><td>2</td><td>976</td><td>6,950</td><td>OBB</td><td>Google Earth</td><td>2014</td><td>中科院</td></tr></tbody></table><ul><li>源地址：<a href="https://onedrive.hyper.ai/home/UCAS-AOD">https://onedrive.hyper.ai/home/UCAS-AOD</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53318">https://aistudio.baidu.com/aistudio/datasetdetail/53318</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1E9L-S6baxLDjSDTYlm985g">https://pan.baidu.com/s/1E9L-S6baxLDjSDTYlm985g</a>  提取码：sd6l</li></ul><h2 id="NWPU-VHR-10"><a href="#NWPU-VHR-10" class="headerlink" title="NWPU VHR-10"></a>NWPU VHR-10</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500<del>1100 * 500</del>1100 * 3</td><td>10</td><td>1510</td><td>14,596</td><td>HBB</td><td>Google Earth、Vaihingen</td><td>2014</td><td>西北工业大学</td></tr></tbody></table><ul><li>源地址：<a href="http://www.escience.cn/people/gongcheng/NWPU-VHR-10.html">http://www.escience.cn/people/gongcheng/NWPU-VHR-10.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52812">https://aistudio.baidu.com/aistudio/datasetdetail/52812</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1Wm73acTD1WfBM_YZlKMjXw">https://pan.baidu.com/s/1Wm73acTD1WfBM_YZlKMjXw</a>  提取码：35wf</li></ul><h2 id="HRSC2016"><a href="#HRSC2016" class="headerlink" title="HRSC2016"></a>HRSC2016</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1100± * 1100± * 3</td><td>27</td><td>1,061</td><td>2,976</td><td>OBB</td><td>Google Earth</td><td>2016</td><td>西北工业大学</td></tr></tbody></table><ul><li>源地址：<a href="https://sites.google.com/site/hrsc2016/">https://sites.google.com/site/hrsc2016/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54106">https://aistudio.baidu.com/aistudio/datasetdetail/54106</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1BO2UX6dICVa33qUqIMCFrw">https://pan.baidu.com/s/1BO2UX6dICVa33qUqIMCFrw</a>  提取码：shbp</li></ul><h2 id="DLR3k"><a href="#DLR3k" class="headerlink" title="DLR3k"></a>DLR3k</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">5616 * 3744 * 3</td><td>7</td><td>20</td><td>14,235</td><td>0.13m</td><td>OBB</td><td>无人机(Canon Eos 1Ds Mark III)</td><td>2016</td><td>德国航天航空中心</td></tr></tbody></table><ul><li>源地址：<a href="https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-12760/22294_read-52777">https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-12760/22294_read-52777</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/xxxxx">https://aistudio.baidu.com/aistudio/datasetdetail/xxxxx</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1hIbSQd-XdHxpmYIdFYHCQw%20">https://pan.baidu.com/s/1hIbSQd-XdHxpmYIdFYHCQw</a> 提取码：1lpx</li></ul><h2 id="RSOD"><a href="#RSOD" class="headerlink" title="RSOD"></a>RSOD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1044<del>1288 * 915</del>992 * 3</td><td>4</td><td>976</td><td>6,950</td><td>HBB</td><td>Google Earth、天地图</td><td>2017</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/RSIA-LIESMARS-WHU/RSOD-Dataset-">https://github.com/RSIA-LIESMARS-WHU/RSOD-Dataset-</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52980">https://aistudio.baidu.com/aistudio/datasetdetail/52980</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1lUooUOJE2QCpCXSBrzKysQ">https://pan.baidu.com/s/1lUooUOJE2QCpCXSBrzKysQ</a>  提取码：pqih</li></ul><h2 id="TGRS-HRRSD"><a href="#TGRS-HRRSD" class="headerlink" title="TGRS-HRRSD"></a>TGRS-HRRSD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">152<del>10569 * 152</del>10569 * 3</td><td>13</td><td>21,761</td><td>55,740</td><td>0.15 ~ 1.2m</td><td>HBB</td><td>Google Earth、百度地图</td><td>2017</td><td>中科院</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/CrazyStoneonRoad/TGRS-HRRSD-Dataset">https://github.com/CrazyStoneonRoad/TGRS-HRRSD-Dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53186">https://aistudio.baidu.com/aistudio/datasetdetail/53186</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1hru_vjugopeJCXT7SguhcQ%20">https://pan.baidu.com/s/1hru_vjugopeJCXT7SguhcQ</a> 提取码：row5</li></ul><h2 id="SSDD"><a href="#SSDD" class="headerlink" title="SSDD"></a>SSDD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500 * 500 * 3</td><td>1</td><td>1160</td><td>2456</td><td>1~15m</td><td>HBB、OBB</td><td>RadarSat-2、TerraSAR-X、Sentinel-1</td><td>2017</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://zhuanlan.zhihu.com/p/58404659">https://zhuanlan.zhihu.com/p/58404659</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54806">https://aistudio.baidu.com/aistudio/datasetdetail/54806</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1tcKcCTjUXJJW--erUFyzqg%20">https://pan.baidu.com/s/1tcKcCTjUXJJW--erUFyzqg</a> 提取码：hb0d</li></ul><h2 id="OpenSARShip"><a href="#OpenSARShip" class="headerlink" title="OpenSARShip"></a>OpenSARShip</h2><table><thead><tr><th align="left">类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1</td><td>41</td><td>11346</td><td>Chip</td><td>Sentinel-1</td><td>2017</td><td>上海交通大学</td></tr></tbody></table><ul><li>源地址：<a href="http://opensar.sjtu.edu.cn/">http://opensar.sjtu.edu.cn/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/77017">https://aistudio.baidu.com/aistudio/datasetdetail/77017</a></li></ul><h2 id="ships-in-satellite-imagery"><a href="#ships-in-satellite-imagery" class="headerlink" title="ships in satellite imagery"></a>ships in satellite imagery</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">80 * 80 * 3</td><td>1</td><td>4000</td><td>1000+</td><td>Planet</td><td>2017</td><td>Planet Team</td><td>3m</td></tr></tbody></table><ul><li>源地址：<a href="https://www.kaggle.com/rhammell/ships-in-satellite-imagery">https://www.kaggle.com/rhammell/ships-in-satellite-imagery</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/77233">https://aistudio.baidu.com/aistudio/datasetdetail/77233</a></li></ul><h2 id="LEVIR"><a href="#LEVIR" class="headerlink" title="LEVIR"></a>LEVIR</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800 * 600 * 3</td><td>3</td><td>22,000</td><td>11,000</td><td>0.2∼1.0m</td><td>HBB</td><td>Google Earth</td><td>2018</td><td>北京航天航空大学</td></tr></tbody></table><ul><li>源地址：<a href="http://levir.buaa.edu.cn/Code.htm">http://levir.buaa.edu.cn/Code.htm</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53714">https://aistudio.baidu.com/aistudio/datasetdetail/53714</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1kfTozB7SHIH1ioaJdzHwFA%20">https://pan.baidu.com/s/1kfTozB7SHIH1ioaJdzHwFA</a> 提取码：eydb</li></ul><h2 id="VisDrone2019-DET"><a href="#VisDrone2019-DET" class="headerlink" title="VisDrone2019-DET"></a>VisDrone2019-DET</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500<del>2,000* 500</del>2,000 * 3</td><td>10</td><td>10,209</td><td>54,200</td><td>HBB</td><td>无人机数据</td><td>2018</td><td>天津大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/VisDrone/VisDrone-Dataset">https://github.com/VisDrone/VisDrone-Dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54054">https://aistudio.baidu.com/aistudio/datasetdetail/54054</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1ZvWrcRimr-UMU1QI1P6Y8g">https://pan.baidu.com/s/1ZvWrcRimr-UMU1QI1P6Y8g</a>  提取码：fcjs</li></ul><h2 id="MASATI"><a href="#MASATI" class="headerlink" title="MASATI"></a>MASATI</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">512± * 512± * 3</td><td>7</td><td>7,389</td><td>未知</td><td>HBB</td><td>Bing Maps</td><td>2018</td><td>阿利坎特大学</td></tr></tbody></table><ul><li>源地址：<a href="https://www.iuii.ua.es/datasets/masati/">https://www.iuii.ua.es/datasets/masati/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53895">https://aistudio.baidu.com/aistudio/datasetdetail/53895</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1t84BiCmTY8PTBnjBGf0gbQ%20">https://pan.baidu.com/s/1t84BiCmTY8PTBnjBGf0gbQ</a> 提取码：npwo</li></ul><h2 id="ITCVD"><a href="#ITCVD" class="headerlink" title="ITCVD"></a>ITCVD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>备注</th></tr></thead><tbody><tr><td align="left">5616 * 3744 * 3</td><td>1</td><td>135</td><td>23543</td><td>0.1 m</td><td>HBB</td><td>航拍影像</td><td>2018</td><td>University of Twente Research Information</td><td>标注为mat格式</td></tr></tbody></table><ul><li>源地址：<a href="https://research.utwente.nl/en/datasets/itcvd-dataset">https://research.utwente.nl/en/datasets/itcvd-dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54674">https://aistudio.baidu.com/aistudio/datasetdetail/54674</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1TA2_HU3qb96-lXkZJ5BtEg%20">https://pan.baidu.com/s/1TA2_HU3qb96-lXkZJ5BtEg</a> 提取码：6hht</li></ul><h2 id="DIOR"><a href="#DIOR" class="headerlink" title="DIOR"></a>DIOR</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800 * 800 * 3</td><td>20</td><td>23463</td><td>190288</td><td>0.5 ~ 30 m</td><td>HBB</td><td>Google Earth</td><td>2019</td><td>西北工业大学</td></tr></tbody></table><ul><li>源地址：<a href="http://www.escience.cn/people/JunweiHan/DIOR.html">http://www.escience.cn/people/JunweiHan/DIOR.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53045">https://aistudio.baidu.com/aistudio/datasetdetail/53045</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1Z3xJC_l5MpGSFuW0u4shuQ">https://pan.baidu.com/s/1Z3xJC_l5MpGSFuW0u4shuQ</a>  提取码：vt56</li></ul><h2 id="AIR-SARShip"><a href="#AIR-SARShip" class="headerlink" title="AIR-SARShip"></a>AIR-SARShip</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1000 * 1000 * 3</td><td>1</td><td>300</td><td>2040</td><td>1~3m</td><td>HBB</td><td>高分3</td><td>2019</td><td>《雷达学报》编辑部</td></tr></tbody></table><ul><li>源地址：<a href="http://radars.ie.ac.cn/web/data/getData?dataType=SARDataset">http://radars.ie.ac.cn/web/data/getData?dataType=SARDataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54270">https://aistudio.baidu.com/aistudio/datasetdetail/54270</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1NnLVijCr0BN8CIg86srDUA">https://pan.baidu.com/s/1NnLVijCr0BN8CIg86srDUA</a>  提取码：bfri</li></ul><h2 id="SAR-Ship"><a href="#SAR-Ship" class="headerlink" title="SAR-Ship"></a>SAR-Ship</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>1</td><td>210</td><td>43,819</td><td>1.7~25m</td><td>HBB</td><td>高分3、哨兵1</td><td>2019</td><td>中科院</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/CAESAR-Radi/SAR-Ship-Dataset">https://github.com/CAESAR-Radi/SAR-Ship-Dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54361">https://aistudio.baidu.com/aistudio/datasetdetail/54361</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/13MViO0EdhCZSNlesF-32Bg%20">https://pan.baidu.com/s/13MViO0EdhCZSNlesF-32Bg</a> 提取码：s0qf</li></ul><h2 id="iSAID"><a href="#iSAID" class="headerlink" title="iSAID"></a>iSAID</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800<del>13, 000 * 800</del>13, 000 * 3</td><td>15</td><td>2,806</td><td>655,451</td><td>OBB</td><td>Google Earth、JL-1、GF-2</td><td>2019</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://captain-whu.github.io/iSAID/index.html">https://captain-whu.github.io/iSAID/index.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/73458">https://aistudio.baidu.com/aistudio/datasetdetail/73458</a></li></ul><h2 id="Bridge-Dataset"><a href="#Bridge-Dataset" class="headerlink" title="Bridge Dataset"></a>Bridge Dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4800 * 2843 * 3</td><td>1</td><td>500</td><td>500+</td><td>HBB</td><td>Google Earth、 OpenStreetMap</td><td>2019</td><td>Federal University of Minas Gerais</td></tr></tbody></table><ul><li>源地址：<a href="http://www.patreo.dcc.ufmg.br/2019/07/10/bridge-dataset/">http://www.patreo.dcc.ufmg.br/2019/07/10/bridge-dataset/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/57457">https://aistudio.baidu.com/aistudio/datasetdetail/57457</a></li></ul><h2 id="HRSID"><a href="#HRSID" class="headerlink" title="HRSID"></a>HRSID</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800 * 800 * 3</td><td>1</td><td>5604</td><td>16951</td><td>0.5~3 m</td><td>HBB</td><td>Sentinel-1B、TerraSAR-X、TanDEM-X</td><td>2020</td><td>电子科技大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/chaozhong2010/HRSID">https://github.com/chaozhong2010/HRSID</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54512">https://aistudio.baidu.com/aistudio/datasetdetail/54512</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/13MViO0EdhCZSNlesF-32Bg%20">https://pan.baidu.com/s/13MViO0EdhCZSNlesF-32Bg</a> 提取码：s0qf</li></ul><h2 id="RarePlanes"><a href="#RarePlanes" class="headerlink" title="RarePlanes"></a>RarePlanes</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>10</td><td>1244 train，263 test</td><td>~14,700</td><td>HBB</td><td>WorldView-3</td><td>2020</td><td>In-Q-Tel、AI.Reverie</td><td>0.3~1.5m</td></tr></tbody></table><ul><li>源地址：<a href="https://www.cosmiqworks.org/RarePlanes/">https://www.cosmiqworks.org/RarePlanes/</a><br><a href="https://www.graviti.cn/open-datasets/RarePlanes">https://www.graviti.cn/open-datasets/RarePlanes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/70215">https://aistudio.baidu.com/aistudio/datasetdetail/70215</a></li></ul><h2 id="FAIR1M"><a href="#FAIR1M" class="headerlink" title="FAIR1M"></a>FAIR1M</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">1000<del>10000 * 1000</del>10000 * 3</td><td>36</td><td>15000+</td><td>100 0000+</td><td>OBB</td><td>Gaofen satellites、Google Earth</td><td>2021</td><td>中科院等</td><td>0.3 ~ 0.8m</td></tr></tbody></table><ul><li>源地址：<a href="http://gaofen-challenge.com/">http://gaofen-challenge.com</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/78453">https://aistudio.baidu.com/aistudio/datasetdetail/78453</a></li></ul><h2 id="VEDAI"><a href="#VEDAI" class="headerlink" title="VEDAI"></a>VEDAI</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">512 * 512 * 4<br>1024 * 1024 * 4</td><td>9</td><td>1210</td><td>3640</td><td>0.125m</td><td>OBB</td><td>Utah AGRC</td><td>2015</td><td>卡昂大学</td></tr></tbody></table><ul><li>源地址：<a href="https://downloads.greyc.fr/vedai/">https://downloads.greyc.fr/vedai/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53383">https://aistudio.baidu.com/aistudio/datasetdetail/53383</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1t3qCIUnEpJ1lIUTZpjlCyw">https://pan.baidu.com/s/1t3qCIUnEpJ1lIUTZpjlCyw</a>  提取码：doid</li></ul><h2 id="DOTA"><a href="#DOTA" class="headerlink" title="DOTA"></a>DOTA</h2><h3 id="DOTA1-0"><a href="#DOTA1-0" class="headerlink" title="DOTA1.0"></a>DOTA1.0</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800<del>4000 * 800</del>4000 * 3</td><td>15</td><td>2806</td><td>188282</td><td>OBB</td><td>Google Earth、高分2、吉林1</td><td>2018</td><td>武汉大学</td></tr></tbody></table><h3 id="DOTA1-5"><a href="#DOTA1-5" class="headerlink" title="DOTA1.5"></a>DOTA1.5</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800<del>4000 * 800</del>4000 * 3</td><td>16</td><td>2806</td><td>400000±</td><td>OBB</td><td>Google Earth、高分2、吉林1</td><td>2019</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://captain-whu.github.io/DOTA/index.html">https://captain-whu.github.io/DOTA/index.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53125">https://aistudio.baidu.com/aistudio/datasetdetail/53125</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/19AhrPGnbrPwCdHsThv0XBQ">https://pan.baidu.com/s/19AhrPGnbrPwCdHsThv0XBQ</a>  提取码：avsl</li></ul><h2 id="SZTAKI-INRIA"><a href="#SZTAKI-INRIA" class="headerlink" title="SZTAKI-INRIA"></a>SZTAKI-INRIA</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800± * 800±</td><td>1</td><td>9</td><td>665</td><td>OBB</td><td>未知</td><td>2012</td><td>MTA SZTAKI、INRIA Sophia-Antipolis</td></tr></tbody></table><ul><li>源地址：<a href="http://web.eee.sztaki.hu/remotesensing/building_benchmark.html">http://web.eee.sztaki.hu/remotesensing/building_benchmark.html</a></li></ul><h2 id="COWC"><a href="#COWC" class="headerlink" title="COWC"></a>COWC</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">2,000<del>19,000 * 2,000</del>19,000</td><td>1</td><td>53</td><td>32,716</td><td>0.15m</td><td>one dot</td><td>Utah</td><td>2016</td><td>劳伦斯利弗莫尔国家实验室</td></tr></tbody></table><ul><li>源地址：<a href="https://gdo152.llnl.gov/cowc/">https://gdo152.llnl.gov/cowc/</a></li></ul><h2 id="Functional-Map-of-the-World-Challenge"><a href="#Functional-Map-of-the-World-Challenge" class="headerlink" title="Functional Map of the World Challenge"></a>Functional Map of the World Challenge</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4bands<br>8bands</td><td>63</td><td>31</td><td>1,000,000</td><td>one dot</td><td>Utah</td><td>2016</td><td>劳伦斯利弗莫尔国家实验室</td></tr></tbody></table><ul><li>源地址：<a href="https://www.iarpa.gov/challenges/fmow.html">https://www.iarpa.gov/challenges/fmow.html</a><br><a href="https://github.com/fMoW/dataset">https://github.com/fMoW/dataset</a></li></ul><h2 id="CARPK"><a href="#CARPK" class="headerlink" title="CARPK"></a>CARPK</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">未知</td><td>1</td><td>1,573</td><td>106,233</td><td>未知</td><td>无人机</td><td>2017</td><td>台湾国立大学</td></tr></tbody></table><ul><li>源地址：<a href="https://lafi.github.io/LPN/">https://lafi.github.io/LPN/</a></li></ul><h2 id="rscup-1"><a href="#rscup-1" class="headerlink" title="rscup"></a>rscup</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1024 * 1024 * 3</td><td>18</td><td>未知</td><td>未知</td><td>OBB</td><td>GF, JL, Google Earth, Aerial</td><td>2019</td><td>rscup组委会</td></tr></tbody></table><ul><li>源地址：<a href="http://rscup.bjxintong.com.cn/#/theme/2">http://rscup.bjxintong.com.cn/#/theme/2</a></li></ul><h2 id="planet-understanding-the-amazon-from-space"><a href="#planet-understanding-the-amazon-from-space" class="headerlink" title="planet-understanding-the-amazon-from-space"></a>planet-understanding-the-amazon-from-space</h2><ul><li>源地址：<a href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data">https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data</a></li></ul><h2 id="airbus-ship-detection"><a href="#airbus-ship-detection" class="headerlink" title="airbus-ship-detection"></a>airbus-ship-detection</h2><ul><li>源地址：<a href="https://www.kaggle.com/c/airbus-ship-detection">https://www.kaggle.com/c/airbus-ship-detection</a></li></ul><h2 id="Open-AI-Tanzania-Building-Footprint-Segmentation-Challenge"><a href="#Open-AI-Tanzania-Building-Footprint-Segmentation-Challenge" class="headerlink" title="Open AI Tanzania Building Footprint Segmentation Challenge"></a>Open AI Tanzania Building Footprint Segmentation Challenge</h2><ul><li>源地址：<a href="https://competitions.codalab.org/competitions/20100">https://competitions.codalab.org/competitions/20100</a></li></ul><h2 id="MAFAT-Challenge-Fine-Grained-Classification-of-Objects-from-Aerial-Imagery"><a href="#MAFAT-Challenge-Fine-Grained-Classification-of-Objects-from-Aerial-Imagery" class="headerlink" title="MAFAT Challenge - Fine-Grained Classification of Objects from Aerial Imagery"></a>MAFAT Challenge - Fine-Grained Classification of Objects from Aerial Imagery</h2><ul><li>源地址：<a href="https://competitions.codalab.org/competitions/19854">https://competitions.codalab.org/competitions/19854</a></li></ul><h1 id="遥感影像图像分割"><a href="#遥感影像图像分割" class="headerlink" title="遥感影像图像分割"></a>遥感影像图像分割</h1><p><strong>收集网络中开源的、关于遥感影像 图像分割的深度学习数据集</strong></p><p>main ref </p><blockquote><p><a href="https://captain-whu.github.io/DiRS/">https://captain-whu.github.io/DiRS/</a><br><a href="https://mp.weixin.qq.com/s/x-sgus9bJz7esbsRvb-IfA">https://mp.weixin.qq.com/s/x-sgus9bJz7esbsRvb-IfA</a><br><a href="https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-12760/">https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-12760/</a><br><a href="http://www.patreo.dcc.ufmg.br/category/downloads/datasets/">http://www.patreo.dcc.ufmg.br/category/downloads/datasets/</a></p></blockquote><h2 id="Massachusetts-Roads"><a href="#Massachusetts-Roads" class="headerlink" title="Massachusetts Roads"></a>Massachusetts Roads</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1500 * 1500 * 3</td><td>1</td><td>804</td><td>航空影像</td><td>2013</td><td>University of Toronto</td></tr></tbody></table><ul><li>源地址：<a href="https://www.cs.toronto.edu/~vmnih/data/">https://www.cs.toronto.edu/~vmnih/data/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56961">https://aistudio.baidu.com/aistudio/datasetdetail/56961</a></li></ul><h2 id="Massachusetts-Builds"><a href="#Massachusetts-Builds" class="headerlink" title="Massachusetts Builds"></a>Massachusetts Builds</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1500 * 1500 * 3</td><td>1</td><td>151</td><td>航空影像</td><td>2013</td><td>University of Toronto</td></tr></tbody></table><ul><li>源地址：<a href="https://www.cs.toronto.edu/~vmnih/data/">https://www.cs.toronto.edu/~vmnih/data/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/57019">https://aistudio.baidu.com/aistudio/datasetdetail/57019</a></li></ul><h2 id="Zurich-Summer"><a href="#Zurich-Summer" class="headerlink" title="Zurich Summer"></a>Zurich Summer</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">600<del>1600 * 600</del>1600 * 4</td><td>8</td><td>20</td><td>QuickBird</td><td>2015</td><td>The University of Edinburgh, Scotland (UK)</td><td>0.62m</td></tr></tbody></table><ul><li>源地址：<a href="https://sites.google.com/view/zhouwx/dataset#h.p_hQS2jYeaFpV0">https://sites.google.com/view/zhouwx/dataset#h.p_hQS2jYeaFpV0</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55102">https://aistudio.baidu.com/aistudio/datasetdetail/55102</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1_Cydh88pJXKJnmlvUv2y6Q">https://pan.baidu.com/s/1_Cydh88pJXKJnmlvUv2y6Q</a>  提取码：u7cr</li></ul><h2 id="ERM-PAIW"><a href="#ERM-PAIW" class="headerlink" title="ERM-PAIW"></a>ERM-PAIW</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4000± * 4000± * 3</td><td>1</td><td>41</td><td>航空影像</td><td>2015</td><td>German Aerospace Center (DLR)</td></tr></tbody></table><ul><li>源地址：<a href="https://www.cs.toronto.edu/~vmnih/data/">https://www.cs.toronto.edu/~vmnih/data/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/57293">https://aistudio.baidu.com/aistudio/datasetdetail/57293</a></li></ul><h2 id="HD-Maps"><a href="#HD-Maps" class="headerlink" title="HD-Maps"></a>HD-Maps</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4000± * 4000± * 3</td><td>4</td><td>20</td><td>航空影像</td><td>2016</td><td>German Aerospace Center (DLR)</td></tr></tbody></table><ul><li>源地址：<a href="https://www.cs.toronto.edu/~vmnih/data/">https://www.cs.toronto.edu/~vmnih/data/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/57162">https://aistudio.baidu.com/aistudio/datasetdetail/57162</a></li></ul><h2 id="BDCI2017"><a href="#BDCI2017" class="headerlink" title="BDCI2017"></a>BDCI2017</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">8000± * 8000± * 3</td><td>5</td><td>5</td><td>未知</td><td>2017</td><td>BDCI</td></tr></tbody></table><ul><li>源地址：<a href="https://www.datafountain.cn/competitions/270">https://www.datafountain.cn/competitions/270</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55424">https://aistudio.baidu.com/aistudio/datasetdetail/55424</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1sww-vuY9m4snAJY9IFRnIA">https://pan.baidu.com/s/1sww-vuY9m4snAJY9IFRnIA</a>  提取码：ocnn</li></ul><h2 id="Learning-Aerial-Image-Segmentation-From-Online-Maps"><a href="#Learning-Aerial-Image-Segmentation-From-Online-Maps" class="headerlink" title="Learning Aerial Image Segmentation From Online Maps"></a>Learning Aerial Image Segmentation From Online Maps</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">3000± * 3000± * 3</td><td>2</td><td>1671</td><td>Google Maps、OpenStreetMap</td><td>2017</td><td>TH Zürich</td></tr></tbody></table><ul><li>源地址：<a href="https://zenodo.org/record/1154821#.X4rCKS6HqUm">https://zenodo.org/record/1154821#.X4rCKS6HqUm</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56140">https://aistudio.baidu.com/aistudio/datasetdetail/56140</a></li></ul><h2 id="2018-Open-AI-Tanzania-Building-Footprint-Segmentation-Challenge-TBF"><a href="#2018-Open-AI-Tanzania-Building-Footprint-Segmentation-Challenge-TBF" class="headerlink" title="2018 Open AI Tanzania Building Footprint Segmentation Challenge(TBF)"></a>2018 Open AI Tanzania Building Footprint Segmentation Challenge(TBF)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">40000± * 40000± * 3</td><td>1</td><td>13</td><td>航空影像</td><td>2018</td><td>SUZA</td></tr></tbody></table><ul><li>源地址：<a href="https://competitions.codalab.org/competitions/20100">https://competitions.codalab.org/competitions/20100</a><br><a href="http://www.graphnetcloud.cn/4-10">http://www.graphnetcloud.cn/4-10</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/76145">https://aistudio.baidu.com/aistudio/datasetdetail/76145</a></li></ul><h2 id="WHDLD"><a href="#WHDLD" class="headerlink" title="WHDLD"></a>WHDLD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>6</td><td>4940</td><td>UC Merced</td><td>2018</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://sites.google.com/view/zhouwx/dataset#h.p_hQS2jYeaFpV0">https://sites.google.com/view/zhouwx/dataset#h.p_hQS2jYeaFpV0</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55589">https://aistudio.baidu.com/aistudio/datasetdetail/55589</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1MgOUJpmClc_h4CLzJVQYvQ">https://pan.baidu.com/s/1MgOUJpmClc_h4CLzJVQYvQ</a>  提取码：gvui</li></ul><h2 id="DLRSD"><a href="#DLRSD" class="headerlink" title="DLRSD"></a>DLRSD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>21</td><td>2100</td><td>USGS National Map</td><td>2018</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://sites.google.com/view/zhouwx/dataset#h.p_hQS2jYeaFpV0">https://sites.google.com/view/zhouwx/dataset#h.p_hQS2jYeaFpV0</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55005">https://aistudio.baidu.com/aistudio/datasetdetail/55005</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1CmRdyNdz85ji0Z7JsrCHDw">https://pan.baidu.com/s/1CmRdyNdz85ji0Z7JsrCHDw</a>  提取码：1y38</li></ul><h2 id="DeepGlobe-Land-Cover-Classification-Challenge"><a href="#DeepGlobe-Land-Cover-Classification-Challenge" class="headerlink" title="DeepGlobe Land Cover Classification Challenge"></a>DeepGlobe Land Cover Classification Challenge</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">2448 * 2448 * 3</td><td>7</td><td>803</td><td>DigitalGlobe</td><td>2018</td><td>CVPR</td><td>0.5m</td></tr></tbody></table><ul><li>源地址：<a href="http://deepglobe.org/challenge.html">http://deepglobe.org/challenge.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55681">https://aistudio.baidu.com/aistudio/datasetdetail/55681</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1D-RpKNNuP5L-hLo4H-8txA">https://pan.baidu.com/s/1D-RpKNNuP5L-hLo4H-8txA</a>  提取码：lk1a</li></ul><h2 id="DeepGlobe-Road-Detection-Challenge"><a href="#DeepGlobe-Road-Detection-Challenge" class="headerlink" title="DeepGlobe Road Detection Challenge"></a>DeepGlobe Road Detection Challenge</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1024 * 1024 * 3</td><td>1</td><td>6,226</td><td>DigitalGlobe</td><td>2018</td><td>CVPR</td></tr></tbody></table><ul><li>源地址：<a href="http://deepglobe.org/challenge.html">http://deepglobe.org/challenge.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55682">https://aistudio.baidu.com/aistudio/datasetdetail/55682</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1uvZ7dUOjkt9_i120hr9JUQ">https://pan.baidu.com/s/1uvZ7dUOjkt9_i120hr9JUQ</a>  提取码：74be</li></ul><h2 id="Aeroscapes"><a href="#Aeroscapes" class="headerlink" title="Aeroscapes"></a>Aeroscapes</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">720 * 720 * 3</td><td>11</td><td>3269</td><td>航空影像</td><td>2018</td><td>Carnegie Mellon University</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/ishann/aeroscapes">https://github.com/ishann/aeroscapes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55222">https://aistudio.baidu.com/aistudio/datasetdetail/55222</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1-AcaV3nTDsBibbzk6Y92kw">https://pan.baidu.com/s/1-AcaV3nTDsBibbzk6Y92kw</a>  提取码：zodm</li></ul><h2 id="Map-Challenge"><a href="#Map-Challenge" class="headerlink" title="Map Challenge"></a>Map Challenge</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>标注格式</th></tr></thead><tbody><tr><td align="left">300 * 300 * 3</td><td>1</td><td>341,058</td><td>Google Map</td><td>2018</td><td>crowdAI</td><td>json</td></tr></tbody></table><ul><li>源地址：<a href="https://www.crowdai.org/challenges/mapping-challenge">https://www.crowdai.org/challenges/mapping-challenge</a> ；<a href="https://www.jianshu.com/p/90efc39975da">https://www.jianshu.com/p/90efc39975da</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54858">https://aistudio.baidu.com/aistudio/datasetdetail/54858</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1keRAm0OEanCDuRUPpUra6A%20">https://pan.baidu.com/s/1keRAm0OEanCDuRUPpUra6A</a> 提取码：pm0m</li></ul><h2 id="38-Cloud-A-Cloud-Segmentation-Dataset"><a href="#38-Cloud-A-Cloud-Segmentation-Dataset" class="headerlink" title="38-Cloud: A Cloud Segmentation Dataset"></a>38-Cloud: A Cloud Segmentation Dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">384* 384 * 4</td><td>1</td><td>8400</td><td>Landsat 8</td><td>2018</td><td>Science Simon Fraser University</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/SorourMo/38-Cloud-A-Cloud-Segmentation-Dataset">https://github.com/SorourMo/38-Cloud-A-Cloud-Segmentation-Dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56236">https://aistudio.baidu.com/aistudio/datasetdetail/56236</a></li></ul><h2 id="WHU-Building-Dataset-Satellite-dataset-Ⅰ-global-cities"><a href="#WHU-Building-Dataset-Satellite-dataset-Ⅰ-global-cities" class="headerlink" title="WHU Building Dataset,Satellite dataset Ⅰ (global cities)"></a>WHU Building Dataset,Satellite dataset Ⅰ (global cities)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>1</td><td>204</td><td>QuickBird, Worldview series, IKONOS, ZY-3</td><td>2019</td><td>武汉大学</td><td>0.3 ~ 2.5m</td></tr></tbody></table><ul><li>源地址：<a href="https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html">https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55774">https://aistudio.baidu.com/aistudio/datasetdetail/55774</a></li></ul><h2 id="WHU-Building-Dataset-Satellite-dataset-Ⅱ-East-Asia"><a href="#WHU-Building-Dataset-Satellite-dataset-Ⅱ-East-Asia" class="headerlink" title="WHU Building Dataset,Satellite dataset Ⅱ (East Asia)"></a>WHU Building Dataset,Satellite dataset Ⅱ (East Asia)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>1</td><td>17388</td><td>QuickBird, Worldview series, IKONOS, ZY-3</td><td>2019</td><td>武汉大学</td><td>0.45 m</td></tr></tbody></table><ul><li>源地址：<a href="https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html">https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56356">https://aistudio.baidu.com/aistudio/datasetdetail/56356</a></li></ul><h2 id="WHU-Building-Dataset-Aerial-imagery-dataset"><a href="#WHU-Building-Dataset-Aerial-imagery-dataset" class="headerlink" title="WHU Building Dataset,Aerial imagery dataset"></a>WHU Building Dataset,Aerial imagery dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>1</td><td>8,189</td><td>未知</td><td>2019</td><td>武汉大学</td><td>0.3 m</td></tr></tbody></table><ul><li>源地址：<a href="https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html">https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56502">https://aistudio.baidu.com/aistudio/datasetdetail/56502</a></li></ul><h2 id="DroneDeploy"><a href="#DroneDeploy" class="headerlink" title="DroneDeploy"></a>DroneDeploy</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">6000± * 6000± * 3</td><td>7</td><td>35 train, 8 val, 12 test</td><td>航空影像drones</td><td>2019</td><td>DroneDeploy</td><td>0.1 m</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/dronedeploy/dd-ml-segmentation-benchmark">https://github.com/dronedeploy/dd-ml-segmentation-benchmark</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/79283">https://aistudio.baidu.com/aistudio/datasetdetail/79283</a></li></ul><h2 id="RoadTracer"><a href="#RoadTracer" class="headerlink" title="RoadTracer"></a>RoadTracer</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">4096 * 4096 * 3</td><td>1</td><td>3000</td><td>Google earth、OSM</td><td>2019</td><td>MIT</td><td>0.6m</td></tr></tbody></table><ul><li>源地址：<a href="%20https://github.com/mitroadmaps/roadtracer/">https://github.com/mitroadmaps/roadtracer/</a><br><a href="https://github.com/tansor/VecRoad">https://github.com/tansor/VecRoad</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/74848">https://aistudio.baidu.com/aistudio/datasetdetail/74848</a></li></ul><h2 id="ORSSD"><a href="#ORSSD" class="headerlink" title="ORSSD"></a>ORSSD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500± * 500± * 3</td><td>8</td><td>600train，200test</td><td>Google Earth</td><td>2019</td><td>北京交通大学</td></tr></tbody></table><ul><li>源地址：<a href="https://hub.fastgit.org/rmcong/ORSSD-dataset">https://hub.fastgit.org/rmcong/ORSSD-dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/98275">https://aistudio.baidu.com/aistudio/datasetdetail/98275</a></li></ul><h2 id="EORSSD"><a href="#EORSSD" class="headerlink" title="EORSSD"></a>EORSSD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500± * 500± * 3</td><td>8</td><td>1400train, 600test</td><td>Google Earth</td><td>2020</td><td>北京交通大学</td></tr></tbody></table><ul><li>源地址：[<a href="https://hub.fastgit.org/rmcong/EORSSD-dataset">https://hub.fastgit.org/rmcong/EORSSD-dataset</a> ]<a href="https://hub.fastgit.org/rmcong/EORSSD-dataset">https://hub.fastgit.org/rmcong/EORSSD-dataset</a> )</li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/98372">https://aistudio.baidu.com/aistudio/datasetdetail/98372</a></li></ul><h2 id="Land-Cover-from-Aerial-Imagery（landcover-ai）"><a href="#Land-Cover-from-Aerial-Imagery（landcover-ai）" class="headerlink" title="Land Cover from Aerial Imagery（landcover_ai）"></a>Land Cover from Aerial Imagery（landcover_ai）</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">9000 * 9500 * 3，4200 * 4700 * 3</td><td>3</td><td>41</td><td>public geodetic resource</td><td>2020</td><td>linuxpolska</td><td>0.25m，0.5m</td></tr></tbody></table><ul><li>源地址：<a href="https://landcover.ai/">https://landcover.ai/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/76629">https://aistudio.baidu.com/aistudio/datasetdetail/76629</a></li></ul><h2 id="UAVid"><a href="#UAVid" class="headerlink" title="UAVid"></a>UAVid</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4096 * 2160 * 3 or 3840 * 2160 * 3</td><td>8</td><td>300</td><td>航空影像</td><td>2020</td><td>University of Twente</td></tr></tbody></table><ul><li>源地址：<a href="https://www.uavid.nl/">https://www.uavid.nl/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55774">https://aistudio.baidu.com/aistudio/datasetdetail/55774</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/14y_il9Axq_vMYQGdfz9JqQ%20">https://pan.baidu.com/s/14y_il9Axq_vMYQGdfz9JqQ</a> 提取码：4dv7</li></ul><h2 id="95-Cloud-An-Extension-to-38-Cloud-Dataset"><a href="#95-Cloud-An-Extension-to-38-Cloud-Dataset" class="headerlink" title="95-Cloud: An Extension to 38-Cloud Dataset"></a>95-Cloud: An Extension to 38-Cloud Dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">384 * 384 * 4</td><td>1</td><td>34,701</td><td>Landsat 8</td><td>2020</td><td>Simon Fraser University</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/SorourMo/95-Cloud-An-Extension-to-38-Cloud-Dataset">https://github.com/SorourMo/95-Cloud-An-Extension-to-38-Cloud-Dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56839">https://aistudio.baidu.com/aistudio/datasetdetail/56839</a></li></ul><h2 id="AI-遥感影像"><a href="#AI-遥感影像" class="headerlink" title="AI+遥感影像"></a>AI+遥感影像</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>8</td><td>100000</td><td>未知</td><td>2020</td><td>全国人工智能大赛组委会</td></tr></tbody></table><ul><li>源地址：<a href="https://www.datafountain.cn/competitions/457">https://www.datafountain.cn/competitions/457</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/51568">https://aistudio.baidu.com/aistudio/datasetdetail/51568</a></li></ul><h2 id="BDCI2020"><a href="#BDCI2020" class="headerlink" title="BDCI2020"></a>BDCI2020</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>7</td><td>145981</td><td>未知</td><td>2020</td><td>BDCI</td></tr></tbody></table><ul><li>源地址：<a href="https://www.datafountain.cn/competitions/475">https://www.datafountain.cn/competitions/475</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56051">https://aistudio.baidu.com/aistudio/datasetdetail/56051</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1XiECAvdc0UIOpyPHMvR09w">https://pan.baidu.com/s/1XiECAvdc0UIOpyPHMvR09w</a>  提取码：71sz</li></ul><h2 id="mini-Inria-Aerial-Image-Labeling-Dataset"><a href="#mini-Inria-Aerial-Image-Labeling-Dataset" class="headerlink" title="mini Inria Aerial Image Labeling Dataset"></a>mini Inria Aerial Image Labeling Dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>1</td><td>30000 train, 2500 test</td><td>航空影像</td><td>2021</td><td>天池大赛</td><td>0.3 m</td></tr></tbody></table><ul><li>源地址：<a href="https://tianchi.aliyun.com/competition/entrance/531872/introduction">https://tianchi.aliyun.com/competition/entrance/531872/introduction</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/70361">https://aistudio.baidu.com/aistudio/datasetdetail/70361</a></li></ul><h2 id="ISPRS-2D-Semantic-Labeling-Contest"><a href="#ISPRS-2D-Semantic-Labeling-Contest" class="headerlink" title="ISPRS 2D Semantic Labeling Contest"></a>ISPRS 2D Semantic Labeling Contest</h2><h3 id="Postdam"><a href="#Postdam" class="headerlink" title="Postdam"></a>Postdam</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">6000 * 6000 * 3</td><td>6</td><td>38</td><td>航空影像</td><td>2012</td><td>ISPRS</td><td>0.05m</td></tr></tbody></table><h3 id="Vaihingen"><a href="#Vaihingen" class="headerlink" title="Vaihingen"></a>Vaihingen</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">1000<del>4000 * 1000</del>4000 * 3</td><td>6</td><td>33</td><td>航空影像</td><td>2012</td><td>ISPRS</td><td>0.09m</td></tr></tbody></table><ul><li>源地址：<a href="https://www2.isprs.org/commissions/comm2/wg4/benchmark/semantic-labeling/">https://www2.isprs.org/commissions/comm2/wg4/benchmark/semantic-labeling/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55153">https://aistudio.baidu.com/aistudio/datasetdetail/55153</a>  ； <a href="https://aistudio.baidu.com/aistudio/datasetdetail/55408">https://aistudio.baidu.com/aistudio/datasetdetail/55408</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1Mu-rzPorbtsYr1XJpkJq-Q">https://pan.baidu.com/s/1Mu-rzPorbtsYr1XJpkJq-Q</a>  提取码：n4i0</li></ul><h2 id="GID-1"><a href="#GID-1" class="headerlink" title="GID"></a>GID</h2><h3 id="GID-Fine-Land-cover-Classification-15classes"><a href="#GID-Fine-Land-cover-Classification-15classes" class="headerlink" title="GID Fine Land-cover Classification_15classes"></a>GID Fine Land-cover Classification_15classes</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">7200 * 6800 * 4 <br>7200 * 6800 * 3</td><td>15</td><td>10</td><td>高分2</td><td>2018</td><td>武汉大学</td><td>0.8 to 10m</td></tr></tbody></table><h3 id="GID-Large-scale-Classification-5classes"><a href="#GID-Large-scale-Classification-5classes" class="headerlink" title="GID Large-scale Classification_5classes"></a>GID Large-scale Classification_5classes</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">7200 * 6800 * 4 <br>7200 * 6800 * 3</td><td>5</td><td>150</td><td>高分2</td><td>2018</td><td>武汉大学</td><td>0.8 to 10m</td></tr></tbody></table><ul><li>源地址：<a href="https://x-ytong.github.io/project/GID.html">https://x-ytong.github.io/project/GID.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54878">https://aistudio.baidu.com/aistudio/datasetdetail/54878</a>  ； <a href="https://aistudio.baidu.com/aistudio/datasetdetail/54934">https://aistudio.baidu.com/aistudio/datasetdetail/54934</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1Xux-3nKTbu1v3Mp5tdyM7A">https://pan.baidu.com/s/1Xux-3nKTbu1v3Mp5tdyM7A</a>  提取码：d3qs</li></ul><h2 id="UDD"><a href="#UDD" class="headerlink" title="UDD"></a>UDD</h2><h3 id="UDD5"><a href="#UDD5" class="headerlink" title="UDD5"></a>UDD5</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4096± * 2160± * 3</td><td>5</td><td>120 trian，40 val</td><td>无人机数据（DJI Phantom 4）</td><td>2018</td><td>北京大学</td></tr></tbody></table><h3 id="UDD6"><a href="#UDD6" class="headerlink" title="UDD6"></a>UDD6</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4096± * 2160± * 3</td><td>6</td><td>106 trian，35 val</td><td>无人机数据（DJI Phantom 4）</td><td>2018</td><td>北京大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/MarcWong/UDD">https://github.com/MarcWong/UDD</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/75675">https://aistudio.baidu.com/aistudio/datasetdetail/75675</a></li></ul><h2 id="BH-DATASET"><a href="#BH-DATASET" class="headerlink" title="BH-DATASET"></a>BH-DATASET</h2><h3 id="BH-POOLS"><a href="#BH-POOLS" class="headerlink" title="BH-POOLS"></a>BH-POOLS</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">3840 * 2160 * 3</td><td>1</td><td>200</td><td>GoogleEarth</td><td>2020</td><td>Federal University of Minas Gerais</td></tr></tbody></table><h3 id="BH-WATERTANKS"><a href="#BH-WATERTANKS" class="headerlink" title="BH-WATERTANKS"></a>BH-WATERTANKS</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">3840 * 2160 * 3</td><td>1</td><td>200</td><td>GoogleEarth</td><td>2020</td><td>Federal University of Minas Gerais</td></tr></tbody></table><ul><li>源地址：<a href="http://www.patreo.dcc.ufmg.br/2020/07/29/bh-pools-watertanks-datasets/">http://www.patreo.dcc.ufmg.br/2020/07/29/bh-pools-watertanks-datasets/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/57579">https://aistudio.baidu.com/aistudio/datasetdetail/57579</a></li></ul><h2 id="Inria-Aerial-Image-Labeling-Dataset"><a href="#Inria-Aerial-Image-Labeling-Dataset" class="headerlink" title="Inria Aerial Image Labeling Dataset"></a>Inria Aerial Image Labeling Dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">未知</td><td>1</td><td>未知</td><td>航空影像</td><td>2017</td><td>Inria Sophia Antipolis - Mediterran ee, TITANE team; Inria Saclay, TAO team, France</td><td>0.3 m</td></tr></tbody></table><ul><li>源地址：<a href="https://project.inria.fr/aerialimagelabeling/">https://project.inria.fr/aerialimagelabeling/</a> ；<a href="https://hyper.ai/datasets/5428">https://hyper.ai/datasets/5428</a></li></ul><h2 id="rscup-2"><a href="#rscup-2" class="headerlink" title="rscup"></a>rscup</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">7200 * 6800 * 4</td><td>16</td><td>train 8, val 2, test 10</td><td>高分二号MSS影像</td><td>2019</td><td>rscup组委会</td><td>4 m</td></tr></tbody></table><ul><li>源地址：<a href="http://rscup.bjxintong.com.cn/#/theme/3">http://rscup.bjxintong.com.cn/#/theme/3</a></li></ul><h2 id="suichang-dataset"><a href="#suichang-dataset" class="headerlink" title="suichang dataset"></a>suichang dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">256 * 256 * 4</td><td>10</td><td>16017</td><td>高分系列</td><td>2021</td><td>浙江大学、天池大赛</td><td>0.8~ 2m</td></tr></tbody></table><ul><li>源地址：<a href="https://tianchi.aliyun.com/competition/entrance/531860/rankingList">https://tianchi.aliyun.com/competition/entrance/531860/rankingList</a></li></ul><h2 id="LRSNY"><a href="#LRSNY" class="headerlink" title="LRSNY"></a>LRSNY</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">1000 *  1000 * 3</td><td>1</td><td>716 train, 220 val, 432 test</td><td>未知</td><td>2021</td><td>IEEE</td><td>0.5 m</td></tr></tbody></table><ul><li>源地址：<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9333652">https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9333652</a><br><a href="ftp://154.85.52.76/LRSNY/">ftp://154.85.52.76/LRSNY/</a></li></ul><h1 id="遥感影像变化检测"><a href="#遥感影像变化检测" class="headerlink" title="遥感影像变化检测"></a>遥感影像变化检测</h1><p><strong>收集网络中开源的、关于遥感影像 变化检测的深度学习数据集</strong></p><p>main ref </p><blockquote><p><a href="https://captain-whu.github.io/DiRS/">https://captain-whu.github.io/DiRS/</a><br><a href="https://mp.weixin.qq.com/s/GN9SEztb61cm0ukn81Szbg">https://mp.weixin.qq.com/s/GN9SEztb61cm0ukn81SzbgA</a></p></blockquote><h2 id="SZTAKI-INRIA-AirChange"><a href="#SZTAKI-INRIA-AirChange" class="headerlink" title="SZTAKI-INRIA AirChange"></a>SZTAKI-INRIA AirChange</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">952 * 640 * 3, 2 time</td><td>1</td><td>13 * 2</td><td>未知</td><td>2009</td><td>MTA SZTAKI</td><td>1.5m</td></tr></tbody></table><ul><li>源地址：<a href="http://web.eee.sztaki.hu/remotesensing/airchange_benchmark.html">http://web.eee.sztaki.hu/remotesensing/airchange_benchmark.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/77781">https://aistudio.baidu.com/aistudio/datasetdetail/77781</a></li></ul><h2 id="AIST-Building-Change-Detection-ABCD"><a href="#AIST-Building-Change-Detection-ABCD" class="headerlink" title="AIST Building Change Detection(ABCD)"></a>AIST Building Change Detection(ABCD)</h2><h3 id="fixed-scale"><a href="#fixed-scale" class="headerlink" title="fixed-scale"></a>fixed-scale</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">160 * 160 * 6, 2 time</td><td>1</td><td>4,253 * 2</td><td>aerial images</td><td>2017</td><td>AIST</td><td>1.5m</td></tr></tbody></table><h3 id="resized"><a href="#resized" class="headerlink" title="resized"></a>resized</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">128 * 128 * 3, 2 time</td><td>1</td><td>4,223 * 2</td><td>aerial images</td><td>2017</td><td>AIST</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/gistairc/ABCDdataset">https://github.com/gistairc/ABCDdataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/79596">https://aistudio.baidu.com/aistudio/datasetdetail/79596</a></li></ul><h2 id="WHU-Building-Change-Detection-Dataset"><a href="#WHU-Building-Change-Detection-Dataset" class="headerlink" title="WHU Building Change Detection Dataset"></a>WHU Building Change Detection Dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">32,207 * 15,354 * 3, 2 time</td><td>1</td><td>1 * 2</td><td>航空影像</td><td>2018</td><td>武汉大学</td><td>0.2m</td></tr></tbody></table><ul><li>源地址：<a href="https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html">https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/79035">https://aistudio.baidu.com/aistudio/datasetdetail/79035</a></li></ul><h2 id="season-varying"><a href="#season-varying" class="headerlink" title="season-varying"></a>season-varying</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3, 2 time</td><td>1</td><td>10000train, 3000val, 3000test</td><td>Google Earth (DigitalGlobe)</td><td>2018</td><td>GosNIIAS</td><td>0.03~1m</td></tr></tbody></table><ul><li>源地址：<a href="https://paperswithcode.com/dataset/cdd-dataset-season-varying">https://paperswithcode.com/dataset/cdd-dataset-season-varying</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/78676">https://aistudio.baidu.com/aistudio/datasetdetail/78676</a></li></ul><h2 id="Onera-Satellite-Change-Detection-OSCD"><a href="#Onera-Satellite-Change-Detection-OSCD" class="headerlink" title="Onera Satellite Change Detection (OSCD)"></a>Onera Satellite Change Detection (OSCD)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">600± * 600± * 13, 2 time</td><td>1</td><td>14 * 2 train, 10 * 2 test</td><td>Sentinel-2</td><td>2018</td><td>Universit´e Paris-Saclay、 T´el´ecom ParisTech</td></tr></tbody></table><ul><li>源地址：<a href="https://rcdaudt.github.io/oscd/">https://rcdaudt.github.io/oscd/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/72898">https://aistudio.baidu.com/aistudio/datasetdetail/72898</a></li></ul><h2 id="Multi-temporal-Scene-WuHan-MtS-WH"><a href="#Multi-temporal-Scene-WuHan-MtS-WH" class="headerlink" title="Multi-temporal Scene WuHan (MtS-WH)"></a>Multi-temporal Scene WuHan (MtS-WH)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">7200 * 6000 * 4，2 time</td><td>9</td><td>190 * 2</td><td>IKONOS传感器</td><td>2019</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="http://sigma.whu.edu.cn/newspage.php?q=2019_03_26">http://sigma.whu.edu.cn/newspage.php?q=2019_03_26</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/70452">https://aistudio.baidu.com/aistudio/datasetdetail/70452</a></li></ul><h2 id="High-Resolution-Semantic-Change-HRSCD"><a href="#High-Resolution-Semantic-Change-HRSCD" class="headerlink" title="High Resolution Semantic Change (HRSCD)"></a>High Resolution Semantic Change (HRSCD)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">10000 * 10000 * 3, 2 time</td><td>6</td><td>291</td><td>IGS’s BD ORTHO database</td><td>2019</td><td>ETH Zürich / EcoVision Lab</td><td>0.5 m</td></tr></tbody></table><ul><li>源地址：<a href="https://rcdaudt.github.io/hrscd/">https://rcdaudt.github.io/hrscd/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/87088">https://aistudio.baidu.com/aistudio/datasetdetail/87088</a></li></ul><h2 id="Change-Detection-Dataset-CDD"><a href="#Change-Detection-Dataset-CDD" class="headerlink" title="Change Detection Dataset(CDD)"></a>Change Detection Dataset(CDD)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">984 * 740 * 224,  600 * 500 * 224, 390 * 200 * 242,  2 time</td><td>3,5</td><td>3 * 2</td><td>HYPERION,AVIRIS sensor</td><td>2019</td><td>未知</td><td>未知</td></tr></tbody></table><ul><li>源地址：<a href="https://gitlab.citius.usc.es/hiperespectral/ChangeDetectionDataset">https://gitlab.citius.usc.es/hiperespectral/ChangeDetectionDataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/89523">https://aistudio.baidu.com/aistudio/datasetdetail/89523</a></li></ul><h2 id="xBD"><a href="#xBD" class="headerlink" title="xBD"></a>xBD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1024 * 1024,  3/4/8 band, 4 state</td><td>4</td><td>22068</td><td>DigitalGlobe</td><td>2020</td><td>MIT</td></tr></tbody></table><ul><li>源地址：<a href="https://xview2.org/dataset">https://xview2.org/dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/86660">https://aistudio.baidu.com/aistudio/datasetdetail/86660</a></li></ul><h2 id="Google-dataset"><a href="#Google-dataset" class="headerlink" title="Google dataset"></a>Google dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">1000<del>5000 * 1000</del>5000 * 3, 2 time</td><td>1</td><td>20</td><td>Google Earth</td><td>2020</td><td>ieee</td><td>0.55 m</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/daifeng2016/Change-Detection-Dataset-for-High-Resolution-Satellite-Imagery">https://github.com/daifeng2016/Change-Detection-Dataset-for-High-Resolution-Satellite-Imagery</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/75099">https://aistudio.baidu.com/aistudio/datasetdetail/75099</a></li></ul><h2 id="LEVIR-CD"><a href="#LEVIR-CD" class="headerlink" title="LEVIR-CD"></a>LEVIR-CD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">1024 * 1024 * 3, 2 time</td><td>1</td><td>637</td><td>Google Earth</td><td>2020</td><td>北京航空航天大学</td><td>0.5 m</td></tr></tbody></table><ul><li>源地址：<a href="https://justchenhao.github.io/LEVIR/">https://justchenhao.github.io/LEVIR/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/75459">https://aistudio.baidu.com/aistudio/datasetdetail/75459</a></li></ul><h2 id="SenseEarth-ChangeDetection"><a href="#SenseEarth-ChangeDetection" class="headerlink" title="SenseEarth ChangeDetection"></a>SenseEarth ChangeDetection</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3, 2 time</td><td>6</td><td>2968 train, 847 val</td><td>未知</td><td>2020</td><td>商汤科技</td><td>0.5~3m</td></tr></tbody></table><ul><li>源地址：<a href="https://rs.sensetime.com/competition/index.html#/info">https://rs.sensetime.com/competition/index.html#/info</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53484">https://aistudio.baidu.com/aistudio/datasetdetail/53484</a></li></ul><h2 id="SEmantic-Change-detectiON-Data-SECOND"><a href="#SEmantic-Change-detectiON-Data-SECOND" class="headerlink" title="SEmantic Change detectiON Data(SECOND)"></a>SEmantic Change detectiON Data(SECOND)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3, 2 time</td><td>6</td><td>4662 * 2</td><td>未知</td><td>2020</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="http://www.captain-whu.com/project/SCD/">http://www.captain-whu.com/project/SCD/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/87088">https://aistudio.baidu.com/aistudio/datasetdetail/87088</a></li></ul><h2 id="Sun-Yat-Sen-University-SYSU-CD"><a href="#Sun-Yat-Sen-University-SYSU-CD" class="headerlink" title="Sun Yat-Sen University (SYSU)-CD"></a>Sun Yat-Sen University (SYSU)-CD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3, 2 time</td><td>1</td><td>20000 * 2</td><td>aerial image</td><td>2021</td><td>中山大学</td></tr></tbody></table><ul><li>源地址：<a href="https://hub.fastgit.org/liumency/SYSU-CD">https://hub.fastgit.org/liumency/SYSU-CD</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/98596">https://aistudio.baidu.com/aistudio/datasetdetail/98596</a></li></ul><h2 id="rscup-3"><a href="#rscup-3" class="headerlink" title="rscup"></a>rscup</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">960 * 960 * 4, 2 time</td><td>1</td><td>未知</td><td>未知</td><td>2020</td><td>rscup组委会</td></tr></tbody></table><ul><li>源地址：<a href="http://rscup.bjxintong.com.cn/#/theme/4">http://rscup.bjxintong.com.cn/#/theme/4</a></li></ul><h1 id="遥感影像场景分类——多-x2F-高光谱"><a href="#遥感影像场景分类——多-x2F-高光谱" class="headerlink" title="遥感影像场景分类——多/高光谱"></a>遥感影像场景分类——多/高光谱</h1><p><strong>收集网络中开源的、关于高光谱遥感影像 图像分类的深度学习数据集</strong></p><h2 id="EuroSAT"><a href="#EuroSAT" class="headerlink" title="EuroSAT"></a>EuroSAT</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">64 * 64 * 3<br>64 * 64 * 13</td><td>10</td><td>27,000</td><td>10 m</td><td>哨兵2</td><td>2018</td><td>德国凯泽斯劳滕大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/phelber/eurosat">https://github.com/phelber/eurosat</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52650">https://aistudio.baidu.com/aistudio/datasetdetail/52650</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/15PEr2iWIq-U0kBF7Jp8Fmg">https://pan.baidu.com/s/15PEr2iWIq-U0kBF7Jp8Fmg</a>  提取码：d34f</li></ul><h2 id="TG1HRSSC"><a href="#TG1HRSSC" class="headerlink" title="TG1HRSSC"></a>TG1HRSSC</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th><th>空间分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 1，512 * 512 * 54，512 * 512 * 52</td><td>9</td><td>204</td><td>天宫一号</td><td>2021</td><td>中国科学院空间应用工程与技术中心</td><td>0.5—0.8μm、1band(PAN), 0.4—1.0μm、54band((VNI), 1.0—2.5μm、52band((SWI),</td><td>5m(PAN), 10m(VNI), 20m(SWI)</td></tr></tbody></table><ul><li>源地址：<a href="http://www.msadc.cn/main/setsubDetail?id=1369487569196158978">http://www.msadc.cn/main/setsubDetail?id=1369487569196158978</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/86229">https://aistudio.baidu.com/aistudio/datasetdetail/86229</a></li></ul><h2 id="NaSC-TG2"><a href="#NaSC-TG2" class="headerlink" title="NaSC-TG2"></a>NaSC-TG2</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th><th>空间分辨率</th></tr></thead><tbody><tr><td align="left">128 * 128 * 3， 128 * 128 * 14</td><td>10</td><td>20000</td><td>天宫二号</td><td>2021</td><td>中国科学院空间应用工程与技术中心</td><td>0.40–1.04 µm</td><td></td></tr></tbody></table><ul><li>源地址：<a href="http://www.msadc.cn/main/setsubDetail?id=1370312964720037889">http://www.msadc.cn/main/setsubDetail?id=1370312964720037889</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/86451">https://aistudio.baidu.com/aistudio/datasetdetail/86451</a></li></ul><h1 id="遥感影像目标检测——多-x2F-高光谱"><a href="#遥感影像目标检测——多-x2F-高光谱" class="headerlink" title="遥感影像目标检测——多/高光谱"></a>遥感影像目标检测——多/高光谱</h1><p><strong>收集网络中开源的、关于高光谱遥感影像 图像分类的深度学习数据集</strong></p><h2 id="Dstl-Satellite-Imagery-Feature-Detection"><a href="#Dstl-Satellite-Imagery-Feature-Detection" class="headerlink" title="Dstl Satellite Imagery Feature Detection"></a>Dstl Satellite Imagery Feature Detection</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">13348 * 3392 * 3，837 * 848 * 8，134 * 136 * 8</td><td>10</td><td>25 train, 32 test</td><td>250+</td><td>MultiPolygons</td><td>WorldView 3</td><td>2017</td><td>DigitalGlobe</td><td>全色0.31 m, 多光谱1.24 m, 短波红外7.5 m</td></tr></tbody></table><ul><li>源地址：<a href="https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection/data">https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection/data</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/87506">https://aistudio.baidu.com/aistudio/datasetdetail/87506</a></li></ul><h2 id="xView"><a href="#xView" class="headerlink" title="xView"></a>xView</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">3000± * 3000± * 3<br>3000± * 3000± * 8</td><td>60</td><td>1129</td><td>1,000,000</td><td>0.3m</td><td>HBB</td><td>WorldView 3</td><td>2018</td><td>DIUx、NGA</td></tr></tbody></table><ul><li>源地址：<a href="https://challenge.xviewdataset.org/data-download">https://challenge.xviewdataset.org/data-download</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53622">https://aistudio.baidu.com/aistudio/datasetdetail/53622</a></li></ul><h1 id="遥感影像图像分割——多-x2F-高光谱"><a href="#遥感影像图像分割——多-x2F-高光谱" class="headerlink" title="遥感影像图像分割——多/高光谱"></a>遥感影像图像分割——多/高光谱</h1><p><strong>收集网络中开源的、关于高光谱遥感影像 图像分割的深度学习数据集</strong></p><p>main ref </p><blockquote><p><a href="https://mp.weixin.qq.com/s/S_TEoWyYFtdjtDeEXmpBPg">https://mp.weixin.qq.com/s/S_TEoWyYFtdjtDeEXmpBPg</a><br><a href="http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes">http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes</a></p></blockquote><h2 id="Salinas"><a href="#Salinas" class="headerlink" title="Salinas"></a>Salinas</h2><h3 id="Salinas-scene"><a href="#Salinas-scene" class="headerlink" title="Salinas scene"></a>Salinas scene</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">512 * 217 * 224</td><td>16</td><td>1</td><td>AVIRIS sensor</td><td>2011</td><td>未知</td></tr></tbody></table><h3 id="Salinas-A-scene"><a href="#Salinas-A-scene" class="headerlink" title="Salinas-A scene"></a>Salinas-A scene</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">83 * 86 * 224</td><td>6</td><td>1</td><td>AVIRIS sensor</td><td>2011</td><td>未知</td></tr></tbody></table><ul><li>源地址：<a href="http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes">http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/82020">https://aistudio.baidu.com/aistudio/datasetdetail/82020</a></li></ul><h2 id="Pavia-Centre-and-University"><a href="#Pavia-Centre-and-University" class="headerlink" title="Pavia Centre and University"></a>Pavia Centre and University</h2><h3 id="Pavia-Centre-scene"><a href="#Pavia-Centre-scene" class="headerlink" title="Pavia Centre scene"></a>Pavia Centre scene</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th></tr></thead><tbody><tr><td align="left">1096 * 1096 * 102</td><td>9</td><td>1</td><td>ROSIS sensor</td><td>2011</td><td>NPavia university</td><td>0.43-0.86μm</td></tr></tbody></table><h3 id="Pavia-University-scene"><a href="#Pavia-University-scene" class="headerlink" title="Pavia University scene"></a>Pavia University scene</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th></tr></thead><tbody><tr><td align="left">610 * 610 * 103</td><td>9</td><td>1</td><td>ROSIS sensor)</td><td>2011</td><td>Pavia university</td><td>0.43-0.86μm</td></tr></tbody></table><ul><li>源地址：<a href="http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes">http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/81260">https://aistudio.baidu.com/aistudio/datasetdetail/81260</a></li></ul><h2 id="Washington-DC-MALL"><a href="#Washington-DC-MALL" class="headerlink" title="Washington DC MALL"></a>Washington DC MALL</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th></tr></thead><tbody><tr><td align="left">1280 * 307 * 191</td><td>7</td><td>1</td><td>机载高光谱数据</td><td>2013</td><td>Spectral Information Technology Application Center of Virginia</td><td>0.4到2.4 µm</td></tr></tbody></table><ul><li>源地址：<a href="http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes">http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/83251">https://aistudio.baidu.com/aistudio/datasetdetail/83251</a></li></ul><h2 id="IKennedy-Space-Center-KSC"><a href="#IKennedy-Space-Center-KSC" class="headerlink" title="IKennedy Space Center (KSC)"></a>IKennedy Space Center (KSC)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th></tr></thead><tbody><tr><td align="left">512 * 614 * 176</td><td>13</td><td>1</td><td>NASA AVIRIS</td><td>2014</td><td>Pavia university</td><td>400–2500 nm</td></tr></tbody></table><ul><li>源地址：<a href="http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes">http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/81260">https://aistudio.baidu.com/aistudio/datasetdetail/81260</a></li></ul><h2 id="Botswana"><a href="#Botswana" class="headerlink" title="Botswana"></a>Botswana</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th></tr></thead><tbody><tr><td align="left">1476 * 256 * 145</td><td>14</td><td>1</td><td>Hyperion sensor on EO-1</td><td>2014</td><td>未知</td><td>未知</td></tr></tbody></table><ul><li>源地址：<a href="http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes">http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/82578">https://aistudio.baidu.com/aistudio/datasetdetail/82578</a></li></ul><h2 id="Indian-Pines"><a href="#Indian-Pines" class="headerlink" title="Indian Pines"></a>Indian Pines</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th></tr></thead><tbody><tr><td align="left">145 * 145 * 224；614 * 1848 * 224；2678 * 614 * 224</td><td>16</td><td>3</td><td>AVIRIS sensor</td><td>2015</td><td>Pursue univeristy</td><td>400–2500 nm</td></tr></tbody></table><ul><li>源地址：<a href="https://purr.purdue.edu/publications/1947/1">https://purr.purdue.edu/publications/1947/1</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/80970">https://aistudio.baidu.com/aistudio/datasetdetail/80970</a></li></ul><h2 id="HyRANK"><a href="#HyRANK" class="headerlink" title="HyRANK"></a>HyRANK</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">250± * 1000± * 176</td><td>14</td><td>2train, 3val</td><td>Hyperion sensor (EO-1, USGS)</td><td>2018</td><td>National Technical University of Athen</td></tr></tbody></table><ul><li>源地址：<a href="https://www2.isprs.org/commissions/comm3/wg4/hyrank/">https://www2.isprs.org/commissions/comm3/wg4/hyrank/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/80840">https://aistudio.baidu.com/aistudio/datasetdetail/80840</a></li></ul><h2 id="RIT-18"><a href="#RIT-18" class="headerlink" title="RIT-18"></a>RIT-18</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">9,393 * 5,642 * 7；8,833 * 6,918 * 7；12,446 * 7,654 * 7</td><td>18</td><td>3</td><td>Tetracam Micro-MCA6</td><td>2018</td><td>Rochester Institute of Technology</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/rmkemker/RIT-18">https://github.com/rmkemker/RIT-18</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/98991">https://aistudio.baidu.com/aistudio/datasetdetail/98991</a></li></ul><h1 id="遥感影像多标签分类"><a href="#遥感影像多标签分类" class="headerlink" title="遥感影像多标签分类"></a>遥感影像多标签分类</h1><h2 id="MLRSNet"><a href="#MLRSNet" class="headerlink" title="MLRSNet"></a>MLRSNet</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th><th>最大标签数</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>46</td><td>109,161</td><td>Google Earth</td><td>2020</td><td>中国地质大学</td><td>0.1~10m</td><td>13</td></tr></tbody></table><ul><li>源地址：<a href="https://data.mendeley.com/datasets/7j9bv9vwsx/2">https://data.mendeley.com/datasets/7j9bv9vwsx/2</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/76804">https://aistudio.baidu.com/aistudio/datasetdetail/76804</a></li></ul><h2 id="BigEarthNet"><a href="#BigEarthNet" class="headerlink" title="BigEarthNet"></a>BigEarthNet</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>备注</th></tr></thead><tbody><tr><td align="left">120 * 120 <br> 60 * 60 <br> 20 * 20;</td><td>43</td><td>590326</td><td>10 m <br> 20 m <br> 60m</td><td>哨兵2</td><td>2019</td><td>柏林工业大学</td><td>多标签</td></tr></tbody></table><ul><li>源地址：<a href="http://bigearth.net/">http://bigearth.net/</a></li></ul><h1 id="遥感影像图像标题"><a href="#遥感影像图像标题" class="headerlink" title="遥感影像图像标题"></a>遥感影像图像标题</h1><p><strong>收集网络中开源的、关于图像标题的深度学习数据集</strong></p><p>main ref </p><blockquote><p><a href="https://github.com/201528014227051/RSICD_optimal">https://github.com/201528014227051/RSICD_optimal</a></p></blockquote><h2 id="UCM-caption"><a href="#UCM-caption" class="headerlink" title="UCM caption"></a>UCM caption</h2><table><thead><tr><th align="left">图像大小</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>名词数</th><th>句子总数</th><th>单样本句数</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>2100</td><td>USGS National Map</td><td>2016</td><td>中科院</td><td>21</td><td>10500</td><td>5</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/201528014227051/RSICD_optimal">https://github.com/201528014227051/RSICD_optimal</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/90740">https://aistudio.baidu.com/aistudio/datasetdetail/90740</a></li></ul><h2 id="Sydney-caption"><a href="#Sydney-caption" class="headerlink" title="Sydney caption"></a>Sydney caption</h2><table><thead><tr><th align="left">图像大小</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>名词数</th><th>句子总数</th><th>单样本句数</th></tr></thead><tbody><tr><td align="left">500 * 500 * 3</td><td>613</td><td>Google Earth</td><td>2016</td><td>中科院</td><td>7</td><td>3065</td><td>5</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/201528014227051/RSICD_optimal">https://github.com/201528014227051/RSICD_optimal</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/91126">https://aistudio.baidu.com/aistudio/datasetdetail/91126</a></li></ul><h2 id="RSICD"><a href="#RSICD" class="headerlink" title="RSICD"></a>RSICD</h2><table><thead><tr><th align="left">图像大小</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>名词数</th><th>句子总数</th><th>单样本句数</th></tr></thead><tbody><tr><td align="left">224 * 244 * 3</td><td>10921</td><td>Google Earth, Baidu Map, MapABC, Tianditu</td><td>2017</td><td>IEEE</td><td>30</td><td>2433</td><td>5</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/201528014227051/RSICD_optimal">https://github.com/201528014227051/RSICD_optimal</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/90307">https://aistudio.baidu.com/aistudio/datasetdetail/90307</a></li></ul><h1 id="遥感影像视频目标检测"><a href="#遥感影像视频目标检测" class="headerlink" title="遥感影像视频目标检测"></a>遥感影像视频目标检测</h1><h1 id="遥感影像视频跟踪"><a href="#遥感影像视频跟踪" class="headerlink" title="遥感影像视频跟踪"></a>遥感影像视频跟踪</h1><p><strong>收集网络中开源的、关于图像标题的深度学习数据集</strong></p><h2 id="UAV123"><a href="#UAV123" class="headerlink" title="UAV123"></a>UAV123</h2><table><thead><tr><th align="left">视频大小</th><th>视频帧数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">720 * 1280 * 3, jpg; 123 video sub-sequences</td><td>110,000 + frames, 10/30 FPS</td><td>UAV (DJIS1000)、UAV simulator</td><td>2017</td><td>King Abdullah University of Science and Technology</td></tr></tbody></table><ul><li>源地址：<a href="https://cemse.kaust.edu.sa/ivul/uav123">https://cemse.kaust.edu.sa/ivul/uav123</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/91853">https://aistudio.baidu.com/aistudio/datasetdetail/91853</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>文章介绍</title>
      <link href="/2018/09/07/shi-li/"/>
      <url>/2018/09/07/shi-li/</url>
      
        <content type="html"><![CDATA[<p>主题使用指南：<a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md">https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md</a></p>]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Typora </tag>
            
            <tag> Markdown </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
