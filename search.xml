<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2022/04/02/mmdetection-xue-xi-bi-ji-1-zheng-ti-gou-jian-liu-cheng/"/>
      <url>/2022/04/02/mmdetection-xue-xi-bi-ji-1-zheng-ti-gou-jian-liu-cheng/</url>
      
        <content type="html"><![CDATA[<h1 id="轻松掌握-MMDetection-整体构建流程-一"><a href="#轻松掌握-MMDetection-整体构建流程-一" class="headerlink" title="轻松掌握 MMDetection 整体构建流程(一)"></a>轻松掌握 MMDetection 整体构建流程(一)</h1><h2 id="0-涉及内容"><a href="#0-涉及内容" class="headerlink" title="0 涉及内容"></a>0 涉及内容</h2><ul><li>MMDetection整体构建流程与思想</li><li>目标检测算法核心组件划分</li><li>目标检测核心组件功能</li></ul><h2 id="1-目标检测算法抽象流程"><a href="#1-目标检测算法抽象流程" class="headerlink" title="1 目标检测算法抽象流程"></a>1 目标检测算法抽象流程</h2><p>目前目标检测算法大致归纳如下：</p><p><img src="https://pic1.zhimg.com/80/v2-23f3f33d5ed5792e7ad55e559a6798fc_720w.jpg" alt="img"></p><p>目标检测算法可以按照 3 个维度划分：</p><ul><li><p><strong>按照 stage 个数划分</strong>，常规是 one-stage 和 two-stage，但是实际上界限不是特别清晰，例如带 refine 阶段的算法 RepPoints，实际上可以认为是1.5 stage 算法，而 Cascade R-CNN 可以认为是多阶段算法，为了简单，上面图示没有划分如此细致。</p></li><li><p><strong>按照是否需要预定义 anchor 划分</strong>，常规是 anchor-based 和 anchor-free，当然也有些算法是两者混合的。</p></li><li><p><strong>按照是否采用了 transformer 结构划分</strong>，目前基于 transformer 结构的目标检测算法发展迅速，也引起了极大的关注，所以这里特意增加了这个类别的划分</p></li></ul><h2 id="2-MMDetection整体构建流程与思想"><a href="#2-MMDetection整体构建流程与思想" class="headerlink" title="2 MMDetection整体构建流程与思想"></a>2 MMDetection整体构建流程与思想</h2><p>基于目前代码实现，所有目标检测算法都按照以下流程进行划分：<img src="https://pic1.zhimg.com/80/v2-7ecc8e5e19c59a3e6682c5e3cdc34918_720w.jpg" alt="img"></p><p>上述流程对应 MMDetection 代码构建流程，理解每个组件的作用不仅仅对阅读算法源码有帮助，而且还能够快速理解新提出算法对应的改进部分。下面对每个模块进行详细解读。</p><h3 id="2-1-训练核心组件"><a href="#2-1-训练核心组件" class="headerlink" title="2.1 训练核心组件"></a>2.1 训练核心组件</h3><p>训练部分一般包括 9 个核心组件，总体流程是：</p><ol><li>任何一个 batch 的图片先输入到 <strong>backbone 中进行特征提取</strong>，典型的骨干网络是 <strong>ResNet</strong>。</li><li>输出的单尺度或者多尺度特征图输入到 <strong>neck 模块中进行特征融合或者增强</strong>，典型的 neck 是 <strong>FPN</strong>。</li><li>上述多尺度特征最终输入到 <strong>head 部分</strong>，一般都会包括<strong>分类和回归</strong>分支输出。</li><li>在整个<strong>网络构建阶段</strong>都可以引入一些即插即用<strong>增强算子</strong>来增加提取提取能力，典型的例如 <strong>SPP、DCN</strong> 等等。</li><li>目标检测 <strong>head 输出一般是特征图</strong>，对于分类任务存在严重的正负样本不平衡，可以通过正负样本<strong>属性分配和采样</strong>控制。</li><li>为了方便收敛和平衡多分支，一般都会<strong>对 gt bbox 进行编码</strong>。</li><li>最后一步是<strong>计算分类和回归 loss</strong>，进行训练。</li><li>在训练过程中也包括非常多的 <strong>trick</strong>，例如优化器选择等，参数调节也非常关键</li></ol><p>注意上述 9 个组件不是每个算法都需要的，下面详细分析。</p><h4 id="2-1-1-Backbone"><a href="#2-1-1-Backbone" class="headerlink" title="2.1.1 Backbone"></a>2.1.1 Backbone</h4><p><strong>backbone 作用主要是特征提取</strong>，最常用的是 ResNet 系列、ResNetV1d 系列和 Res2Net 系列。</p><p><img src="https://pic2.zhimg.com/80/v2-cdee2bd9f289d650ddbcbd748c4be0f9_720w.jpg" alt="img"></p><p>目前 MMDetection 中已经集成了大部分骨架网络，具体见文件：<code>mmdet/models/backbones</code>，V2.7 已经实现的骨架如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'RegNet'</span><span class="token punctuation">,</span> <span class="token string">'ResNet'</span><span class="token punctuation">,</span> <span class="token string">'ResNetV1d'</span><span class="token punctuation">,</span> <span class="token string">'ResNeXt'</span><span class="token punctuation">,</span> <span class="token string">'SSDVGG'</span><span class="token punctuation">,</span> <span class="token string">'HRNet'</span><span class="token punctuation">,</span> <span class="token string">'Res2Net'</span><span class="token punctuation">,</span>    <span class="token string">'HourglassNet'</span><span class="token punctuation">,</span> <span class="token string">'DetectoRS_ResNet'</span><span class="token punctuation">,</span> <span class="token string">'DetectoRS_ResNeXt'</span><span class="token punctuation">,</span> <span class="token string">'Darknet'</span><span class="token punctuation">,</span>    <span class="token string">'ResNeSt'</span><span class="token punctuation">,</span> <span class="token string">'TridentResNet'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果需要对骨架进行扩展，可以继承上述网络，然后通过<strong>注册器机制</strong>注册使用。一个典型用法为：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 骨架的预训练权重路径</span>pretrained<span class="token operator">=</span><span class="token string">'torchvision://resnet50'</span><span class="token punctuation">,</span>backbone<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'ResNet'</span><span class="token punctuation">,</span> <span class="token comment"># 骨架类名，后面的参数都是该类的初始化参数</span>    depth<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span>    num_stages<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>    out_indices<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    frozen_stages<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>    norm_cfg<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'BN'</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>     norm_eval<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    style<span class="token operator">=</span><span class="token string">'pytorch'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>通过 MMCV 中的注册器机制，你可以通过 dict 形式的配置来实例化任何已经注册的类</strong>，非常方便和灵活。</p><h4 id="2-1-2-Neck"><a href="#2-1-2-Neck" class="headerlink" title="2.1.2 Neck"></a>2.1.2 Neck</h4><p>neck 可以认为<strong>是 backbone 和 head 的连接层</strong>，主要负责对 backbone 的特征进行高效融合和增强，能够对输入的单尺度或者多尺度<strong>特征进行融合、增强输出</strong>等。</p><p><img src="https://pic1.zhimg.com/80/v2-f0975c00a32fa03a80860f9c09234bbc_720w.jpg" alt="img"></p><p>具体见文件：<code>mmdet/models/necks</code>，V2.7 已经实现的 neck 如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'FPN'</span><span class="token punctuation">,</span> <span class="token string">'BFP'</span><span class="token punctuation">,</span> <span class="token string">'ChannelMapper'</span><span class="token punctuation">,</span> <span class="token string">'HRFPN'</span><span class="token punctuation">,</span> <span class="token string">'NASFPN'</span><span class="token punctuation">,</span> <span class="token string">'FPN_CARAFE'</span><span class="token punctuation">,</span> <span class="token string">'PAFPN'</span><span class="token punctuation">,</span>    <span class="token string">'NASFCOS_FPN'</span><span class="token punctuation">,</span> <span class="token string">'RFP'</span><span class="token punctuation">,</span> <span class="token string">'YOLOV3Neck'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>最常用的应该是 FPN，一个典型用法是：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">neck<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'FPN'</span><span class="token punctuation">,</span>    in_channels<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">2048</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token comment"># 骨架多尺度特征图输出通道</span>    out_channels<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token comment"># 增强后通道输出</span>    num_outs<span class="token operator">=</span><span class="token number">5</span><span class="token comment"># 输出num_outs个多尺度特征图</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-1-3-Head"><a href="#2-1-3-Head" class="headerlink" title="2.1.3 Head"></a>2.1.3 Head</h4><p>目标检测算法<strong>输出一般包括分类和框坐标回归</strong>两个分支，不同算法 head 模块复杂程度不一样，灵活度比较高。在网络构建方面，<strong>理解目标检测算法主要是要理解 head 模块</strong>。几乎<strong>每个算法都包括一个独立的 head</strong>。</p><p><img src="https://pic2.zhimg.com/80/v2-fdd9a6232e62c75b143153dab8ba9bc1_720w.jpg" alt="img"></p><p>MMDetection 中 head 模块又划分为 <strong>two-stage 所需的 RoIHead</strong> 和 <strong>one-stage 所需的 DenseHead</strong>，也就是说所有的 one-stage 算法的 head 模块都在<code>mmdet/models/dense_heads</code>中，而 two-stage 算法还包括额外的<code>mmdet/models/roi_heads</code>。</p><p>目前 V2.7 中已经实现的 dense_heads 包括：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'AnchorFreeHead'</span><span class="token punctuation">,</span> <span class="token string">'AnchorHead'</span><span class="token punctuation">,</span> <span class="token string">'GuidedAnchorHead'</span><span class="token punctuation">,</span> <span class="token string">'FeatureAdaption'</span><span class="token punctuation">,</span>    <span class="token string">'RPNHead'</span><span class="token punctuation">,</span> <span class="token string">'GARPNHead'</span><span class="token punctuation">,</span> <span class="token string">'RetinaHead'</span><span class="token punctuation">,</span> <span class="token string">'RetinaSepBNHead'</span><span class="token punctuation">,</span> <span class="token string">'GARetinaHead'</span><span class="token punctuation">,</span>    <span class="token string">'SSDHead'</span><span class="token punctuation">,</span> <span class="token string">'FCOSHead'</span><span class="token punctuation">,</span> <span class="token string">'RepPointsHead'</span><span class="token punctuation">,</span> <span class="token string">'FoveaHead'</span><span class="token punctuation">,</span>    <span class="token string">'FreeAnchorRetinaHead'</span><span class="token punctuation">,</span> <span class="token string">'ATSSHead'</span><span class="token punctuation">,</span> <span class="token string">'FSAFHead'</span><span class="token punctuation">,</span> <span class="token string">'NASFCOSHead'</span><span class="token punctuation">,</span>    <span class="token string">'PISARetinaHead'</span><span class="token punctuation">,</span> <span class="token string">'PISASSDHead'</span><span class="token punctuation">,</span> <span class="token string">'GFLHead'</span><span class="token punctuation">,</span> <span class="token string">'CornerHead'</span><span class="token punctuation">,</span> <span class="token string">'YOLACTHead'</span><span class="token punctuation">,</span>    <span class="token string">'YOLACTSegmHead'</span><span class="token punctuation">,</span> <span class="token string">'YOLACTProtonet'</span><span class="token punctuation">,</span> <span class="token string">'YOLOV3Head'</span><span class="token punctuation">,</span> <span class="token string">'PAAHead'</span><span class="token punctuation">,</span>    <span class="token string">'SABLRetinaHead'</span><span class="token punctuation">,</span> <span class="token string">'CentripetalHead'</span><span class="token punctuation">,</span> <span class="token string">'VFNetHead'</span><span class="token punctuation">,</span> <span class="token string">'TransformerHead'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>而 roi_heads 比较杂，就不列出了。</p><p>需要注意的是：<strong>two-stage 或者 mutli-stage 算法，会额外包括一个区域提取器 roi extractor，用于将不同大小的 RoI 特征图统一成相同大小</strong>。</p><p>虽然 head 部分的网络构建比较简单，但是由于正负样本属性定义、正负样本采样和 bbox 编解码模块都在 head 模块中进行组合调用，故 MMDetection <strong>中最复杂的模块就是 head</strong>。在最后的整体流程部分会对该模块进行详细分析。</p><h4 id="2-1-4-Enhance"><a href="#2-1-4-Enhance" class="headerlink" title="2.1.4 Enhance"></a>2.1.4 Enhance</h4><p>enhance 是<strong>即插即用、能够对特征进行增强的模块</strong>，其具体代码可以<strong>通过 dict 形式注册到 backbone、neck 和 head 中</strong>，非常方便(目前还不完善)。常用的 enhance 模块是 <strong>SPP、ASPP、RFB、Dropout、Dropblock、DCN 和各种注意力模块</strong> SeNet、Non_Local、CBA 等。目前 MMDetection 中部分模块支持 enhance 的接入，例如 ResNet 骨架中的 plugins，这个部分的解读放在具体算法模块中讲解。</p><p><img src="https://pic3.zhimg.com/80/v2-65a706efe224f0b7ffc7f4fd7a65f2ca_720w.jpg" alt="img"></p><h4 id="2-1-5-BBox-Assigner"><a href="#2-1-5-BBox-Assigner" class="headerlink" title="2.1.5 BBox Assigner"></a>2.1.5 BBox Assigner</h4><p><strong>正负样本属性分配模块</strong>作用是<strong>进行正负样本定义或者正负样本分配</strong>（可能也包括忽略样本定义），正样本就是常说的前景样本（可以是任何类别），负样本就是背景样本。因为目标检测是一个同时进行分类和回归的问题，对于分类场景必然需要确定正负样本，否则无法训练。该模块至关重要，不同的正负样本分配策略会带来显著的性能差异，目前大部分目标检测算法都会对这个部分进行改进，至关重要。一些典型的分配策略如下：</p><p><img src="https://pic3.zhimg.com/80/v2-12bae70e2ea2e4afb05d0d8d3f38ca56_720w.jpg" alt="img"></p><p>对应的代码在<code>mmdet/core/bbox/assigners</code>中，V2.7 主要包括：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'BaseAssigner'</span><span class="token punctuation">,</span> <span class="token string">'MaxIoUAssigner'</span><span class="token punctuation">,</span> <span class="token string">'ApproxMaxIoUAssigner'</span><span class="token punctuation">,</span>     <span class="token string">'PointAssigner'</span><span class="token punctuation">,</span> <span class="token string">'ATSSAssigner'</span><span class="token punctuation">,</span> <span class="token string">'CenterRegionAssigner'</span><span class="token punctuation">,</span> <span class="token string">'GridAssigner'</span><span class="token punctuation">,</span>    <span class="token string">'HungarianAssigner'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-1-7-BBox-Encoder"><a href="#2-1-7-BBox-Encoder" class="headerlink" title="2.1.7 BBox Encoder"></a>2.1.7 BBox Encoder</h4><p>为了<strong>更好的收敛和平衡多个 loss</strong>，具体解决办法非常多，而 bbox 编解码策略也算其中一个，bbox <strong>编码阶段</strong>对应的是<strong>对正样本的 gt bbox 采用某种编码变换</strong>（反操作就是 bbox 解码），最简单的编码是对 gt bbox 除以图片宽高进行归一化以平衡分类和回归分支，一些典型的编解码策略如下：</p><p><img src="https://pic4.zhimg.com/80/v2-1f8d5e5e45886423df474d168452f50b_720w.jpg" alt="img"></p><p>对应的代码在<code>mmdet/core/bbox/coder</code>中，V2.7 主要包括：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'BaseBBoxCoder'</span><span class="token punctuation">,</span> <span class="token string">'PseudoBBoxCoder'</span><span class="token punctuation">,</span> <span class="token string">'DeltaXYWHBBoxCoder'</span><span class="token punctuation">,</span>    <span class="token string">'LegacyDeltaXYWHBBoxCoder'</span><span class="token punctuation">,</span> <span class="token string">'TBLRBBoxCoder'</span><span class="token punctuation">,</span> <span class="token string">'YOLOBBoxCoder'</span><span class="token punctuation">,</span>    <span class="token string">'BucketingBBoxCoder'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-1-8-Loss"><a href="#2-1-8-Loss" class="headerlink" title="2.1.8 Loss"></a>2.1.8 Loss</h4><p>Loss 通常都分为<strong>分类和回归 loss</strong>，其<strong>对网络 head 输出的预测值和 bbox encoder 得到的 targets 进行梯度下降</strong>迭代训练。</p><p>loss 的设计也是各大算法重点改进对象，常用的 loss 如下：</p><p><img src="https://pic4.zhimg.com/80/v2-686b0b9ac6a82f9945ae454d18783227_720w.jpg" alt="img"></p><p>对应的代码在<code>mmdet/models/losses</code>中，V2.7 主要包括：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'cross_entropy'</span><span class="token punctuation">,</span> <span class="token string">'binary_cross_entropy'</span><span class="token punctuation">,</span>    <span class="token string">'mask_cross_entropy'</span><span class="token punctuation">,</span> <span class="token string">'CrossEntropyLoss'</span><span class="token punctuation">,</span> <span class="token string">'sigmoid_focal_loss'</span><span class="token punctuation">,</span>    <span class="token string">'FocalLoss'</span><span class="token punctuation">,</span> <span class="token string">'smooth_l1_loss'</span><span class="token punctuation">,</span> <span class="token string">'SmoothL1Loss'</span><span class="token punctuation">,</span> <span class="token string">'balanced_l1_loss'</span><span class="token punctuation">,</span>    <span class="token string">'BalancedL1Loss'</span><span class="token punctuation">,</span> <span class="token string">'mse_loss'</span><span class="token punctuation">,</span> <span class="token string">'MSELoss'</span><span class="token punctuation">,</span> <span class="token string">'iou_loss'</span><span class="token punctuation">,</span> <span class="token string">'bounded_iou_loss'</span><span class="token punctuation">,</span>    <span class="token string">'IoULoss'</span><span class="token punctuation">,</span> <span class="token string">'BoundedIoULoss'</span><span class="token punctuation">,</span> <span class="token string">'GIoULoss'</span><span class="token punctuation">,</span> <span class="token string">'DIoULoss'</span><span class="token punctuation">,</span> <span class="token string">'CIoULoss'</span><span class="token punctuation">,</span> <span class="token string">'GHMC'</span><span class="token punctuation">,</span>    <span class="token string">'GHMR'</span><span class="token punctuation">,</span> <span class="token string">'reduce_loss'</span><span class="token punctuation">,</span> <span class="token string">'weight_reduce_loss'</span><span class="token punctuation">,</span> <span class="token string">'weighted_loss'</span><span class="token punctuation">,</span> <span class="token string">'L1Loss'</span><span class="token punctuation">,</span>    <span class="token string">'l1_loss'</span><span class="token punctuation">,</span> <span class="token string">'isr_p'</span><span class="token punctuation">,</span> <span class="token string">'carl_loss'</span><span class="token punctuation">,</span> <span class="token string">'AssociativeEmbeddingLoss'</span><span class="token punctuation">,</span>    <span class="token string">'GaussianFocalLoss'</span><span class="token punctuation">,</span> <span class="token string">'QualityFocalLoss'</span><span class="token punctuation">,</span> <span class="token string">'DistributionFocalLoss'</span><span class="token punctuation">,</span>    <span class="token string">'VarifocalLoss'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看出 MMDetection 中已经实现了非常多的 loss，可以直接使用。</p><h4 id="2-1-9-Training-tricks"><a href="#2-1-9-Training-tricks" class="headerlink" title="2.1.9 Training tricks"></a>2.1.9 Training tricks</h4><p>训练技巧非常多，常说的调参很大一部分工作都是在设置这部分超参。这部分内容比较杂乱，很难做到完全统一，目前主流的 tricks 如下所示:</p><p><img src="https://pic3.zhimg.com/80/v2-569a12b6d4a20f8619a27b48d5b2fa42_720w.jpg" alt="img"></p><p>MMDetection 目前这部分还会继续完善。</p><h3 id="2-2-测试核心组件"><a href="#2-2-测试核心组件" class="headerlink" title="2.2 测试核心组件"></a>2.2 测试核心组件</h3><p>测试核心组件和训练非常类似，但是简单很多，除了必备的网络构建部分外( <strong>backbone、neck、head 和 enhance</strong> )，不需要正负样本定义、正负样本采样和 loss 计算三个最难的部分，但是其<strong>额外需要一个 bbox 后处理模块</strong>和<strong>测试 trick</strong>。</p><h4 id="2-2-1-BBox-Decoder"><a href="#2-2-1-BBox-Decoder" class="headerlink" title="2.2.1 BBox Decoder"></a>2.2.1 BBox Decoder</h4><p><strong>训练时候进行了编码，那么对应的测试环节需要进行解码。</strong>根据编码的不同，解码也是不同的。举个简单例子：假设训练时候对宽高是直接除以图片宽高进行归一化的，那么解码过程也仅仅需要乘以图片宽高即可。其代码和 bbox encoder 放在一起，在<code>mmdet/core/bbox/coder</code>中。</p><h4 id="2-2-2-BBox-PostProcess"><a href="#2-2-2-BBox-PostProcess" class="headerlink" title="2.2.2 BBox PostProcess"></a>2.2.2 BBox PostProcess</h4><p>在<strong>得到原图尺度 bbox 后，由于可能会出现重叠 bbox 现象</strong>，故一般都需要进行后处理，最常用的后处理就是<strong>非极大值抑制以及其变种</strong>。</p><p>其对应的文件在<code>mmdet/core/post_processing</code>中，V2.7 主要包括：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">__all__ <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token string">'multiclass_nms'</span><span class="token punctuation">,</span> <span class="token string">'merge_aug_proposals'</span><span class="token punctuation">,</span> <span class="token string">'merge_aug_bboxes'</span><span class="token punctuation">,</span>    <span class="token string">'merge_aug_scores'</span><span class="token punctuation">,</span> <span class="token string">'merge_aug_masks'</span><span class="token punctuation">,</span> <span class="token string">'fast_nms'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-2-3-Testing-tricks"><a href="#2-2-3-Testing-tricks" class="headerlink" title="2.2.3 Testing tricks"></a>2.2.3 Testing tricks</h4><p>为了提高检测性能，测试阶段也会采用 trick。这个阶段的 tricks 也非常多，难以完全统一，最典型的是<strong>多尺度测试</strong>以及<strong>各种模型集成手段</strong>，典型配置如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token builtin">dict</span><span class="token punctuation">(</span>    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'MultiScaleFlipAug'</span><span class="token punctuation">,</span>    img_scale<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1333</span><span class="token punctuation">,</span> <span class="token number">800</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    flip<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>    transforms<span class="token operator">=</span><span class="token punctuation">[</span>        <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'Resize'</span><span class="token punctuation">,</span> keep_ratio<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'RandomFlip'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'Normalize'</span><span class="token punctuation">,</span> <span class="token operator">**</span>img_norm_cfg<span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'Pad'</span><span class="token punctuation">,</span> size_divisor<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'ImageToTensor'</span><span class="token punctuation">,</span> keys<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'img'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'Collect'</span><span class="token punctuation">,</span> keys<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'img'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://pic3.zhimg.com/80/v2-16e307727f0c3e941ec72c21f214b982_720w.jpg" alt="img"></p><h3 id="2-3-训练测试算法流程"><a href="#2-3-训练测试算法流程" class="headerlink" title="2.3 训练测试算法流程"></a>2.3 训练测试算法流程</h3><p>在分析完每个训练流程的各个核心组件后，为了方便大家理解整个算法构建，下面分析 MMDetection 是<strong>如何组合各个组件进行训练的</strong>，这里<strong>以 one-stage 检测器为例</strong>，two-stage 也比较类似。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SingleStageDetector</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 构建backbone、neck和head</span>        self<span class="token punctuation">.</span>backbone <span class="token operator">=</span> build_backbone<span class="token punctuation">(</span>backbone<span class="token punctuation">)</span>        <span class="token keyword">if</span> neck <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>neck <span class="token operator">=</span> build_neck<span class="token punctuation">(</span>neck<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bbox_head <span class="token operator">=</span> build_head<span class="token punctuation">(</span>bbox_head<span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward_train</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token comment"># 先运行backbone+neck进行特征提取</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>extract_feat<span class="token punctuation">(</span>img<span class="token punctuation">)</span>        <span class="token comment"># 对head进行forward train，输出loss</span>        losses <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">.</span>forward_train<span class="token punctuation">(</span>            x<span class="token punctuation">,</span>             img_metas<span class="token punctuation">,</span>             gt_bboxes<span class="token punctuation">,</span>                                        gt_labels<span class="token punctuation">,</span>             gt_bboxes_ignore        <span class="token punctuation">)</span>        <span class="token keyword">return</span> losses  <span class="token keyword">def</span> <span class="token function">simple_test</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 先运行backbone+neck进行特征提取</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>extract_feat<span class="token punctuation">(</span>img<span class="token punctuation">)</span>        <span class="token comment"># head输出预测特征图</span>        outs <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment"># bbox解码和还原</span>        bbox_list <span class="token operator">=</span> self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">.</span>get_bboxes<span class="token punctuation">(</span>            <span class="token operator">*</span>outs<span class="token punctuation">,</span>             img_metas<span class="token punctuation">,</span>             rescale<span class="token operator">=</span>rescale        <span class="token punctuation">)</span>        <span class="token comment"># 重组结果返回</span>        bbox_results <span class="token operator">=</span> <span class="token punctuation">[</span>            bbox2result<span class="token punctuation">(</span>                det_bboxes<span class="token punctuation">,</span>                 det_labels<span class="token punctuation">,</span>                self<span class="token punctuation">.</span>bbox_head<span class="token punctuation">.</span>num_classes            <span class="token punctuation">)</span>            <span class="token keyword">for</span> det_bboxes<span class="token punctuation">,</span> det_labels <span class="token keyword">in</span> bbox_list        <span class="token punctuation">]</span>        <span class="token keyword">return</span> bbox_results<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>以上就是整个检测器算法训练和测试最简逻辑，可以发现训练部分最核心的就是<code>bbox_head.forward_train</code>，测试部分最核心的是<code>bbox_head.get_bboxes</code>，下面单独简要分析。</p><h4 id="2-3-1-bbox-head-forward-train"><a href="#2-3-1-bbox-head-forward-train" class="headerlink" title="2.3.1 bbox_head.forward_train"></a>2.3.1 bbox_head.forward_train</h4><p>forward_train 是通用函数，如下所示：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward_train</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 调用每个head自身的forward方法</span>    outs <span class="token operator">=</span> self<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token keyword">if</span> gt_labels <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>        loss_inputs <span class="token operator">=</span> outs <span class="token operator">+</span> <span class="token punctuation">(</span>gt_bboxes<span class="token punctuation">,</span> img_metas<span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        loss_inputs <span class="token operator">=</span> outs <span class="token operator">+</span> <span class="token punctuation">(</span>gt_bboxes<span class="token punctuation">,</span> gt_labels<span class="token punctuation">,</span> img_metas<span class="token punctuation">)</span>    <span class="token comment"># 计算每个head自身的loss方法</span>    losses <span class="token operator">=</span> self<span class="token punctuation">.</span>loss<span class="token punctuation">(</span>        <span class="token operator">*</span>loss_inputs<span class="token punctuation">,</span>         gt_bboxes_ignore<span class="token operator">=</span>gt_bboxes_ignore    <span class="token punctuation">)</span>    <span class="token comment"># 返回</span>    <span class="token keyword">return</span> losses<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对于不同的 head，虽然 forward 内容不一样，但是依然可以抽象为： <code>outs = self(x)</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> feats<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token comment"># 多尺度特征图，一个一个迭代进行forward_single</span>   <span class="token keyword">return</span> multi_apply<span class="token punctuation">(</span>self<span class="token punctuation">.</span>forward_single<span class="token punctuation">,</span> feats<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">forward_single</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token comment"># 运行各个head独特的head forward方法，得到预测图</span>   <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>   <span class="token keyword">return</span> cls_score<span class="token punctuation">,</span> bbox_pred<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>而对于不同的 head，其 loss 计算部分也比较复杂，可以简单抽象为：<code>losses = self.loss(...)</code></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">loss</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 1 生成anchor-base需要的anchor或者anchor-free需要的points</span>    <span class="token comment"># 2 利用gt bbox对特征图或者anchor计算其正负和忽略样本属性</span>    <span class="token comment"># 3 进行正负样本采样</span>    <span class="token comment"># 4 对gt bbox进行bbox编码</span>    <span class="token comment"># 5 loss计算，并返回</span>    <span class="token keyword">return</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>loss_cls<span class="token operator">=</span>losses_cls<span class="token punctuation">,</span> loss_bbox<span class="token operator">=</span>losses_bbox<span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-3-2-bbox-head-get-bboxes"><a href="#2-3-2-bbox-head-get-bboxes" class="headerlink" title="2.3.2 bbox_head.get_bboxes"></a>2.3.2 bbox_head.get_bboxes</h4><p>get_bboxes函数更加简单</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_bboxes</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token comment"># 1 生成anchor-base需要的anchor或者anchor-free需要的points</span>   <span class="token comment"># 2 遍历每个输出层，遍历batch内部的每张图片，对每张图片先提取指定个数的预测结果，缓解后面后处理压力；对保留的位置进行bbox解码和还原到原图尺度</span>   <span class="token comment"># 3 统一nms后处理</span>   <span class="token keyword">return</span> det_bboxes<span class="token punctuation">,</span> det_labels<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h2><p>本文重点分析了一个目标检测器是如何通过多个核心组件堆叠而成，不涉及具体代码，大家只需要总体把握即可，其中最应该了解的是：<strong>任何一个目标检测算法都可以分成 n 个核心组件，组件和组件之间是隔离的，方便复用和设计</strong>。当面对一个新算法时候我们可以先分析其主要是改进了哪几个核心组件，然后就可以高效的掌握该算法。</p><p>另外还有一些重要的模块没有分析，特别是 dataset、dataloader 和分布式训练相关的检测代码，由于篇幅限制就不介绍了，如有需要欢迎在评论区留言。</p><p>再次欢迎大家使用 MMDetection，也非常欢迎社区贡献！</p><p>最后附上总图：</p><p><img src="https://pic3.zhimg.com/80/v2-c4e6229a1fd42692d090108481be34a6_720w.jpg" alt="img"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>智能视频感知与分析工作室招新考核题目</title>
      <link href="/2022/03/18/zhao-xin-kao-he/"/>
      <url>/2022/03/18/zhao-xin-kao-he/</url>
      
        <content type="html"><![CDATA[<h1 id="智能视频感知与分析工作室招新考核题目"><a href="#智能视频感知与分析工作室招新考核题目" class="headerlink" title="智能视频感知与分析工作室招新考核题目"></a>智能视频感知与分析工作室招新考核题目</h1><h2 id="时间：2022-x2F-03-x2F-19-2022-x2F-03-x2F-26"><a href="#时间：2022-x2F-03-x2F-19-2022-x2F-03-x2F-26" class="headerlink" title="时间：2022/03/19 ~ 2022/03/26"></a>时间：2022/03/19 ~ 2022/03/26</h2><p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20220318222607061.png" alt="image-20220318222607061"></p><hr><h2 id="技术类（算法设计组、轻量化及产品设计组）"><a href="#技术类（算法设计组、轻量化及产品设计组）" class="headerlink" title="技术类（算法设计组、轻量化及产品设计组）"></a>技术类（算法设计组、轻量化及产品设计组）</h2><p><strong>组队题目要求：</strong></p><ul><li><strong>自由组队，每队<code>1-3</code>人，分工明确；</strong></li><li><strong>由题目清单选择题目，亦可自选题目；</strong></li></ul><p><strong>项目规范要求：</strong></p><ul><li>需要程序实现，制作PPT并展示（5-10分钟展示时间）；</li><li>上传至<code>Github</code>并提交链接（如自选题目涉及自有知识产权，可不提供）；</li><li>提交实验结果录屏演示；</li><li>工程结构规范、注释丰富；</li><li>必须包含<code>Readme.md</code>说明文件；</li><li>语言不限；</li><li>如能将实现过程进行记录汇总为Blog可作为加分项。</li></ul><p><strong>题目清单：</strong></p><ol><li>基于U-Net的图像分割；包括：搭建U-Net网络结构、实验数据处理、网络的训练与测试。（<a href="https://drive.google.com/file/d/1OQBErLlZ-xNGz5PsoOaJwjG9E-N4dcbE/view?usp=sharing">数据集链接</a>）提示：很多博客都有类似讲解，复现难度不大，数据集很小（30训练+30测试），CPU也可训练。不要求准确率，自主编写代码实现即可。</li><li>基于OpenCV的实时人脸识别（可直接调用OpenCV人脸检测器，但所使用原理方法需要阐述清楚）；</li><li>基于Dlib的人脸识别（可直接调用，阐述其与OpenCV人脸检测器方法的原理及实现区别）；</li><li>基于Yolo的图像目标检测技术（不要求训练，只需要利用官方给定权重预测图像即可）；</li><li>LeNet手写体字符识别；</li><li>基于OpenMP、MPI、CUDA或OneAPI（四选一、或结合）的卷积操作加速（C\C++实现可优先考虑）；</li><li>基于嵌入式设备（例如有树莓派等，可做）目标检测或图片分类；</li><li>机械设计制造、实体模型设计、3D打印等（可自选的相关内容）；</li><li>复现任一较新的深度学习方法（可自选的相关内容）。</li></ol><h2 id="非技术类（策划管理组及UI交互设计组）"><a href="#非技术类（策划管理组及UI交互设计组）" class="headerlink" title="非技术类（策划管理组及UI交互设计组）"></a>非技术类（策划管理组及UI交互设计组）</h2><p><strong>要求：</strong></p><ul><li><strong>自由组队，每队<code>1-2</code>人；</strong></li><li><strong>从以下题目任选其一，可撰写策划案、制作PPT或图形化界面，亦可自选主题；</strong></li><li><strong>制作并提交项目计划书(提交PPT、或设计草图等)。</strong></li></ul><p><strong>题目清单</strong></p><ol><li><p><code>策划</code>调研国内外智能监控视频分析系统的发展趋势，分析各系统不足与优势，根据调研结果，制作不少于10页的PPT（项目计划书格式），拟定未来基本研究方向（要求：提交PPT或其他策划资料）。</p></li><li><p><code>策划</code>调研校园暴力事件识别系统的潜在应用场景，深入剖析行业痛点需求，给出分析思考，结合相应技术进行分析更佳，提交PPT及其他材料；</p></li><li><p><code>策划</code>策划结合人工智能技术，剖析身边技术方案、产品等方面的需求，给出整体性的策划方法，明确需求、市场、技术背景等要素。</p></li><li><p><code>GUI</code>复现QT-GUI项目PyDracula，进行自定义美化编辑，并在窗口内实现：读取本地视频进行播放，在窗口可控制视频的播放与暂停。<a href="https://github.com/Wanderson-Magalhaes/Modern_GUI_PyDracula_PySide6_or_PyQt6">项目地址</a>（要求：提交UI运行视频与代码github链接）</p></li><li><p><code>设计</code>独立设计的任一作品（例如图片、海报等，自有版权的记得加水印等措施），阐明设计思想，设计理念；</p></li></ol><hr><h2 id="组队及报名地址"><a href="#组队及报名地址" class="headerlink" title="组队及报名地址"></a>组队及报名地址</h2><h3 id="智能视频感知与分析工作室招新考核登记表"><a href="#智能视频感知与分析工作室招新考核登记表" class="headerlink" title="智能视频感知与分析工作室招新考核登记表"></a><a href="https://docs.qq.com/sheet/DWGxweHlkVldZQ1RE">智能视频感知与分析工作室招新考核登记表</a></h3>]]></content>
      
      
      
        <tags>
            
            <tag> 考核题目 </tag>
            
            <tag> 智能视频感知与分析工作室 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/03/15/hello-world/"/>
      <url>/2022/03/15/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span>$ hexo server$ hexo server$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ZerotTier内网穿透+VSCode配置远程开发+tmux基础操作</title>
      <link href="/2021/11/11/zerotier-nei-wang-chuan-tou-vscode-pei-zhi-yuan-cheng-kai-fa-tmux-ji-chu-cao-zuo/"/>
      <url>/2021/11/11/zerotier-nei-wang-chuan-tou-vscode-pei-zhi-yuan-cheng-kai-fa-tmux-ji-chu-cao-zuo/</url>
      
        <content type="html"><![CDATA[<h1 id="ZerotTier内网穿透-VSCode配置远程开发-tmux基础操作"><a href="#ZerotTier内网穿透-VSCode配置远程开发-tmux基础操作" class="headerlink" title="ZerotTier内网穿透+VSCode配置远程开发+tmux基础操作"></a>ZerotTier内网穿透+VSCode配置远程开发+tmux基础操作</h1><ul><li>平时在开发深度学习等相关项目时，往往需要大型服务器或工作站的支持，远程开发时使用向日葵等远程桌面软件往往不是那么明知，多人使用冲突、限速卡顿，代码体验极其**。</li><li>那么有没有更好的解决方案呢？单就小型团队而言，如果能将所有散布在各个实验室的机器使用内网穿透统一接入同一局域网，便可方便的使用ssh连接，搭配<code>vscode</code>的<code>ssh-remote</code>插件与<code>tmux</code>进行终端复用，便可实现本地无感的远程开发。</li></ul><h2 id="1-ZeroTier配置内网穿透"><a href="#1-ZeroTier配置内网穿透" class="headerlink" title="1. ZeroTier配置内网穿透"></a>1. ZeroTier配置内网穿透</h2><h3 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1 简介"></a>1.1 简介</h3><p><a href="https://www.zerotier.com/">ZeroTier </a>作为一款非常简单易用的内网穿透工具，<strong>不需要复杂配置</strong>，就能实现虚拟局域网的组建，让你可以在外也能连回实验室的NAS、服务器获取数据、远程开发。</p><h3 id="1-2-费用"><a href="#1-2-费用" class="headerlink" title="1.2 费用"></a>1.2 费用</h3><p>免费网络限制 100 台设备，超过了就要付费。100 台对于个人或者小团队使用来说都足够了。</p><h3 id="1-3-支持平台"><a href="#1-3-支持平台" class="headerlink" title="1.3 支持平台"></a>1.3 <a href="https://www.zerotier.com/download/">支持平台</a></h3><p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024133645939.png" alt="image-20211024133645939"></p><h3 id="1-4-使用步骤"><a href="#1-4-使用步骤" class="headerlink" title="1.4 使用步骤"></a>1.4 使用步骤</h3><ul><li>说明：如已有Network ID，直接执行步骤3，安装<a href="https://www.zerotier.com/download/">客户端</a>，加入Network ID即可</li></ul><ol><li><p>注册ZeroTier ID：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024141003246.png" alt="image-20211024141003246" style="zoom:50%;"></li><li><p>创建私有局域网，得到Network ID与子网地址：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024135727789.png" alt="image-20211024135727789" style="zoom:80%;"></li><li><p>安装<a href="https://www.zerotier.com/download/">客户端</a>，<strong>加入Network ID</strong></p><ul><li>windows下<code>ipconfig</code>，ubuntu下<code>ifconfig</code>出现ZeroTier的网段后说明连接成功（也可直接ping其他ip验证）：</li></ul><p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024140430169.png" alt="image-20211024140430169"></p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024143314079.png" alt="image-20211024143314079" style="zoom: 80%;"></li><li><p>管理：</p><ul><li>可以选择子网地址：</li></ul><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024135944372.png" alt="image-20211024135944372" style="zoom: 67%;"><ul><li><p>查看连接客户端，第三列机器即为所有机器的局域网IP，接下来的步骤即shh该ip地址：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024140138785.png" alt="image-20211024140138785" style="zoom: 50%;"></li></ul></li></ol><hr><h2 id="2-VSCode配置Remote-SSH插件"><a href="#2-VSCode配置Remote-SSH插件" class="headerlink" title="2. VSCode配置Remote-SSH插件"></a>2. VSCode配置Remote-SSH插件</h2><h3 id="2-1-安装OpenSSH"><a href="#2-1-安装OpenSSH" class="headerlink" title="2.1 安装OpenSSH"></a>2.1 安装OpenSSH</h3><ul><li><p>Remote-SSH插件是基于SHH的，所以首先要确保本机和远程服务器都安装好了OpenSHH</p></li><li><p>Ubuntu：</p><p>ubuntu默认并没有安装ssh服务，如果通过ssh链接ubuntu，需要自己手动安装ssh-server。判断是否安装ssh服务，可以通过如下命令进行：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">ssh</span> localhost   ssh: connect to <span class="token function">host</span> localhost port <span class="token number">22</span>: Connection refused   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>若出现上述情况，表示还没有安装，可通过以下命令安装：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> openssh-server<span class="token function">sudo</span> /etc/init.d/ssh start<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>安装启动后，可以通过如下命令查看服务是否正确启动：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">ps</span> -e<span class="token operator">|</span><span class="token function">grep</span> <span class="token function">ssh</span><span class="token number">6212</span> ?        00:00:00 sshd  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>如上表示安装成功</p></li><li><p>Windows：</p><p>Win10现在已经支持OpenSSH，可在设置-&gt;应用-&gt;可选功能中查看：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024143529860.png" alt="image-20211024143529860" style="zoom:67%;"><p>若未安装，直接使用添加功能-&gt;搜索SSH安装即可，其他安装方式可参考<a href="https://www.jianshu.com/p/f8ba3e51d60e">Windows安装OpenSSH支持SSH - 简书 (jianshu.com)</a></p></li><li><p>Mac OS：</p><p>Mac OS X系统已经默认安装了SSH，但是SSH服务并未启用，启用SSH服务的方法：</p><p>系统偏好设置-&gt;共享-&gt;勾选“远程登陆”：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024143911895.png" alt="image-20211024143911895" style="zoom:67%;"></li><li><p>验证ssh：</p><p>使用：<code>ssh &lt;username&gt;@&lt;ip&gt;</code>连接任意主机，输入密码连接成功即可：</p><p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024144324785.png" alt="image-20211024144324785"></p></li></ul><h3 id="2-2-配置Remote-SSH插件"><a href="#2-2-配置Remote-SSH插件" class="headerlink" title="2.2 配置Remote-SSH插件"></a>2.2 配置Remote-SSH插件</h3><ul><li><p>安装Remote-SSH：</p><p>在vscode的拓展商店中搜索Remote-SSH进行安装，安装完成后左侧会出现以下按钮：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024145412604.png" alt="image-20211024145412604" style="zoom:50%;"></li><li><p>配置config文件：</p><p>进入该拓展，点击SSH TARGETS上面的设置按钮，选择所要配置的ssh config文件（一般为第一个）：</p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024145620858.png" alt="image-20211024145620858" style="zoom:67%;"><p>配置远程服务器的名称、ip与用户名：</p><ul><li><p><code>Host</code>: 主机的自定义显示名，可以随便起</p></li><li><p><code>HostName</code>: 登录远程主机的内网IP，即1.4中主机内网穿透后得到的虚拟IP</p></li><li><p><code>User</code>: 登录远程主机的用户名</p></li><li><p><code>Port</code>: 用于登录远程主机的端口（可选）</p></li><li><p><code>IdentityFile</code>: 本地的id_rsa的路径（用于免密登陆的私钥）（多人使用不推荐配置私钥免密）（可选）</p></li></ul></li></ul><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024150620523.png" alt="image-20211024150620523" style="zoom:80%;"><ul><li>远程连接测试：</li></ul><p>​    配置完成后，该窗口下会出现所配置的主机，可以在新窗口下进行连接：</p><p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024151155738.png" alt="image-20211024151155738"></p><p>（第一次连接需要选择服务器操作系统）-&gt; 输入密码-&gt;等待服务器安装vscode远程端-&gt;打开远程项目文件夹后即可开始使用，所有的使用均和本地使用无任何差异：    </p><p><img src="https://gitee.com/renjunjie-xdu/PicGo_img/raw/master/images/image-20211024151750787.png" alt="image-20211024151750787"></p><hr><h2 id="3-使用Tmux保证会话持续运行"><a href="#3-使用Tmux保证会话持续运行" class="headerlink" title="3.  使用Tmux保证会话持续运行"></a>3.  使用Tmux保证会话持续运行</h2><h3 id="3-1-Tmux简介"><a href="#3-1-Tmux简介" class="headerlink" title="3.1 Tmux简介"></a>3.1 Tmux简介</h3><ul><li><strong>目的：****避免训练过程中因为本地Terminal关闭后服务器上的进程也被关闭。</strong></li></ul><h4 id="为什么需要终端复用？"><a href="#为什么需要终端复用？" class="headerlink" title="为什么需要终端复用？"></a><a href="https://www.ruanyifeng.com/blog/2019/10/tmux.html">为什么需要终端复用</a>？</h4><blockquote><p>命令行的典型使用方式是，打开一个终端窗口（terminal window，以下简称”窗口”），在里面输入命令。<strong>用户与计算机的这种临时的交互，称为一次”会话”（session）</strong> 。</p></blockquote><blockquote><p>会话的一个重要特点是，窗口与其中启动的进程是连在一起的。打开窗口，会话开始；关闭窗口，会话结束，会话内部的进程也会随之终止，不管有没有运行完。</p></blockquote><blockquote><p>一个典型的例子就是，SSH 登录远程计算机，打开一个远程窗口执行命令。这时，网络突然断线，再次登录的时候，是找不回上一次执行的命令的。因为上一次 SSH 会话已经终止了，里面的进程也随之消失了。</p><p>为了解决这个问题，会话与窗口可以”解绑”：窗口关闭时，会话并不终止，而是继续运行，等到以后需要的时候，再让会话”绑定”其他窗口。</p></blockquote><h4 id="Tmux-终端复用的作用？"><a href="#Tmux-终端复用的作用？" class="headerlink" title="Tmux 终端复用的作用？"></a>Tmux 终端复用的作用？</h4><p>（1）它允许在单个窗口中，同时访问多个会话。这对于同时运行多个命令行程序很有用。</p><p>（2） 它可以让新窗口”接入”已经存在的会话。</p><p>（3）它允许每个会话有多个连接窗口，因此可以多人实时共享会话。</p><p>（4）它还支持窗口任意的垂直和水平拆分。</p><h3 id="3-2-Tmux的安装"><a href="#3-2-Tmux的安装" class="headerlink" title="3.2 Tmux的安装"></a>3.2 Tmux的安装</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># Ubuntu 或 Debian</span>$ <span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> tmux<span class="token comment"># CentOS 或 Fedora</span>$ <span class="token function">sudo</span> yum <span class="token function">install</span> tmux<span class="token comment"># Mac</span>$ brew <span class="token function">install</span> tmux<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-3-Tmux使用常用命令"><a href="#3-3-Tmux使用常用命令" class="headerlink" title="3.3 Tmux使用常用命令"></a>3.3 Tmux使用常用命令</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ tmux new -s <span class="token operator">&lt;</span>窗口名<span class="token operator">&gt;</span> <span class="token comment"># 打开新窗口</span>ctrl+b d  <span class="token comment"># 分离窗口</span>$ tmux info  <span class="token comment"># 列出当前所有 Tmux 会话信息</span>$ tmux attach -t <span class="token operator">&lt;</span>窗口名<span class="token operator">&gt;</span> <span class="token comment"># 连接窗口</span>ctrl+b %<span class="token comment"># 分割窗口</span>ctrl+b s <span class="token comment"># 切换窗口</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h2 id="4-远程文件拷贝"><a href="#4-远程文件拷贝" class="headerlink" title="4. 远程文件拷贝"></a>4. 远程文件拷贝</h2><ul><li><p>直接使用以下命令即可：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ <span class="token function">scp</span> <span class="token operator">&lt;</span>本地文件路径<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>远程主机用户名<span class="token operator">&gt;</span>@<span class="token operator">&lt;</span>远程主机ip<span class="token operator">&gt;</span>:<span class="token operator">&lt;</span>要拷贝到的远程主机路径<span class="token operator">&gt;</span><span class="token comment">#或</span>$ <span class="token function">scp</span> <span class="token operator">&lt;</span>远程主机用户名<span class="token operator">&gt;</span>@<span class="token operator">&lt;</span>远程主机ip<span class="token operator">&gt;</span>:<span class="token operator">&lt;</span>要拷贝的远程主机文件路径<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>拷贝到的本地文件路径<span class="token operator">&gt;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>例如:</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ <span class="token function">scp</span> .<span class="token punctuation">\</span>labels.zip hp3090@192.168.192.164:/media/hp3090/HDD-2T/renjunjie/WSOL_RS/dataset/C45V2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 内网穿透 </tag>
            
            <tag> tmux </tag>
            
            <tag> 远程开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>遥感常用数据集介绍</title>
      <link href="/2020/12/07/yao-gan-shu-ju-ji-zheng-li/"/>
      <url>/2020/12/07/yao-gan-shu-ju-ji-zheng-li/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>【转载】：<a href="https://aistudio.baidu.com/aistudio/projectdetail/882508">遥感影像数据集汇总 - 飞桨AI Studio - 人工智能学习实训社区 (baidu.com)</a><br>在使用深度学习处理遥感影像的过程中，经常在数据集上遇到各种问题：</p><p> 找不到针对自己任务的数据集<br> 数据集网址404或龟速下载<br> 数据集在空间分辨率、图像尺寸等方面不符合需求</p><p>现在，<strong>遥感影像数据集汇总</strong> 项目可以助你解决上述问题。<br>该项目是一个遥感影像领域常用的深度学习数据集的汇总，包括数据集<strong>基本信息</strong>，并附上数据集<strong>源地址</strong>与<strong>Aistudi备份链接（包括详细的类别信息，提供高速下载）</strong>。</p><p>目前本项目共收录</p><ul><li><p>图像分类数据集27个（整理完结）；</p></li><li><p>目标检测数据集31+个（整理完结）；</p></li><li><p>图像分割数据集36+个（整理完结）；</p></li><li><p>变化数据集14个；</p></li><li><p>高光谱分类3个（整理完结）；</p></li><li><p>高光谱检测2个（整理完结）；</p></li><li><p>高光谱分割8个（整理完结）；</p></li><li><p>多标签分类2个（整理完结）；</p></li><li><p>视频跟踪1个（整理完结）；</p></li></ul><h1 id="遥感影像场景分类"><a href="#遥感影像场景分类" class="headerlink" title="遥感影像场景分类"></a>遥感影像场景分类</h1><p><strong>收集网络中开源的、关于遥感影像 场景分类的深度学习数据集</strong></p><p>main ref </p><blockquote><p><a href="https://captain-whu.github.io/DiRS/">https://captain-whu.github.io/DiRS/</a><br><a href="https://mp.weixin.qq.com/s/kThlSqItIwuCFTIfC_-5lQ">https://mp.weixin.qq.com/s/kThlSqItIwuCFTIfC_-5lQ</a></p></blockquote><h2 id="Moving-and-Stationary-TargetAcquisition-and-Recognition-MSTAR-8class"><a href="#Moving-and-Stationary-TargetAcquisition-and-Recognition-MSTAR-8class" class="headerlink" title="Moving and Stationary TargetAcquisition and Recognition (MSTAR) 8class"></a>Moving and Stationary TargetAcquisition and Recognition (MSTAR) 8class</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">368 * 368 * 3</td><td>8</td><td>9466</td><td>0.3m</td><td>STARLOS SAR</td><td>1996</td><td>Defense Advanced Research Projects Agency and the Air Force Research Laboratory</td></tr></tbody></table><ul><li>源地址：<a href="https://www.kaggle.com/atreyamajumdar/mstar-dataset-8-classes">https://www.kaggle.com/atreyamajumdar/mstar-dataset-8-classes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/78849">https://aistudio.baidu.com/aistudio/datasetdetail/78849</a></li></ul><h2 id="UCMerced-LandUse"><a href="#UCMerced-LandUse" class="headerlink" title="UCMerced_LandUse"></a>UCMerced_LandUse</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>21</td><td>21 * 100 = 2100</td><td>1 foot</td><td>USGS</td><td>2010</td><td>加利福尼亚大学</td></tr></tbody></table><ul><li>源地址：<a href="http://weegee.vision.ucmerced.edu/datasets/landuse.html">http://weegee.vision.ucmerced.edu/datasets/landuse.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/51628">https://aistudio.baidu.com/aistudio/datasetdetail/51628</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1ryrLb5HnCw0Uxzbpva9Vnw">https://pan.baidu.com/s/1ryrLb5HnCw0Uxzbpva9Vnw</a>  提取码：n71j</li></ul><h2 id="WHU-RS19"><a href="#WHU-RS19" class="headerlink" title="WHU-RS19"></a>WHU-RS19</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">600 * 600 * 3</td><td>19</td><td>19 * 50± = 1005</td><td>最高0.5 m</td><td>GoogleEarth</td><td>2012</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="http://captain.whu.edu.cn/repository.html">http://captain.whu.edu.cn/repository.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/51733">https://aistudio.baidu.com/aistudio/datasetdetail/51733</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1-w68vcKpj0NYlOltEWdi6w">https://pan.baidu.com/s/1-w68vcKpj0NYlOltEWdi6w</a>  提取码：gpjd</li></ul><h2 id="RSSCN7"><a href="#RSSCN7" class="headerlink" title="RSSCN7"></a>RSSCN7</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">400 * 400 * 3</td><td>7</td><td>7 * 400 = 2800</td><td>未知</td><td>GoogleEarth</td><td>2015</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://sites.google.com/site/qinzoucn/documents">https://sites.google.com/site/qinzoucn/documents</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52117">https://aistudio.baidu.com/aistudio/datasetdetail/52117</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1wS3-TgykUS5svytVsJnU-Q">https://pan.baidu.com/s/1wS3-TgykUS5svytVsJnU-Q</a>  提取码：ppy1</li></ul><h2 id="RS-C11"><a href="#RS-C11" class="headerlink" title="RS_C11"></a>RS_C11</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>11</td><td>10 * 100± = 1232</td><td>约0.2m</td><td>GoogleEarth</td><td>2016</td><td>中科院</td></tr></tbody></table><ul><li>源地址：<a href="https://www.researchgate.net/publication/271647282_RS_C11_Database/comments">https://www.researchgate.net/publication/271647282_RS_C11_Database/comments</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52227">https://aistudio.baidu.com/aistudio/datasetdetail/52227</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1sOIaya_CibyZhx2RTd3C_Q">https://pan.baidu.com/s/1sOIaya_CibyZhx2RTd3C_Q</a>  提取码：hdnr</li></ul><h2 id="NWPU-RESISC45"><a href="#NWPU-RESISC45" class="headerlink" title="NWPU-RESISC45"></a>NWPU-RESISC45</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>45</td><td>45 * 700 = 31500</td><td>0.2~30m</td><td>GoogleEarth</td><td>2016</td><td>西北工业大学</td></tr></tbody></table><ul><li>源地址：<a href="http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.html">http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/51873">https://aistudio.baidu.com/aistudio/datasetdetail/51873</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1gkvrsiN4r3CpAl3k-UfIkA">https://pan.baidu.com/s/1gkvrsiN4r3CpAl3k-UfIkA</a>  提取码：3n0g</li></ul><h2 id="AID"><a href="#AID" class="headerlink" title="AID"></a>AID</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">600 * 600 * 3</td><td>30</td><td>30 * 300± = 10000</td><td>0.5~8m</td><td>GoogleEarth</td><td>2017</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://captain-whu.github.io/AID/">https://captain-whu.github.io/AID/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52025">https://aistudio.baidu.com/aistudio/datasetdetail/52025</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1dWMqt6y8Xe7g2gjpygOALQ">https://pan.baidu.com/s/1dWMqt6y8Xe7g2gjpygOALQ</a>  提取码：svzw</li></ul><h2 id="GID"><a href="#GID" class="headerlink" title="GID"></a>GID</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">56 * 56 * 4<br>56 * 56 * 3</td><td>15</td><td>15 * 2000 = 30000</td><td>未知</td><td>高分2</td><td>2018</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://x-ytong.github.io/project/GID.html">https://x-ytong.github.io/project/GID.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55324">https://aistudio.baidu.com/aistudio/datasetdetail/55324</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1Xux-3nKTbu1v3Mp5tdyM7A">https://pan.baidu.com/s/1Xux-3nKTbu1v3Mp5tdyM7A</a>  提取码：d3qs</li></ul><h2 id="RSD46-WHU"><a href="#RSD46-WHU" class="headerlink" title="RSD46-WHU"></a>RSD46-WHU</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>46</td><td>117000</td><td>0.5~2m</td><td>GoogleEarth、天地图</td><td>2017</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/RSIA-LIESMARS-WHU/RSD46-WHU">https://github.com/RSIA-LIESMARS-WHU/RSD46-WHU</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52359">https://aistudio.baidu.com/aistudio/datasetdetail/52359</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1WNR8wV86jl_g04JX7bCpNQ">https://pan.baidu.com/s/1WNR8wV86jl_g04JX7bCpNQ</a>  提取码：gn2a</li></ul><h2 id="PatternNet"><a href="#PatternNet" class="headerlink" title="PatternNet"></a>PatternNet</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>38</td><td>38 * 800 = 30400</td><td>0.062~4.693m</td><td>GoogleMap</td><td>2018</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://sites.google.com/view/zhouwx/dataset#h.p_Tgef10WTuEFr">https://sites.google.com/view/zhouwx/dataset#h.p_Tgef10WTuEFr</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52411">https://aistudio.baidu.com/aistudio/datasetdetail/52411</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1_1KgQFi7QaUp8p6dz3TbWw">https://pan.baidu.com/s/1_1KgQFi7QaUp8p6dz3TbWw</a>  提取码：j2y2</li></ul><h2 id="AIRS"><a href="#AIRS" class="headerlink" title="AIRS"></a>AIRS</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">10000 * 10000 * 3</td><td>1</td><td>857 train，94 val， 96 test</td><td>0.075m</td><td>LINZ Data Service</td><td>2018</td><td>University of Tokyo等</td></tr></tbody></table><ul><li>源地址：<a href="https://www.airs-dataset.com/">https://www.airs-dataset.com/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/74274">https://aistudio.baidu.com/aistudio/datasetdetail/74274</a></li></ul><h2 id="Satellite-Images-of-Hurricane-Damage"><a href="#Satellite-Images-of-Hurricane-Damage" class="headerlink" title="Satellite Images of Hurricane Damage"></a>Satellite Images of Hurricane Damage</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">128 * 128 * 3</td><td>2</td><td>23000</td><td>未知</td><td>2018</td><td>University of Washington等</td></tr></tbody></table><ul><li>源地址：<a href="https://www.kaggle.com/kmader/satellite-images-of-hurricane-damage">https://www.kaggle.com/kmader/satellite-images-of-hurricane-damage</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/88155">https://aistudio.baidu.com/aistudio/datasetdetail/88155</a></li></ul><h2 id="How-to-make-high-resolution-remote-sensing-image-dataset"><a href="#How-to-make-high-resolution-remote-sensing-image-dataset" class="headerlink" title="How-to-make-high-resolution-remote-sensing-image-dataset"></a>How-to-make-high-resolution-remote-sensing-image-dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>数据源</th><th>发布时间</th><th>空间分辨率</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>5</td><td>533</td><td>谷歌地球</td><td>2018</td><td>0.075m</td></tr></tbody></table><ul><li>源地址：<a href="https://blog.csdn.net/u012193416/article/details/79472533">https://blog.csdn.net/u012193416/article/details/79472533</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/88597">https://aistudio.baidu.com/aistudio/datasetdetail/88597</a></li></ul><h2 id="OPTIMAL-31"><a href="#OPTIMAL-31" class="headerlink" title="OPTIMAL-31"></a>OPTIMAL-31</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>31</td><td>31* 60 = 1860</td><td>未知</td><td>GoogleEarth</td><td>2019</td><td>西北工业大学</td></tr></tbody></table><ul><li>源地址：<a href="http://crabwq.github.io/">http://crabwq.github.io/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/51798">https://aistudio.baidu.com/aistudio/datasetdetail/51798</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1ETHM44DuZYpKkUTqTxs4-w">https://pan.baidu.com/s/1ETHM44DuZYpKkUTqTxs4-w</a>  提取码：1dyd</li></ul><h2 id="WiDS-Datathon-2019"><a href="#WiDS-Datathon-2019" class="headerlink" title="WiDS Datathon 2019"></a>WiDS Datathon 2019</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>1</td><td>11000train</td><td>Planet</td><td>2019</td><td>Stanford</td><td>3m</td></tr></tbody></table><ul><li>源地址：<a href="https://www.kaggle.com/c/widsdatathon2019/data">https://www.kaggle.com/c/widsdatathon2019/data</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/76927">https://aistudio.baidu.com/aistudio/datasetdetail/76927</a></li></ul><h2 id="Continual-Learning-Benchmark-for-Remote-Sensing-Image-Scene-Classification-CLRS"><a href="#Continual-Learning-Benchmark-for-Remote-Sensing-Image-Scene-Classification-CLRS" class="headerlink" title="Continual Learning Benchmark for Remote Sensing Image Scene Classification (CLRS)"></a>Continual Learning Benchmark for Remote Sensing Image Scene Classification (CLRS)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>25</td><td>15,000</td><td>Google Earth, Bing Map, Google Map, and Tianditu</td><td>2020</td><td>中南大学</td><td>0.26 m to 8.85 m</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/lehaifeng/CLRS">https://github.com/lehaifeng/CLRS</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/76417">https://aistudio.baidu.com/aistudio/datasetdetail/76417</a></li></ul><h2 id="SenseEarth-Classify"><a href="#SenseEarth-Classify" class="headerlink" title="SenseEarth Classify"></a>SenseEarth Classify</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">100 * 100 ~ 12655 * 12655 * 3</td><td>28</td><td>~70000</td><td>未知</td><td>2020</td><td>商汤科技</td><td>0.2~153m</td></tr></tbody></table><ul><li>源地址：<a href="https://rs.sensetime.com/competition/index.html#/info">https://rs.sensetime.com/competition/index.html#/info</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52728">https://aistudio.baidu.com/aistudio/datasetdetail/52728</a></li></ul><h2 id="Multi-View-Datasets，CV-BrCT"><a href="#Multi-View-Datasets，CV-BrCT" class="headerlink" title="Multi-View Datasets，CV-BrCT"></a>Multi-View Datasets，CV-BrCT</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500 * 475 * 3</td><td>8</td><td>24171 * 2</td><td>航空影像及地面街景影像</td><td>2020</td><td>Federal University of Minas Gerais</td></tr></tbody></table><ul><li>源地址：<a href="http://www.patreo.dcc.ufmg.br/2020/07/22/multi-view-datasets/">http://www.patreo.dcc.ufmg.br/2020/07/22/multi-view-datasets/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/58013">https://aistudio.baidu.com/aistudio/datasetdetail/58013</a></li></ul><h2 id="Multi-View-Datasets，AiRound"><a href="#Multi-View-Datasets，AiRound" class="headerlink" title="Multi-View Datasets，AiRound"></a>Multi-View Datasets，AiRound</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500 * 475 * 3</td><td>11</td><td>3495 * 4</td><td>Sentinel-2等多源数据</td><td>2020</td><td>Federal University of Minas Gerais</td></tr></tbody></table><ul><li>源地址：<a href="http://www.patreo.dcc.ufmg.br/2020/07/22/multi-view-datasets/">http://www.patreo.dcc.ufmg.br/2020/07/22/multi-view-datasets/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/58760">https://aistudio.baidu.com/aistudio/datasetdetail/58760</a></li></ul><h2 id="SIRI-WHU"><a href="#SIRI-WHU" class="headerlink" title="SIRI-WHU"></a>SIRI-WHU</h2><h3 id="SIRI-WHU：google"><a href="#SIRI-WHU：google" class="headerlink" title="SIRI-WHU：google"></a>SIRI-WHU：google</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">200 * 200 * 3</td><td>12</td><td>12 * 200 = 2400</td><td>2m</td><td>GoogleEarth</td><td>2016</td><td>武汉大学</td></tr></tbody></table><h3 id="SIRI-WHU：USGS"><a href="#SIRI-WHU：USGS" class="headerlink" title="SIRI-WHU：USGS"></a>SIRI-WHU：USGS</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">10000 * 9000 * 3</td><td>4</td><td>1</td><td>2 foot</td><td>USGS</td><td>2016</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="http://www.lmars.whu.edu.cn/prof_web/zhongyanfei/e-code.html">http://www.lmars.whu.edu.cn/prof_web/zhongyanfei/e-code.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/51921">https://aistudio.baidu.com/aistudio/datasetdetail/51921</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/14C4Km92FekHy2dWsrXLckA">https://pan.baidu.com/s/14C4Km92FekHy2dWsrXLckA</a>  提取码：6259</li></ul><h2 id="RSI-CB"><a href="#RSI-CB" class="headerlink" title="RSI-CB"></a>RSI-CB</h2><h3 id="RSI-CB128"><a href="#RSI-CB128" class="headerlink" title="RSI-CB128"></a>RSI-CB128</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>45</td><td>36000</td><td>0.3–3m</td><td>多源</td><td>2017</td><td>中南大学</td></tr></tbody></table><h3 id="RSI-CB256"><a href="#RSI-CB256" class="headerlink" title="RSI-CB256"></a>RSI-CB256</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>35</td><td>24000</td><td>0.3–3m</td><td>多源</td><td>2017</td><td>中南大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/lehaifeng/RSI-CB">https://github.com/lehaifeng/RSI-CB</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52487">https://aistudio.baidu.com/aistudio/datasetdetail/52487</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1f77k5g8mhsSzxPantyxemA">https://pan.baidu.com/s/1f77k5g8mhsSzxPantyxemA</a>  提取码：246r</li></ul><h2 id="SAT"><a href="#SAT" class="headerlink" title="SAT"></a>SAT</h2><h3 id="SAT-4"><a href="#SAT-4" class="headerlink" title="SAT-4"></a>SAT-4</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>备注</th></tr></thead><tbody><tr><td align="left">28 * 28 * 4</td><td>4</td><td>500,000</td><td>1-6m</td><td>NAIP dataset</td><td>2015</td><td>路易斯安那州立大学与NASA</td><td>采用MATLAB的.mat数据存储格式</td></tr></tbody></table><h3 id="SAT-6"><a href="#SAT-6" class="headerlink" title="SAT-6"></a>SAT-6</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>备注</th></tr></thead><tbody><tr><td align="left">28 * 28 * 4</td><td>6</td><td>405,000</td><td>1-6m</td><td>NAIP dataset</td><td>2015</td><td>路易斯安那州立大学与NASA</td><td>采用MATLAB的.mat数据存储格式</td></tr></tbody></table><ul><li>源地址：<a href="http://csc.lsu.edu/~saikat/deepsat/">http://csc.lsu.edu/~saikat/deepsat/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52534">https://aistudio.baidu.com/aistudio/datasetdetail/52534</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1LKCAPAz-DVGTkjQ1DMS0_Q">https://pan.baidu.com/s/1LKCAPAz-DVGTkjQ1DMS0_Q</a>  提取码：12du</li></ul><h2 id="V-RSIR"><a href="#V-RSIR" class="headerlink" title="V-RSIR"></a>V-RSIR</h2><h3 id="rs-VArcGIS"><a href="#rs-VArcGIS" class="headerlink" title="rs VArcGIS"></a>rs VArcGIS</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>38</td><td>59071</td><td>0.07 ~ 19.11m</td><td>ArcGIS World Imagery</td><td>2020</td><td>中南大学</td></tr></tbody></table><h3 id="rs-VBing"><a href="#rs-VBing" class="headerlink" title="rs VBing"></a>rs VBing</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>38</td><td>58944</td><td>0.07 ~ 38.22m</td><td>Bing World Imagery</td><td>2020</td><td>中南大学</td></tr></tbody></table><h3 id="rs-VGoogle"><a href="#rs-VGoogle" class="headerlink" title="rs VGoogle"></a>rs VGoogle</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>38</td><td>59404</td><td>0.075 ~ 9.555m</td><td>VGoogle</td><td>2020</td><td>中南大学</td></tr></tbody></table><ul><li>源地址：<br><a href="https://aistudio.baidu.com/aistudio/datasetdetail/47630">https://aistudio.baidu.com/aistudio/datasetdetail/47630</a><br><a href="https://aistudio.baidu.com/aistudio/datasetdetail/47638">https://aistudio.baidu.com/aistudio/datasetdetail/47638</a><br><a href="https://aistudio.baidu.com/aistudio/datasetdetail/47619">https://aistudio.baidu.com/aistudio/datasetdetail/47619</a></li></ul><h2 id="rs-sensetime"><a href="#rs-sensetime" class="headerlink" title="rs sensetime"></a>rs sensetime</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">100 * 100 * 3 ~ <br>12655 * 12655 * 3</td><td>50</td><td>60040</td><td>0.2~153m</td><td>未知</td><td>2020</td><td>商汤科技SenseEarth</td></tr></tbody></table><ul><li>源地址：<a href="https://rs.sensetime.com/competition/index.html#/data">https://rs.sensetime.com/competition/index.html#/data</a></li></ul><h2 id="rscup"><a href="#rscup" class="headerlink" title="rscup"></a>rscup</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>45</td><td>197,121</td><td>未知</td><td>2019</td><td>rscup组委会</td></tr></tbody></table><ul><li>源地址：<a href="http://rscup.bjxintong.com.cn/#/theme/1">http://rscup.bjxintong.com.cn/#/theme/1</a></li></ul><h2 id="Planet-Understanding-the-Amazon-from-Space"><a href="#Planet-Understanding-the-Amazon-from-Space" class="headerlink" title="Planet: Understanding the Amazon from Space"></a>Planet: Understanding the Amazon from Space</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>备注</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3<br>256 * 256 * 4</td><td>17</td><td>40480</td><td>3m</td><td>plant传感器</td><td>2017</td><td>Planet、SCCON公司</td><td>多标签</td></tr></tbody></table><ul><li>源地址：<a href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data">https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data</a></li></ul><h2 id="brazilian-coffee-scenes"><a href="#brazilian-coffee-scenes" class="headerlink" title="brazilian coffee scenes"></a>brazilian coffee scenes</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">64 * 64 * 3</td><td>2</td><td>2,876</td><td>未知</td><td>SPOT传感器</td><td>2015</td><td>米纳斯联邦大学</td></tr></tbody></table><ul><li>源地址：<a href="http://patreo.dcc.ufmg.br/2017/11/12/brazilian-coffee-scenes-dataset">http://patreo.dcc.ufmg.br/2017/11/12/brazilian-coffee-scenes-dataset</a></li></ul><h1 id="遥感影像目标检测"><a href="#遥感影像目标检测" class="headerlink" title="遥感影像目标检测"></a>遥感影像目标检测</h1><p><strong>收集网络中开源的、关于遥感影像 目标检测的深度学习数据集</strong></p><p>main ref </p><blockquote><p><a href="https://captain-whu.github.io/DiRS/">https://captain-whu.github.io/DiRS/</a><br><a href="https://mp.weixin.qq.com/s/QGIFeriC3s0xvok-TVhmPQ">https://mp.weixin.qq.com/s/QGIFeriC3s0xvok-TVhmPQ</a><br><a href="https://zhuanlan.zhihu.com/p/113579163">https://zhuanlan.zhihu.com/p/113579163</a></p></blockquote><h2 id="TAS"><a href="#TAS" class="headerlink" title="TAS"></a>TAS</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">792 * 636 * 3</td><td>1</td><td>30</td><td>1,319</td><td>HBB</td><td>Google Earth</td><td>2008</td><td>斯坦福大学</td></tr></tbody></table><ul><li>源地址：<a href="http://ai.stanford.edu/~gaheitz/Research/TAS/">http://ai.stanford.edu/~gaheitz/Research/TAS/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53806">https://aistudio.baidu.com/aistudio/datasetdetail/53806</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1jGUThjNj3GsU0EIAQlQaiw%20">https://pan.baidu.com/s/1jGUThjNj3GsU0EIAQlQaiw</a> 提取码：t7lo</li></ul><h2 id="OIRDS"><a href="#OIRDS" class="headerlink" title="OIRDS"></a>OIRDS</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256<del>640 * 256</del>640 * 3</td><td>5</td><td>900</td><td>1800</td><td>约0.15m</td><td>OBB</td><td>USGS、DARPA、VIVID</td><td>2009</td><td>雷神公司等</td></tr></tbody></table><ul><li>源地址：<a href="https://sourceforge.net/projects/oirds/">https://sourceforge.net/projects/oirds/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53461">https://aistudio.baidu.com/aistudio/datasetdetail/53461</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/173yxlf279R8EESlT7mxbPw">https://pan.baidu.com/s/173yxlf279R8EESlT7mxbPw</a>  提取码：k8di</li></ul><h2 id="SZTAKI-INRIA-Building-Detection-Benchmark"><a href="#SZTAKI-INRIA-Building-Detection-Benchmark" class="headerlink" title="SZTAKI-INRIA Building Detection Benchmark"></a>SZTAKI-INRIA Building Detection Benchmark</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">700± * 700± * 3</td><td>1</td><td>9</td><td>665</td><td>OBB</td><td>未知</td><td>2012</td><td>MTA SZTAKI</td></tr></tbody></table><ul><li>源地址：<a href="http://web.eee.sztaki.hu/remotesensing/building_benchmark.html">http://web.eee.sztaki.hu/remotesensing/building_benchmark.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/77674">https://aistudio.baidu.com/aistudio/datasetdetail/77674</a></li></ul><h2 id="UCAS-AOD"><a href="#UCAS-AOD" class="headerlink" title="UCAS_AOD"></a>UCAS_AOD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1000± * 1000± * 3</td><td>2</td><td>976</td><td>6,950</td><td>OBB</td><td>Google Earth</td><td>2014</td><td>中科院</td></tr></tbody></table><ul><li>源地址：<a href="https://onedrive.hyper.ai/home/UCAS-AOD">https://onedrive.hyper.ai/home/UCAS-AOD</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53318">https://aistudio.baidu.com/aistudio/datasetdetail/53318</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1E9L-S6baxLDjSDTYlm985g">https://pan.baidu.com/s/1E9L-S6baxLDjSDTYlm985g</a>  提取码：sd6l</li></ul><h2 id="NWPU-VHR-10"><a href="#NWPU-VHR-10" class="headerlink" title="NWPU VHR-10"></a>NWPU VHR-10</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500<del>1100 * 500</del>1100 * 3</td><td>10</td><td>1510</td><td>14,596</td><td>HBB</td><td>Google Earth、Vaihingen</td><td>2014</td><td>西北工业大学</td></tr></tbody></table><ul><li>源地址：<a href="http://www.escience.cn/people/gongcheng/NWPU-VHR-10.html">http://www.escience.cn/people/gongcheng/NWPU-VHR-10.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52812">https://aistudio.baidu.com/aistudio/datasetdetail/52812</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1Wm73acTD1WfBM_YZlKMjXw">https://pan.baidu.com/s/1Wm73acTD1WfBM_YZlKMjXw</a>  提取码：35wf</li></ul><h2 id="HRSC2016"><a href="#HRSC2016" class="headerlink" title="HRSC2016"></a>HRSC2016</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1100± * 1100± * 3</td><td>27</td><td>1,061</td><td>2,976</td><td>OBB</td><td>Google Earth</td><td>2016</td><td>西北工业大学</td></tr></tbody></table><ul><li>源地址：<a href="https://sites.google.com/site/hrsc2016/">https://sites.google.com/site/hrsc2016/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54106">https://aistudio.baidu.com/aistudio/datasetdetail/54106</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1BO2UX6dICVa33qUqIMCFrw">https://pan.baidu.com/s/1BO2UX6dICVa33qUqIMCFrw</a>  提取码：shbp</li></ul><h2 id="DLR3k"><a href="#DLR3k" class="headerlink" title="DLR3k"></a>DLR3k</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">5616 * 3744 * 3</td><td>7</td><td>20</td><td>14,235</td><td>0.13m</td><td>OBB</td><td>无人机(Canon Eos 1Ds Mark III)</td><td>2016</td><td>德国航天航空中心</td></tr></tbody></table><ul><li>源地址：<a href="https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-12760/22294_read-52777">https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-12760/22294_read-52777</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/xxxxx">https://aistudio.baidu.com/aistudio/datasetdetail/xxxxx</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1hIbSQd-XdHxpmYIdFYHCQw%20">https://pan.baidu.com/s/1hIbSQd-XdHxpmYIdFYHCQw</a> 提取码：1lpx</li></ul><h2 id="RSOD"><a href="#RSOD" class="headerlink" title="RSOD"></a>RSOD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1044<del>1288 * 915</del>992 * 3</td><td>4</td><td>976</td><td>6,950</td><td>HBB</td><td>Google Earth、天地图</td><td>2017</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/RSIA-LIESMARS-WHU/RSOD-Dataset-">https://github.com/RSIA-LIESMARS-WHU/RSOD-Dataset-</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52980">https://aistudio.baidu.com/aistudio/datasetdetail/52980</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1lUooUOJE2QCpCXSBrzKysQ">https://pan.baidu.com/s/1lUooUOJE2QCpCXSBrzKysQ</a>  提取码：pqih</li></ul><h2 id="TGRS-HRRSD"><a href="#TGRS-HRRSD" class="headerlink" title="TGRS-HRRSD"></a>TGRS-HRRSD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">152<del>10569 * 152</del>10569 * 3</td><td>13</td><td>21,761</td><td>55,740</td><td>0.15 ~ 1.2m</td><td>HBB</td><td>Google Earth、百度地图</td><td>2017</td><td>中科院</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/CrazyStoneonRoad/TGRS-HRRSD-Dataset">https://github.com/CrazyStoneonRoad/TGRS-HRRSD-Dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53186">https://aistudio.baidu.com/aistudio/datasetdetail/53186</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1hru_vjugopeJCXT7SguhcQ%20">https://pan.baidu.com/s/1hru_vjugopeJCXT7SguhcQ</a> 提取码：row5</li></ul><h2 id="SSDD"><a href="#SSDD" class="headerlink" title="SSDD"></a>SSDD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500 * 500 * 3</td><td>1</td><td>1160</td><td>2456</td><td>1~15m</td><td>HBB、OBB</td><td>RadarSat-2、TerraSAR-X、Sentinel-1</td><td>2017</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://zhuanlan.zhihu.com/p/58404659">https://zhuanlan.zhihu.com/p/58404659</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54806">https://aistudio.baidu.com/aistudio/datasetdetail/54806</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1tcKcCTjUXJJW--erUFyzqg%20">https://pan.baidu.com/s/1tcKcCTjUXJJW--erUFyzqg</a> 提取码：hb0d</li></ul><h2 id="OpenSARShip"><a href="#OpenSARShip" class="headerlink" title="OpenSARShip"></a>OpenSARShip</h2><table><thead><tr><th align="left">类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1</td><td>41</td><td>11346</td><td>Chip</td><td>Sentinel-1</td><td>2017</td><td>上海交通大学</td></tr></tbody></table><ul><li>源地址：<a href="http://opensar.sjtu.edu.cn/">http://opensar.sjtu.edu.cn/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/77017">https://aistudio.baidu.com/aistudio/datasetdetail/77017</a></li></ul><h2 id="ships-in-satellite-imagery"><a href="#ships-in-satellite-imagery" class="headerlink" title="ships in satellite imagery"></a>ships in satellite imagery</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">80 * 80 * 3</td><td>1</td><td>4000</td><td>1000+</td><td>Planet</td><td>2017</td><td>Planet Team</td><td>3m</td></tr></tbody></table><ul><li>源地址：<a href="https://www.kaggle.com/rhammell/ships-in-satellite-imagery">https://www.kaggle.com/rhammell/ships-in-satellite-imagery</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/77233">https://aistudio.baidu.com/aistudio/datasetdetail/77233</a></li></ul><h2 id="LEVIR"><a href="#LEVIR" class="headerlink" title="LEVIR"></a>LEVIR</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800 * 600 * 3</td><td>3</td><td>22,000</td><td>11,000</td><td>0.2∼1.0m</td><td>HBB</td><td>Google Earth</td><td>2018</td><td>北京航天航空大学</td></tr></tbody></table><ul><li>源地址：<a href="http://levir.buaa.edu.cn/Code.htm">http://levir.buaa.edu.cn/Code.htm</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53714">https://aistudio.baidu.com/aistudio/datasetdetail/53714</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1kfTozB7SHIH1ioaJdzHwFA%20">https://pan.baidu.com/s/1kfTozB7SHIH1ioaJdzHwFA</a> 提取码：eydb</li></ul><h2 id="VisDrone2019-DET"><a href="#VisDrone2019-DET" class="headerlink" title="VisDrone2019-DET"></a>VisDrone2019-DET</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500<del>2,000* 500</del>2,000 * 3</td><td>10</td><td>10,209</td><td>54,200</td><td>HBB</td><td>无人机数据</td><td>2018</td><td>天津大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/VisDrone/VisDrone-Dataset">https://github.com/VisDrone/VisDrone-Dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54054">https://aistudio.baidu.com/aistudio/datasetdetail/54054</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1ZvWrcRimr-UMU1QI1P6Y8g">https://pan.baidu.com/s/1ZvWrcRimr-UMU1QI1P6Y8g</a>  提取码：fcjs</li></ul><h2 id="MASATI"><a href="#MASATI" class="headerlink" title="MASATI"></a>MASATI</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">512± * 512± * 3</td><td>7</td><td>7,389</td><td>未知</td><td>HBB</td><td>Bing Maps</td><td>2018</td><td>阿利坎特大学</td></tr></tbody></table><ul><li>源地址：<a href="https://www.iuii.ua.es/datasets/masati/">https://www.iuii.ua.es/datasets/masati/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53895">https://aistudio.baidu.com/aistudio/datasetdetail/53895</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1t84BiCmTY8PTBnjBGf0gbQ%20">https://pan.baidu.com/s/1t84BiCmTY8PTBnjBGf0gbQ</a> 提取码：npwo</li></ul><h2 id="ITCVD"><a href="#ITCVD" class="headerlink" title="ITCVD"></a>ITCVD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>备注</th></tr></thead><tbody><tr><td align="left">5616 * 3744 * 3</td><td>1</td><td>135</td><td>23543</td><td>0.1 m</td><td>HBB</td><td>航拍影像</td><td>2018</td><td>University of Twente Research Information</td><td>标注为mat格式</td></tr></tbody></table><ul><li>源地址：<a href="https://research.utwente.nl/en/datasets/itcvd-dataset">https://research.utwente.nl/en/datasets/itcvd-dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54674">https://aistudio.baidu.com/aistudio/datasetdetail/54674</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1TA2_HU3qb96-lXkZJ5BtEg%20">https://pan.baidu.com/s/1TA2_HU3qb96-lXkZJ5BtEg</a> 提取码：6hht</li></ul><h2 id="DIOR"><a href="#DIOR" class="headerlink" title="DIOR"></a>DIOR</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800 * 800 * 3</td><td>20</td><td>23463</td><td>190288</td><td>0.5 ~ 30 m</td><td>HBB</td><td>Google Earth</td><td>2019</td><td>西北工业大学</td></tr></tbody></table><ul><li>源地址：<a href="http://www.escience.cn/people/JunweiHan/DIOR.html">http://www.escience.cn/people/JunweiHan/DIOR.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53045">https://aistudio.baidu.com/aistudio/datasetdetail/53045</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1Z3xJC_l5MpGSFuW0u4shuQ">https://pan.baidu.com/s/1Z3xJC_l5MpGSFuW0u4shuQ</a>  提取码：vt56</li></ul><h2 id="AIR-SARShip"><a href="#AIR-SARShip" class="headerlink" title="AIR-SARShip"></a>AIR-SARShip</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1000 * 1000 * 3</td><td>1</td><td>300</td><td>2040</td><td>1~3m</td><td>HBB</td><td>高分3</td><td>2019</td><td>《雷达学报》编辑部</td></tr></tbody></table><ul><li>源地址：<a href="http://radars.ie.ac.cn/web/data/getData?dataType=SARDataset">http://radars.ie.ac.cn/web/data/getData?dataType=SARDataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54270">https://aistudio.baidu.com/aistudio/datasetdetail/54270</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1NnLVijCr0BN8CIg86srDUA">https://pan.baidu.com/s/1NnLVijCr0BN8CIg86srDUA</a>  提取码：bfri</li></ul><h2 id="SAR-Ship"><a href="#SAR-Ship" class="headerlink" title="SAR-Ship"></a>SAR-Ship</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>1</td><td>210</td><td>43,819</td><td>1.7~25m</td><td>HBB</td><td>高分3、哨兵1</td><td>2019</td><td>中科院</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/CAESAR-Radi/SAR-Ship-Dataset">https://github.com/CAESAR-Radi/SAR-Ship-Dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54361">https://aistudio.baidu.com/aistudio/datasetdetail/54361</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/13MViO0EdhCZSNlesF-32Bg%20">https://pan.baidu.com/s/13MViO0EdhCZSNlesF-32Bg</a> 提取码：s0qf</li></ul><h2 id="iSAID"><a href="#iSAID" class="headerlink" title="iSAID"></a>iSAID</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800<del>13, 000 * 800</del>13, 000 * 3</td><td>15</td><td>2,806</td><td>655,451</td><td>OBB</td><td>Google Earth、JL-1、GF-2</td><td>2019</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://captain-whu.github.io/iSAID/index.html">https://captain-whu.github.io/iSAID/index.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/73458">https://aistudio.baidu.com/aistudio/datasetdetail/73458</a></li></ul><h2 id="Bridge-Dataset"><a href="#Bridge-Dataset" class="headerlink" title="Bridge Dataset"></a>Bridge Dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4800 * 2843 * 3</td><td>1</td><td>500</td><td>500+</td><td>HBB</td><td>Google Earth、 OpenStreetMap</td><td>2019</td><td>Federal University of Minas Gerais</td></tr></tbody></table><ul><li>源地址：<a href="http://www.patreo.dcc.ufmg.br/2019/07/10/bridge-dataset/">http://www.patreo.dcc.ufmg.br/2019/07/10/bridge-dataset/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/57457">https://aistudio.baidu.com/aistudio/datasetdetail/57457</a></li></ul><h2 id="HRSID"><a href="#HRSID" class="headerlink" title="HRSID"></a>HRSID</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800 * 800 * 3</td><td>1</td><td>5604</td><td>16951</td><td>0.5~3 m</td><td>HBB</td><td>Sentinel-1B、TerraSAR-X、TanDEM-X</td><td>2020</td><td>电子科技大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/chaozhong2010/HRSID">https://github.com/chaozhong2010/HRSID</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54512">https://aistudio.baidu.com/aistudio/datasetdetail/54512</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/13MViO0EdhCZSNlesF-32Bg%20">https://pan.baidu.com/s/13MViO0EdhCZSNlesF-32Bg</a> 提取码：s0qf</li></ul><h2 id="RarePlanes"><a href="#RarePlanes" class="headerlink" title="RarePlanes"></a>RarePlanes</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>10</td><td>1244 train，263 test</td><td>~14,700</td><td>HBB</td><td>WorldView-3</td><td>2020</td><td>In-Q-Tel、AI.Reverie</td><td>0.3~1.5m</td></tr></tbody></table><ul><li>源地址：<a href="https://www.cosmiqworks.org/RarePlanes/">https://www.cosmiqworks.org/RarePlanes/</a><br><a href="https://www.graviti.cn/open-datasets/RarePlanes">https://www.graviti.cn/open-datasets/RarePlanes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/70215">https://aistudio.baidu.com/aistudio/datasetdetail/70215</a></li></ul><h2 id="FAIR1M"><a href="#FAIR1M" class="headerlink" title="FAIR1M"></a>FAIR1M</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">1000<del>10000 * 1000</del>10000 * 3</td><td>36</td><td>15000+</td><td>100 0000+</td><td>OBB</td><td>Gaofen satellites、Google Earth</td><td>2021</td><td>中科院等</td><td>0.3 ~ 0.8m</td></tr></tbody></table><ul><li>源地址：<a href="http://gaofen-challenge.com/">http://gaofen-challenge.com</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/78453">https://aistudio.baidu.com/aistudio/datasetdetail/78453</a></li></ul><h2 id="VEDAI"><a href="#VEDAI" class="headerlink" title="VEDAI"></a>VEDAI</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">512 * 512 * 4<br>1024 * 1024 * 4</td><td>9</td><td>1210</td><td>3640</td><td>0.125m</td><td>OBB</td><td>Utah AGRC</td><td>2015</td><td>卡昂大学</td></tr></tbody></table><ul><li>源地址：<a href="https://downloads.greyc.fr/vedai/">https://downloads.greyc.fr/vedai/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53383">https://aistudio.baidu.com/aistudio/datasetdetail/53383</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1t3qCIUnEpJ1lIUTZpjlCyw">https://pan.baidu.com/s/1t3qCIUnEpJ1lIUTZpjlCyw</a>  提取码：doid</li></ul><h2 id="DOTA"><a href="#DOTA" class="headerlink" title="DOTA"></a>DOTA</h2><h3 id="DOTA1-0"><a href="#DOTA1-0" class="headerlink" title="DOTA1.0"></a>DOTA1.0</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800<del>4000 * 800</del>4000 * 3</td><td>15</td><td>2806</td><td>188282</td><td>OBB</td><td>Google Earth、高分2、吉林1</td><td>2018</td><td>武汉大学</td></tr></tbody></table><h3 id="DOTA1-5"><a href="#DOTA1-5" class="headerlink" title="DOTA1.5"></a>DOTA1.5</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800<del>4000 * 800</del>4000 * 3</td><td>16</td><td>2806</td><td>400000±</td><td>OBB</td><td>Google Earth、高分2、吉林1</td><td>2019</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://captain-whu.github.io/DOTA/index.html">https://captain-whu.github.io/DOTA/index.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53125">https://aistudio.baidu.com/aistudio/datasetdetail/53125</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/19AhrPGnbrPwCdHsThv0XBQ">https://pan.baidu.com/s/19AhrPGnbrPwCdHsThv0XBQ</a>  提取码：avsl</li></ul><h2 id="SZTAKI-INRIA"><a href="#SZTAKI-INRIA" class="headerlink" title="SZTAKI-INRIA"></a>SZTAKI-INRIA</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">800± * 800±</td><td>1</td><td>9</td><td>665</td><td>OBB</td><td>未知</td><td>2012</td><td>MTA SZTAKI、INRIA Sophia-Antipolis</td></tr></tbody></table><ul><li>源地址：<a href="http://web.eee.sztaki.hu/remotesensing/building_benchmark.html">http://web.eee.sztaki.hu/remotesensing/building_benchmark.html</a></li></ul><h2 id="COWC"><a href="#COWC" class="headerlink" title="COWC"></a>COWC</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">2,000<del>19,000 * 2,000</del>19,000</td><td>1</td><td>53</td><td>32,716</td><td>0.15m</td><td>one dot</td><td>Utah</td><td>2016</td><td>劳伦斯利弗莫尔国家实验室</td></tr></tbody></table><ul><li>源地址：<a href="https://gdo152.llnl.gov/cowc/">https://gdo152.llnl.gov/cowc/</a></li></ul><h2 id="Functional-Map-of-the-World-Challenge"><a href="#Functional-Map-of-the-World-Challenge" class="headerlink" title="Functional Map of the World Challenge"></a>Functional Map of the World Challenge</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4bands<br>8bands</td><td>63</td><td>31</td><td>1,000,000</td><td>one dot</td><td>Utah</td><td>2016</td><td>劳伦斯利弗莫尔国家实验室</td></tr></tbody></table><ul><li>源地址：<a href="https://www.iarpa.gov/challenges/fmow.html">https://www.iarpa.gov/challenges/fmow.html</a><br><a href="https://github.com/fMoW/dataset">https://github.com/fMoW/dataset</a></li></ul><h2 id="CARPK"><a href="#CARPK" class="headerlink" title="CARPK"></a>CARPK</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">未知</td><td>1</td><td>1,573</td><td>106,233</td><td>未知</td><td>无人机</td><td>2017</td><td>台湾国立大学</td></tr></tbody></table><ul><li>源地址：<a href="https://lafi.github.io/LPN/">https://lafi.github.io/LPN/</a></li></ul><h2 id="rscup-1"><a href="#rscup-1" class="headerlink" title="rscup"></a>rscup</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1024 * 1024 * 3</td><td>18</td><td>未知</td><td>未知</td><td>OBB</td><td>GF, JL, Google Earth, Aerial</td><td>2019</td><td>rscup组委会</td></tr></tbody></table><ul><li>源地址：<a href="http://rscup.bjxintong.com.cn/#/theme/2">http://rscup.bjxintong.com.cn/#/theme/2</a></li></ul><h2 id="planet-understanding-the-amazon-from-space"><a href="#planet-understanding-the-amazon-from-space" class="headerlink" title="planet-understanding-the-amazon-from-space"></a>planet-understanding-the-amazon-from-space</h2><ul><li>源地址：<a href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data">https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data</a></li></ul><h2 id="airbus-ship-detection"><a href="#airbus-ship-detection" class="headerlink" title="airbus-ship-detection"></a>airbus-ship-detection</h2><ul><li>源地址：<a href="https://www.kaggle.com/c/airbus-ship-detection">https://www.kaggle.com/c/airbus-ship-detection</a></li></ul><h2 id="Open-AI-Tanzania-Building-Footprint-Segmentation-Challenge"><a href="#Open-AI-Tanzania-Building-Footprint-Segmentation-Challenge" class="headerlink" title="Open AI Tanzania Building Footprint Segmentation Challenge"></a>Open AI Tanzania Building Footprint Segmentation Challenge</h2><ul><li>源地址：<a href="https://competitions.codalab.org/competitions/20100">https://competitions.codalab.org/competitions/20100</a></li></ul><h2 id="MAFAT-Challenge-Fine-Grained-Classification-of-Objects-from-Aerial-Imagery"><a href="#MAFAT-Challenge-Fine-Grained-Classification-of-Objects-from-Aerial-Imagery" class="headerlink" title="MAFAT Challenge - Fine-Grained Classification of Objects from Aerial Imagery"></a>MAFAT Challenge - Fine-Grained Classification of Objects from Aerial Imagery</h2><ul><li>源地址：<a href="https://competitions.codalab.org/competitions/19854">https://competitions.codalab.org/competitions/19854</a></li></ul><h1 id="遥感影像图像分割"><a href="#遥感影像图像分割" class="headerlink" title="遥感影像图像分割"></a>遥感影像图像分割</h1><p><strong>收集网络中开源的、关于遥感影像 图像分割的深度学习数据集</strong></p><p>main ref </p><blockquote><p><a href="https://captain-whu.github.io/DiRS/">https://captain-whu.github.io/DiRS/</a><br><a href="https://mp.weixin.qq.com/s/x-sgus9bJz7esbsRvb-IfA">https://mp.weixin.qq.com/s/x-sgus9bJz7esbsRvb-IfA</a><br><a href="https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-12760/">https://www.dlr.de/eoc/en/desktopdefault.aspx/tabid-12760/</a><br><a href="http://www.patreo.dcc.ufmg.br/category/downloads/datasets/">http://www.patreo.dcc.ufmg.br/category/downloads/datasets/</a></p></blockquote><h2 id="Massachusetts-Roads"><a href="#Massachusetts-Roads" class="headerlink" title="Massachusetts Roads"></a>Massachusetts Roads</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1500 * 1500 * 3</td><td>1</td><td>804</td><td>航空影像</td><td>2013</td><td>University of Toronto</td></tr></tbody></table><ul><li>源地址：<a href="https://www.cs.toronto.edu/~vmnih/data/">https://www.cs.toronto.edu/~vmnih/data/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56961">https://aistudio.baidu.com/aistudio/datasetdetail/56961</a></li></ul><h2 id="Massachusetts-Builds"><a href="#Massachusetts-Builds" class="headerlink" title="Massachusetts Builds"></a>Massachusetts Builds</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1500 * 1500 * 3</td><td>1</td><td>151</td><td>航空影像</td><td>2013</td><td>University of Toronto</td></tr></tbody></table><ul><li>源地址：<a href="https://www.cs.toronto.edu/~vmnih/data/">https://www.cs.toronto.edu/~vmnih/data/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/57019">https://aistudio.baidu.com/aistudio/datasetdetail/57019</a></li></ul><h2 id="Zurich-Summer"><a href="#Zurich-Summer" class="headerlink" title="Zurich Summer"></a>Zurich Summer</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">600<del>1600 * 600</del>1600 * 4</td><td>8</td><td>20</td><td>QuickBird</td><td>2015</td><td>The University of Edinburgh, Scotland (UK)</td><td>0.62m</td></tr></tbody></table><ul><li>源地址：<a href="https://sites.google.com/view/zhouwx/dataset#h.p_hQS2jYeaFpV0">https://sites.google.com/view/zhouwx/dataset#h.p_hQS2jYeaFpV0</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55102">https://aistudio.baidu.com/aistudio/datasetdetail/55102</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1_Cydh88pJXKJnmlvUv2y6Q">https://pan.baidu.com/s/1_Cydh88pJXKJnmlvUv2y6Q</a>  提取码：u7cr</li></ul><h2 id="ERM-PAIW"><a href="#ERM-PAIW" class="headerlink" title="ERM-PAIW"></a>ERM-PAIW</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4000± * 4000± * 3</td><td>1</td><td>41</td><td>航空影像</td><td>2015</td><td>German Aerospace Center (DLR)</td></tr></tbody></table><ul><li>源地址：<a href="https://www.cs.toronto.edu/~vmnih/data/">https://www.cs.toronto.edu/~vmnih/data/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/57293">https://aistudio.baidu.com/aistudio/datasetdetail/57293</a></li></ul><h2 id="HD-Maps"><a href="#HD-Maps" class="headerlink" title="HD-Maps"></a>HD-Maps</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4000± * 4000± * 3</td><td>4</td><td>20</td><td>航空影像</td><td>2016</td><td>German Aerospace Center (DLR)</td></tr></tbody></table><ul><li>源地址：<a href="https://www.cs.toronto.edu/~vmnih/data/">https://www.cs.toronto.edu/~vmnih/data/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/57162">https://aistudio.baidu.com/aistudio/datasetdetail/57162</a></li></ul><h2 id="BDCI2017"><a href="#BDCI2017" class="headerlink" title="BDCI2017"></a>BDCI2017</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">8000± * 8000± * 3</td><td>5</td><td>5</td><td>未知</td><td>2017</td><td>BDCI</td></tr></tbody></table><ul><li>源地址：<a href="https://www.datafountain.cn/competitions/270">https://www.datafountain.cn/competitions/270</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55424">https://aistudio.baidu.com/aistudio/datasetdetail/55424</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1sww-vuY9m4snAJY9IFRnIA">https://pan.baidu.com/s/1sww-vuY9m4snAJY9IFRnIA</a>  提取码：ocnn</li></ul><h2 id="Learning-Aerial-Image-Segmentation-From-Online-Maps"><a href="#Learning-Aerial-Image-Segmentation-From-Online-Maps" class="headerlink" title="Learning Aerial Image Segmentation From Online Maps"></a>Learning Aerial Image Segmentation From Online Maps</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">3000± * 3000± * 3</td><td>2</td><td>1671</td><td>Google Maps、OpenStreetMap</td><td>2017</td><td>TH Zürich</td></tr></tbody></table><ul><li>源地址：<a href="https://zenodo.org/record/1154821#.X4rCKS6HqUm">https://zenodo.org/record/1154821#.X4rCKS6HqUm</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56140">https://aistudio.baidu.com/aistudio/datasetdetail/56140</a></li></ul><h2 id="2018-Open-AI-Tanzania-Building-Footprint-Segmentation-Challenge-TBF"><a href="#2018-Open-AI-Tanzania-Building-Footprint-Segmentation-Challenge-TBF" class="headerlink" title="2018 Open AI Tanzania Building Footprint Segmentation Challenge(TBF)"></a>2018 Open AI Tanzania Building Footprint Segmentation Challenge(TBF)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">40000± * 40000± * 3</td><td>1</td><td>13</td><td>航空影像</td><td>2018</td><td>SUZA</td></tr></tbody></table><ul><li>源地址：<a href="https://competitions.codalab.org/competitions/20100">https://competitions.codalab.org/competitions/20100</a><br><a href="http://www.graphnetcloud.cn/4-10">http://www.graphnetcloud.cn/4-10</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/76145">https://aistudio.baidu.com/aistudio/datasetdetail/76145</a></li></ul><h2 id="WHDLD"><a href="#WHDLD" class="headerlink" title="WHDLD"></a>WHDLD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>6</td><td>4940</td><td>UC Merced</td><td>2018</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://sites.google.com/view/zhouwx/dataset#h.p_hQS2jYeaFpV0">https://sites.google.com/view/zhouwx/dataset#h.p_hQS2jYeaFpV0</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55589">https://aistudio.baidu.com/aistudio/datasetdetail/55589</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1MgOUJpmClc_h4CLzJVQYvQ">https://pan.baidu.com/s/1MgOUJpmClc_h4CLzJVQYvQ</a>  提取码：gvui</li></ul><h2 id="DLRSD"><a href="#DLRSD" class="headerlink" title="DLRSD"></a>DLRSD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>21</td><td>2100</td><td>USGS National Map</td><td>2018</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="https://sites.google.com/view/zhouwx/dataset#h.p_hQS2jYeaFpV0">https://sites.google.com/view/zhouwx/dataset#h.p_hQS2jYeaFpV0</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55005">https://aistudio.baidu.com/aistudio/datasetdetail/55005</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1CmRdyNdz85ji0Z7JsrCHDw">https://pan.baidu.com/s/1CmRdyNdz85ji0Z7JsrCHDw</a>  提取码：1y38</li></ul><h2 id="DeepGlobe-Land-Cover-Classification-Challenge"><a href="#DeepGlobe-Land-Cover-Classification-Challenge" class="headerlink" title="DeepGlobe Land Cover Classification Challenge"></a>DeepGlobe Land Cover Classification Challenge</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">2448 * 2448 * 3</td><td>7</td><td>803</td><td>DigitalGlobe</td><td>2018</td><td>CVPR</td><td>0.5m</td></tr></tbody></table><ul><li>源地址：<a href="http://deepglobe.org/challenge.html">http://deepglobe.org/challenge.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55681">https://aistudio.baidu.com/aistudio/datasetdetail/55681</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1D-RpKNNuP5L-hLo4H-8txA">https://pan.baidu.com/s/1D-RpKNNuP5L-hLo4H-8txA</a>  提取码：lk1a</li></ul><h2 id="DeepGlobe-Road-Detection-Challenge"><a href="#DeepGlobe-Road-Detection-Challenge" class="headerlink" title="DeepGlobe Road Detection Challenge"></a>DeepGlobe Road Detection Challenge</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1024 * 1024 * 3</td><td>1</td><td>6,226</td><td>DigitalGlobe</td><td>2018</td><td>CVPR</td></tr></tbody></table><ul><li>源地址：<a href="http://deepglobe.org/challenge.html">http://deepglobe.org/challenge.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55682">https://aistudio.baidu.com/aistudio/datasetdetail/55682</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1uvZ7dUOjkt9_i120hr9JUQ">https://pan.baidu.com/s/1uvZ7dUOjkt9_i120hr9JUQ</a>  提取码：74be</li></ul><h2 id="Aeroscapes"><a href="#Aeroscapes" class="headerlink" title="Aeroscapes"></a>Aeroscapes</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">720 * 720 * 3</td><td>11</td><td>3269</td><td>航空影像</td><td>2018</td><td>Carnegie Mellon University</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/ishann/aeroscapes">https://github.com/ishann/aeroscapes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55222">https://aistudio.baidu.com/aistudio/datasetdetail/55222</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1-AcaV3nTDsBibbzk6Y92kw">https://pan.baidu.com/s/1-AcaV3nTDsBibbzk6Y92kw</a>  提取码：zodm</li></ul><h2 id="Map-Challenge"><a href="#Map-Challenge" class="headerlink" title="Map Challenge"></a>Map Challenge</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>标注格式</th></tr></thead><tbody><tr><td align="left">300 * 300 * 3</td><td>1</td><td>341,058</td><td>Google Map</td><td>2018</td><td>crowdAI</td><td>json</td></tr></tbody></table><ul><li>源地址：<a href="https://www.crowdai.org/challenges/mapping-challenge">https://www.crowdai.org/challenges/mapping-challenge</a> ；<a href="https://www.jianshu.com/p/90efc39975da">https://www.jianshu.com/p/90efc39975da</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54858">https://aistudio.baidu.com/aistudio/datasetdetail/54858</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1keRAm0OEanCDuRUPpUra6A%20">https://pan.baidu.com/s/1keRAm0OEanCDuRUPpUra6A</a> 提取码：pm0m</li></ul><h2 id="38-Cloud-A-Cloud-Segmentation-Dataset"><a href="#38-Cloud-A-Cloud-Segmentation-Dataset" class="headerlink" title="38-Cloud: A Cloud Segmentation Dataset"></a>38-Cloud: A Cloud Segmentation Dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">384* 384 * 4</td><td>1</td><td>8400</td><td>Landsat 8</td><td>2018</td><td>Science Simon Fraser University</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/SorourMo/38-Cloud-A-Cloud-Segmentation-Dataset">https://github.com/SorourMo/38-Cloud-A-Cloud-Segmentation-Dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56236">https://aistudio.baidu.com/aistudio/datasetdetail/56236</a></li></ul><h2 id="WHU-Building-Dataset-Satellite-dataset-Ⅰ-global-cities"><a href="#WHU-Building-Dataset-Satellite-dataset-Ⅰ-global-cities" class="headerlink" title="WHU Building Dataset,Satellite dataset Ⅰ (global cities)"></a>WHU Building Dataset,Satellite dataset Ⅰ (global cities)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>1</td><td>204</td><td>QuickBird, Worldview series, IKONOS, ZY-3</td><td>2019</td><td>武汉大学</td><td>0.3 ~ 2.5m</td></tr></tbody></table><ul><li>源地址：<a href="https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html">https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55774">https://aistudio.baidu.com/aistudio/datasetdetail/55774</a></li></ul><h2 id="WHU-Building-Dataset-Satellite-dataset-Ⅱ-East-Asia"><a href="#WHU-Building-Dataset-Satellite-dataset-Ⅱ-East-Asia" class="headerlink" title="WHU Building Dataset,Satellite dataset Ⅱ (East Asia)"></a>WHU Building Dataset,Satellite dataset Ⅱ (East Asia)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>1</td><td>17388</td><td>QuickBird, Worldview series, IKONOS, ZY-3</td><td>2019</td><td>武汉大学</td><td>0.45 m</td></tr></tbody></table><ul><li>源地址：<a href="https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html">https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56356">https://aistudio.baidu.com/aistudio/datasetdetail/56356</a></li></ul><h2 id="WHU-Building-Dataset-Aerial-imagery-dataset"><a href="#WHU-Building-Dataset-Aerial-imagery-dataset" class="headerlink" title="WHU Building Dataset,Aerial imagery dataset"></a>WHU Building Dataset,Aerial imagery dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>1</td><td>8,189</td><td>未知</td><td>2019</td><td>武汉大学</td><td>0.3 m</td></tr></tbody></table><ul><li>源地址：<a href="https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html">https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56502">https://aistudio.baidu.com/aistudio/datasetdetail/56502</a></li></ul><h2 id="DroneDeploy"><a href="#DroneDeploy" class="headerlink" title="DroneDeploy"></a>DroneDeploy</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">6000± * 6000± * 3</td><td>7</td><td>35 train, 8 val, 12 test</td><td>航空影像drones</td><td>2019</td><td>DroneDeploy</td><td>0.1 m</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/dronedeploy/dd-ml-segmentation-benchmark">https://github.com/dronedeploy/dd-ml-segmentation-benchmark</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/79283">https://aistudio.baidu.com/aistudio/datasetdetail/79283</a></li></ul><h2 id="RoadTracer"><a href="#RoadTracer" class="headerlink" title="RoadTracer"></a>RoadTracer</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">4096 * 4096 * 3</td><td>1</td><td>3000</td><td>Google earth、OSM</td><td>2019</td><td>MIT</td><td>0.6m</td></tr></tbody></table><ul><li>源地址：<a href="%20https://github.com/mitroadmaps/roadtracer/">https://github.com/mitroadmaps/roadtracer/</a><br><a href="https://github.com/tansor/VecRoad">https://github.com/tansor/VecRoad</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/74848">https://aistudio.baidu.com/aistudio/datasetdetail/74848</a></li></ul><h2 id="ORSSD"><a href="#ORSSD" class="headerlink" title="ORSSD"></a>ORSSD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500± * 500± * 3</td><td>8</td><td>600train，200test</td><td>Google Earth</td><td>2019</td><td>北京交通大学</td></tr></tbody></table><ul><li>源地址：<a href="https://hub.fastgit.org/rmcong/ORSSD-dataset">https://hub.fastgit.org/rmcong/ORSSD-dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/98275">https://aistudio.baidu.com/aistudio/datasetdetail/98275</a></li></ul><h2 id="EORSSD"><a href="#EORSSD" class="headerlink" title="EORSSD"></a>EORSSD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">500± * 500± * 3</td><td>8</td><td>1400train, 600test</td><td>Google Earth</td><td>2020</td><td>北京交通大学</td></tr></tbody></table><ul><li>源地址：[<a href="https://hub.fastgit.org/rmcong/EORSSD-dataset">https://hub.fastgit.org/rmcong/EORSSD-dataset</a> ]<a href="https://hub.fastgit.org/rmcong/EORSSD-dataset">https://hub.fastgit.org/rmcong/EORSSD-dataset</a> )</li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/98372">https://aistudio.baidu.com/aistudio/datasetdetail/98372</a></li></ul><h2 id="Land-Cover-from-Aerial-Imagery（landcover-ai）"><a href="#Land-Cover-from-Aerial-Imagery（landcover-ai）" class="headerlink" title="Land Cover from Aerial Imagery（landcover_ai）"></a>Land Cover from Aerial Imagery（landcover_ai）</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">9000 * 9500 * 3，4200 * 4700 * 3</td><td>3</td><td>41</td><td>public geodetic resource</td><td>2020</td><td>linuxpolska</td><td>0.25m，0.5m</td></tr></tbody></table><ul><li>源地址：<a href="https://landcover.ai/">https://landcover.ai/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/76629">https://aistudio.baidu.com/aistudio/datasetdetail/76629</a></li></ul><h2 id="UAVid"><a href="#UAVid" class="headerlink" title="UAVid"></a>UAVid</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4096 * 2160 * 3 or 3840 * 2160 * 3</td><td>8</td><td>300</td><td>航空影像</td><td>2020</td><td>University of Twente</td></tr></tbody></table><ul><li>源地址：<a href="https://www.uavid.nl/">https://www.uavid.nl/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55774">https://aistudio.baidu.com/aistudio/datasetdetail/55774</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/14y_il9Axq_vMYQGdfz9JqQ%20">https://pan.baidu.com/s/14y_il9Axq_vMYQGdfz9JqQ</a> 提取码：4dv7</li></ul><h2 id="95-Cloud-An-Extension-to-38-Cloud-Dataset"><a href="#95-Cloud-An-Extension-to-38-Cloud-Dataset" class="headerlink" title="95-Cloud: An Extension to 38-Cloud Dataset"></a>95-Cloud: An Extension to 38-Cloud Dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">384 * 384 * 4</td><td>1</td><td>34,701</td><td>Landsat 8</td><td>2020</td><td>Simon Fraser University</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/SorourMo/95-Cloud-An-Extension-to-38-Cloud-Dataset">https://github.com/SorourMo/95-Cloud-An-Extension-to-38-Cloud-Dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56839">https://aistudio.baidu.com/aistudio/datasetdetail/56839</a></li></ul><h2 id="AI-遥感影像"><a href="#AI-遥感影像" class="headerlink" title="AI+遥感影像"></a>AI+遥感影像</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>8</td><td>100000</td><td>未知</td><td>2020</td><td>全国人工智能大赛组委会</td></tr></tbody></table><ul><li>源地址：<a href="https://www.datafountain.cn/competitions/457">https://www.datafountain.cn/competitions/457</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/51568">https://aistudio.baidu.com/aistudio/datasetdetail/51568</a></li></ul><h2 id="BDCI2020"><a href="#BDCI2020" class="headerlink" title="BDCI2020"></a>BDCI2020</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>7</td><td>145981</td><td>未知</td><td>2020</td><td>BDCI</td></tr></tbody></table><ul><li>源地址：<a href="https://www.datafountain.cn/competitions/475">https://www.datafountain.cn/competitions/475</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/56051">https://aistudio.baidu.com/aistudio/datasetdetail/56051</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1XiECAvdc0UIOpyPHMvR09w">https://pan.baidu.com/s/1XiECAvdc0UIOpyPHMvR09w</a>  提取码：71sz</li></ul><h2 id="mini-Inria-Aerial-Image-Labeling-Dataset"><a href="#mini-Inria-Aerial-Image-Labeling-Dataset" class="headerlink" title="mini Inria Aerial Image Labeling Dataset"></a>mini Inria Aerial Image Labeling Dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3</td><td>1</td><td>30000 train, 2500 test</td><td>航空影像</td><td>2021</td><td>天池大赛</td><td>0.3 m</td></tr></tbody></table><ul><li>源地址：<a href="https://tianchi.aliyun.com/competition/entrance/531872/introduction">https://tianchi.aliyun.com/competition/entrance/531872/introduction</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/70361">https://aistudio.baidu.com/aistudio/datasetdetail/70361</a></li></ul><h2 id="ISPRS-2D-Semantic-Labeling-Contest"><a href="#ISPRS-2D-Semantic-Labeling-Contest" class="headerlink" title="ISPRS 2D Semantic Labeling Contest"></a>ISPRS 2D Semantic Labeling Contest</h2><h3 id="Postdam"><a href="#Postdam" class="headerlink" title="Postdam"></a>Postdam</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">6000 * 6000 * 3</td><td>6</td><td>38</td><td>航空影像</td><td>2012</td><td>ISPRS</td><td>0.05m</td></tr></tbody></table><h3 id="Vaihingen"><a href="#Vaihingen" class="headerlink" title="Vaihingen"></a>Vaihingen</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">1000<del>4000 * 1000</del>4000 * 3</td><td>6</td><td>33</td><td>航空影像</td><td>2012</td><td>ISPRS</td><td>0.09m</td></tr></tbody></table><ul><li>源地址：<a href="https://www2.isprs.org/commissions/comm2/wg4/benchmark/semantic-labeling/">https://www2.isprs.org/commissions/comm2/wg4/benchmark/semantic-labeling/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/55153">https://aistudio.baidu.com/aistudio/datasetdetail/55153</a>  ； <a href="https://aistudio.baidu.com/aistudio/datasetdetail/55408">https://aistudio.baidu.com/aistudio/datasetdetail/55408</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1Mu-rzPorbtsYr1XJpkJq-Q">https://pan.baidu.com/s/1Mu-rzPorbtsYr1XJpkJq-Q</a>  提取码：n4i0</li></ul><h2 id="GID-1"><a href="#GID-1" class="headerlink" title="GID"></a>GID</h2><h3 id="GID-Fine-Land-cover-Classification-15classes"><a href="#GID-Fine-Land-cover-Classification-15classes" class="headerlink" title="GID Fine Land-cover Classification_15classes"></a>GID Fine Land-cover Classification_15classes</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">7200 * 6800 * 4 <br>7200 * 6800 * 3</td><td>15</td><td>10</td><td>高分2</td><td>2018</td><td>武汉大学</td><td>0.8 to 10m</td></tr></tbody></table><h3 id="GID-Large-scale-Classification-5classes"><a href="#GID-Large-scale-Classification-5classes" class="headerlink" title="GID Large-scale Classification_5classes"></a>GID Large-scale Classification_5classes</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">7200 * 6800 * 4 <br>7200 * 6800 * 3</td><td>5</td><td>150</td><td>高分2</td><td>2018</td><td>武汉大学</td><td>0.8 to 10m</td></tr></tbody></table><ul><li>源地址：<a href="https://x-ytong.github.io/project/GID.html">https://x-ytong.github.io/project/GID.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/54878">https://aistudio.baidu.com/aistudio/datasetdetail/54878</a>  ； <a href="https://aistudio.baidu.com/aistudio/datasetdetail/54934">https://aistudio.baidu.com/aistudio/datasetdetail/54934</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/1Xux-3nKTbu1v3Mp5tdyM7A">https://pan.baidu.com/s/1Xux-3nKTbu1v3Mp5tdyM7A</a>  提取码：d3qs</li></ul><h2 id="UDD"><a href="#UDD" class="headerlink" title="UDD"></a>UDD</h2><h3 id="UDD5"><a href="#UDD5" class="headerlink" title="UDD5"></a>UDD5</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4096± * 2160± * 3</td><td>5</td><td>120 trian，40 val</td><td>无人机数据（DJI Phantom 4）</td><td>2018</td><td>北京大学</td></tr></tbody></table><h3 id="UDD6"><a href="#UDD6" class="headerlink" title="UDD6"></a>UDD6</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">4096± * 2160± * 3</td><td>6</td><td>106 trian，35 val</td><td>无人机数据（DJI Phantom 4）</td><td>2018</td><td>北京大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/MarcWong/UDD">https://github.com/MarcWong/UDD</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/75675">https://aistudio.baidu.com/aistudio/datasetdetail/75675</a></li></ul><h2 id="BH-DATASET"><a href="#BH-DATASET" class="headerlink" title="BH-DATASET"></a>BH-DATASET</h2><h3 id="BH-POOLS"><a href="#BH-POOLS" class="headerlink" title="BH-POOLS"></a>BH-POOLS</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">3840 * 2160 * 3</td><td>1</td><td>200</td><td>GoogleEarth</td><td>2020</td><td>Federal University of Minas Gerais</td></tr></tbody></table><h3 id="BH-WATERTANKS"><a href="#BH-WATERTANKS" class="headerlink" title="BH-WATERTANKS"></a>BH-WATERTANKS</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">3840 * 2160 * 3</td><td>1</td><td>200</td><td>GoogleEarth</td><td>2020</td><td>Federal University of Minas Gerais</td></tr></tbody></table><ul><li>源地址：<a href="http://www.patreo.dcc.ufmg.br/2020/07/29/bh-pools-watertanks-datasets/">http://www.patreo.dcc.ufmg.br/2020/07/29/bh-pools-watertanks-datasets/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/57579">https://aistudio.baidu.com/aistudio/datasetdetail/57579</a></li></ul><h2 id="Inria-Aerial-Image-Labeling-Dataset"><a href="#Inria-Aerial-Image-Labeling-Dataset" class="headerlink" title="Inria Aerial Image Labeling Dataset"></a>Inria Aerial Image Labeling Dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">未知</td><td>1</td><td>未知</td><td>航空影像</td><td>2017</td><td>Inria Sophia Antipolis - Mediterran ee, TITANE team; Inria Saclay, TAO team, France</td><td>0.3 m</td></tr></tbody></table><ul><li>源地址：<a href="https://project.inria.fr/aerialimagelabeling/">https://project.inria.fr/aerialimagelabeling/</a> ；<a href="https://hyper.ai/datasets/5428">https://hyper.ai/datasets/5428</a></li></ul><h2 id="rscup-2"><a href="#rscup-2" class="headerlink" title="rscup"></a>rscup</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">7200 * 6800 * 4</td><td>16</td><td>train 8, val 2, test 10</td><td>高分二号MSS影像</td><td>2019</td><td>rscup组委会</td><td>4 m</td></tr></tbody></table><ul><li>源地址：<a href="http://rscup.bjxintong.com.cn/#/theme/3">http://rscup.bjxintong.com.cn/#/theme/3</a></li></ul><h2 id="suichang-dataset"><a href="#suichang-dataset" class="headerlink" title="suichang dataset"></a>suichang dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">256 * 256 * 4</td><td>10</td><td>16017</td><td>高分系列</td><td>2021</td><td>浙江大学、天池大赛</td><td>0.8~ 2m</td></tr></tbody></table><ul><li>源地址：<a href="https://tianchi.aliyun.com/competition/entrance/531860/rankingList">https://tianchi.aliyun.com/competition/entrance/531860/rankingList</a></li></ul><h2 id="LRSNY"><a href="#LRSNY" class="headerlink" title="LRSNY"></a>LRSNY</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">1000 *  1000 * 3</td><td>1</td><td>716 train, 220 val, 432 test</td><td>未知</td><td>2021</td><td>IEEE</td><td>0.5 m</td></tr></tbody></table><ul><li>源地址：<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9333652">https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9333652</a><br><a href="ftp://154.85.52.76/LRSNY/">ftp://154.85.52.76/LRSNY/</a></li></ul><h1 id="遥感影像变化检测"><a href="#遥感影像变化检测" class="headerlink" title="遥感影像变化检测"></a>遥感影像变化检测</h1><p><strong>收集网络中开源的、关于遥感影像 变化检测的深度学习数据集</strong></p><p>main ref </p><blockquote><p><a href="https://captain-whu.github.io/DiRS/">https://captain-whu.github.io/DiRS/</a><br><a href="https://mp.weixin.qq.com/s/GN9SEztb61cm0ukn81Szbg">https://mp.weixin.qq.com/s/GN9SEztb61cm0ukn81SzbgA</a></p></blockquote><h2 id="SZTAKI-INRIA-AirChange"><a href="#SZTAKI-INRIA-AirChange" class="headerlink" title="SZTAKI-INRIA AirChange"></a>SZTAKI-INRIA AirChange</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">952 * 640 * 3, 2 time</td><td>1</td><td>13 * 2</td><td>未知</td><td>2009</td><td>MTA SZTAKI</td><td>1.5m</td></tr></tbody></table><ul><li>源地址：<a href="http://web.eee.sztaki.hu/remotesensing/airchange_benchmark.html">http://web.eee.sztaki.hu/remotesensing/airchange_benchmark.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/77781">https://aistudio.baidu.com/aistudio/datasetdetail/77781</a></li></ul><h2 id="AIST-Building-Change-Detection-ABCD"><a href="#AIST-Building-Change-Detection-ABCD" class="headerlink" title="AIST Building Change Detection(ABCD)"></a>AIST Building Change Detection(ABCD)</h2><h3 id="fixed-scale"><a href="#fixed-scale" class="headerlink" title="fixed-scale"></a>fixed-scale</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">160 * 160 * 6, 2 time</td><td>1</td><td>4,253 * 2</td><td>aerial images</td><td>2017</td><td>AIST</td><td>1.5m</td></tr></tbody></table><h3 id="resized"><a href="#resized" class="headerlink" title="resized"></a>resized</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">128 * 128 * 3, 2 time</td><td>1</td><td>4,223 * 2</td><td>aerial images</td><td>2017</td><td>AIST</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/gistairc/ABCDdataset">https://github.com/gistairc/ABCDdataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/79596">https://aistudio.baidu.com/aistudio/datasetdetail/79596</a></li></ul><h2 id="WHU-Building-Change-Detection-Dataset"><a href="#WHU-Building-Change-Detection-Dataset" class="headerlink" title="WHU Building Change Detection Dataset"></a>WHU Building Change Detection Dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">32,207 * 15,354 * 3, 2 time</td><td>1</td><td>1 * 2</td><td>航空影像</td><td>2018</td><td>武汉大学</td><td>0.2m</td></tr></tbody></table><ul><li>源地址：<a href="https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html">https://study.rsgis.whu.edu.cn/pages/download/building_dataset.html</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/79035">https://aistudio.baidu.com/aistudio/datasetdetail/79035</a></li></ul><h2 id="season-varying"><a href="#season-varying" class="headerlink" title="season-varying"></a>season-varying</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3, 2 time</td><td>1</td><td>10000train, 3000val, 3000test</td><td>Google Earth (DigitalGlobe)</td><td>2018</td><td>GosNIIAS</td><td>0.03~1m</td></tr></tbody></table><ul><li>源地址：<a href="https://paperswithcode.com/dataset/cdd-dataset-season-varying">https://paperswithcode.com/dataset/cdd-dataset-season-varying</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/78676">https://aistudio.baidu.com/aistudio/datasetdetail/78676</a></li></ul><h2 id="Onera-Satellite-Change-Detection-OSCD"><a href="#Onera-Satellite-Change-Detection-OSCD" class="headerlink" title="Onera Satellite Change Detection (OSCD)"></a>Onera Satellite Change Detection (OSCD)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">600± * 600± * 13, 2 time</td><td>1</td><td>14 * 2 train, 10 * 2 test</td><td>Sentinel-2</td><td>2018</td><td>Universit´e Paris-Saclay、 T´el´ecom ParisTech</td></tr></tbody></table><ul><li>源地址：<a href="https://rcdaudt.github.io/oscd/">https://rcdaudt.github.io/oscd/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/72898">https://aistudio.baidu.com/aistudio/datasetdetail/72898</a></li></ul><h2 id="Multi-temporal-Scene-WuHan-MtS-WH"><a href="#Multi-temporal-Scene-WuHan-MtS-WH" class="headerlink" title="Multi-temporal Scene WuHan (MtS-WH)"></a>Multi-temporal Scene WuHan (MtS-WH)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">7200 * 6000 * 4，2 time</td><td>9</td><td>190 * 2</td><td>IKONOS传感器</td><td>2019</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="http://sigma.whu.edu.cn/newspage.php?q=2019_03_26">http://sigma.whu.edu.cn/newspage.php?q=2019_03_26</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/70452">https://aistudio.baidu.com/aistudio/datasetdetail/70452</a></li></ul><h2 id="High-Resolution-Semantic-Change-HRSCD"><a href="#High-Resolution-Semantic-Change-HRSCD" class="headerlink" title="High Resolution Semantic Change (HRSCD)"></a>High Resolution Semantic Change (HRSCD)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">10000 * 10000 * 3, 2 time</td><td>6</td><td>291</td><td>IGS’s BD ORTHO database</td><td>2019</td><td>ETH Zürich / EcoVision Lab</td><td>0.5 m</td></tr></tbody></table><ul><li>源地址：<a href="https://rcdaudt.github.io/hrscd/">https://rcdaudt.github.io/hrscd/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/87088">https://aistudio.baidu.com/aistudio/datasetdetail/87088</a></li></ul><h2 id="Change-Detection-Dataset-CDD"><a href="#Change-Detection-Dataset-CDD" class="headerlink" title="Change Detection Dataset(CDD)"></a>Change Detection Dataset(CDD)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">984 * 740 * 224,  600 * 500 * 224, 390 * 200 * 242,  2 time</td><td>3,5</td><td>3 * 2</td><td>HYPERION,AVIRIS sensor</td><td>2019</td><td>未知</td><td>未知</td></tr></tbody></table><ul><li>源地址：<a href="https://gitlab.citius.usc.es/hiperespectral/ChangeDetectionDataset">https://gitlab.citius.usc.es/hiperespectral/ChangeDetectionDataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/89523">https://aistudio.baidu.com/aistudio/datasetdetail/89523</a></li></ul><h2 id="xBD"><a href="#xBD" class="headerlink" title="xBD"></a>xBD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">1024 * 1024,  3/4/8 band, 4 state</td><td>4</td><td>22068</td><td>DigitalGlobe</td><td>2020</td><td>MIT</td></tr></tbody></table><ul><li>源地址：<a href="https://xview2.org/dataset">https://xview2.org/dataset</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/86660">https://aistudio.baidu.com/aistudio/datasetdetail/86660</a></li></ul><h2 id="Google-dataset"><a href="#Google-dataset" class="headerlink" title="Google dataset"></a>Google dataset</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">1000<del>5000 * 1000</del>5000 * 3, 2 time</td><td>1</td><td>20</td><td>Google Earth</td><td>2020</td><td>ieee</td><td>0.55 m</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/daifeng2016/Change-Detection-Dataset-for-High-Resolution-Satellite-Imagery">https://github.com/daifeng2016/Change-Detection-Dataset-for-High-Resolution-Satellite-Imagery</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/75099">https://aistudio.baidu.com/aistudio/datasetdetail/75099</a></li></ul><h2 id="LEVIR-CD"><a href="#LEVIR-CD" class="headerlink" title="LEVIR-CD"></a>LEVIR-CD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">1024 * 1024 * 3, 2 time</td><td>1</td><td>637</td><td>Google Earth</td><td>2020</td><td>北京航空航天大学</td><td>0.5 m</td></tr></tbody></table><ul><li>源地址：<a href="https://justchenhao.github.io/LEVIR/">https://justchenhao.github.io/LEVIR/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/75459">https://aistudio.baidu.com/aistudio/datasetdetail/75459</a></li></ul><h2 id="SenseEarth-ChangeDetection"><a href="#SenseEarth-ChangeDetection" class="headerlink" title="SenseEarth ChangeDetection"></a>SenseEarth ChangeDetection</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3, 2 time</td><td>6</td><td>2968 train, 847 val</td><td>未知</td><td>2020</td><td>商汤科技</td><td>0.5~3m</td></tr></tbody></table><ul><li>源地址：<a href="https://rs.sensetime.com/competition/index.html#/info">https://rs.sensetime.com/competition/index.html#/info</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53484">https://aistudio.baidu.com/aistudio/datasetdetail/53484</a></li></ul><h2 id="SEmantic-Change-detectiON-Data-SECOND"><a href="#SEmantic-Change-detectiON-Data-SECOND" class="headerlink" title="SEmantic Change detectiON Data(SECOND)"></a>SEmantic Change detectiON Data(SECOND)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">512 * 512 * 3, 2 time</td><td>6</td><td>4662 * 2</td><td>未知</td><td>2020</td><td>武汉大学</td></tr></tbody></table><ul><li>源地址：<a href="http://www.captain-whu.com/project/SCD/">http://www.captain-whu.com/project/SCD/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/87088">https://aistudio.baidu.com/aistudio/datasetdetail/87088</a></li></ul><h2 id="Sun-Yat-Sen-University-SYSU-CD"><a href="#Sun-Yat-Sen-University-SYSU-CD" class="headerlink" title="Sun Yat-Sen University (SYSU)-CD"></a>Sun Yat-Sen University (SYSU)-CD</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3, 2 time</td><td>1</td><td>20000 * 2</td><td>aerial image</td><td>2021</td><td>中山大学</td></tr></tbody></table><ul><li>源地址：<a href="https://hub.fastgit.org/liumency/SYSU-CD">https://hub.fastgit.org/liumency/SYSU-CD</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/98596">https://aistudio.baidu.com/aistudio/datasetdetail/98596</a></li></ul><h2 id="rscup-3"><a href="#rscup-3" class="headerlink" title="rscup"></a>rscup</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">960 * 960 * 4, 2 time</td><td>1</td><td>未知</td><td>未知</td><td>2020</td><td>rscup组委会</td></tr></tbody></table><ul><li>源地址：<a href="http://rscup.bjxintong.com.cn/#/theme/4">http://rscup.bjxintong.com.cn/#/theme/4</a></li></ul><h1 id="遥感影像场景分类——多-x2F-高光谱"><a href="#遥感影像场景分类——多-x2F-高光谱" class="headerlink" title="遥感影像场景分类——多/高光谱"></a>遥感影像场景分类——多/高光谱</h1><p><strong>收集网络中开源的、关于高光谱遥感影像 图像分类的深度学习数据集</strong></p><h2 id="EuroSAT"><a href="#EuroSAT" class="headerlink" title="EuroSAT"></a>EuroSAT</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">64 * 64 * 3<br>64 * 64 * 13</td><td>10</td><td>27,000</td><td>10 m</td><td>哨兵2</td><td>2018</td><td>德国凯泽斯劳滕大学</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/phelber/eurosat">https://github.com/phelber/eurosat</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/52650">https://aistudio.baidu.com/aistudio/datasetdetail/52650</a> </li><li>百度云盘备份链接：<a href="https://pan.baidu.com/s/15PEr2iWIq-U0kBF7Jp8Fmg">https://pan.baidu.com/s/15PEr2iWIq-U0kBF7Jp8Fmg</a>  提取码：d34f</li></ul><h2 id="TG1HRSSC"><a href="#TG1HRSSC" class="headerlink" title="TG1HRSSC"></a>TG1HRSSC</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th><th>空间分辨率</th></tr></thead><tbody><tr><td align="left">512 * 512 * 1，512 * 512 * 54，512 * 512 * 52</td><td>9</td><td>204</td><td>天宫一号</td><td>2021</td><td>中国科学院空间应用工程与技术中心</td><td>0.5—0.8μm、1band(PAN), 0.4—1.0μm、54band((VNI), 1.0—2.5μm、52band((SWI),</td><td>5m(PAN), 10m(VNI), 20m(SWI)</td></tr></tbody></table><ul><li>源地址：<a href="http://www.msadc.cn/main/setsubDetail?id=1369487569196158978">http://www.msadc.cn/main/setsubDetail?id=1369487569196158978</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/86229">https://aistudio.baidu.com/aistudio/datasetdetail/86229</a></li></ul><h2 id="NaSC-TG2"><a href="#NaSC-TG2" class="headerlink" title="NaSC-TG2"></a>NaSC-TG2</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th><th>空间分辨率</th></tr></thead><tbody><tr><td align="left">128 * 128 * 3， 128 * 128 * 14</td><td>10</td><td>20000</td><td>天宫二号</td><td>2021</td><td>中国科学院空间应用工程与技术中心</td><td>0.40–1.04 µm</td><td></td></tr></tbody></table><ul><li>源地址：<a href="http://www.msadc.cn/main/setsubDetail?id=1370312964720037889">http://www.msadc.cn/main/setsubDetail?id=1370312964720037889</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/86451">https://aistudio.baidu.com/aistudio/datasetdetail/86451</a></li></ul><h1 id="遥感影像目标检测——多-x2F-高光谱"><a href="#遥感影像目标检测——多-x2F-高光谱" class="headerlink" title="遥感影像目标检测——多/高光谱"></a>遥感影像目标检测——多/高光谱</h1><p><strong>收集网络中开源的、关于高光谱遥感影像 图像分类的深度学习数据集</strong></p><h2 id="Dstl-Satellite-Imagery-Feature-Detection"><a href="#Dstl-Satellite-Imagery-Feature-Detection" class="headerlink" title="Dstl Satellite Imagery Feature Detection"></a>Dstl Satellite Imagery Feature Detection</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th></tr></thead><tbody><tr><td align="left">13348 * 3392 * 3，837 * 848 * 8，134 * 136 * 8</td><td>10</td><td>25 train, 32 test</td><td>250+</td><td>MultiPolygons</td><td>WorldView 3</td><td>2017</td><td>DigitalGlobe</td><td>全色0.31 m, 多光谱1.24 m, 短波红外7.5 m</td></tr></tbody></table><ul><li>源地址：<a href="https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection/data">https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection/data</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/87506">https://aistudio.baidu.com/aistudio/datasetdetail/87506</a></li></ul><h2 id="xView"><a href="#xView" class="headerlink" title="xView"></a>xView</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>目标总数</th><th>分辨率</th><th>标注格式</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">3000± * 3000± * 3<br>3000± * 3000± * 8</td><td>60</td><td>1129</td><td>1,000,000</td><td>0.3m</td><td>HBB</td><td>WorldView 3</td><td>2018</td><td>DIUx、NGA</td></tr></tbody></table><ul><li>源地址：<a href="https://challenge.xviewdataset.org/data-download">https://challenge.xviewdataset.org/data-download</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/53622">https://aistudio.baidu.com/aistudio/datasetdetail/53622</a></li></ul><h1 id="遥感影像图像分割——多-x2F-高光谱"><a href="#遥感影像图像分割——多-x2F-高光谱" class="headerlink" title="遥感影像图像分割——多/高光谱"></a>遥感影像图像分割——多/高光谱</h1><p><strong>收集网络中开源的、关于高光谱遥感影像 图像分割的深度学习数据集</strong></p><p>main ref </p><blockquote><p><a href="https://mp.weixin.qq.com/s/S_TEoWyYFtdjtDeEXmpBPg">https://mp.weixin.qq.com/s/S_TEoWyYFtdjtDeEXmpBPg</a><br><a href="http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes">http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes</a></p></blockquote><h2 id="Salinas"><a href="#Salinas" class="headerlink" title="Salinas"></a>Salinas</h2><h3 id="Salinas-scene"><a href="#Salinas-scene" class="headerlink" title="Salinas scene"></a>Salinas scene</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">512 * 217 * 224</td><td>16</td><td>1</td><td>AVIRIS sensor</td><td>2011</td><td>未知</td></tr></tbody></table><h3 id="Salinas-A-scene"><a href="#Salinas-A-scene" class="headerlink" title="Salinas-A scene"></a>Salinas-A scene</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">83 * 86 * 224</td><td>6</td><td>1</td><td>AVIRIS sensor</td><td>2011</td><td>未知</td></tr></tbody></table><ul><li>源地址：<a href="http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes">http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/82020">https://aistudio.baidu.com/aistudio/datasetdetail/82020</a></li></ul><h2 id="Pavia-Centre-and-University"><a href="#Pavia-Centre-and-University" class="headerlink" title="Pavia Centre and University"></a>Pavia Centre and University</h2><h3 id="Pavia-Centre-scene"><a href="#Pavia-Centre-scene" class="headerlink" title="Pavia Centre scene"></a>Pavia Centre scene</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th></tr></thead><tbody><tr><td align="left">1096 * 1096 * 102</td><td>9</td><td>1</td><td>ROSIS sensor</td><td>2011</td><td>NPavia university</td><td>0.43-0.86μm</td></tr></tbody></table><h3 id="Pavia-University-scene"><a href="#Pavia-University-scene" class="headerlink" title="Pavia University scene"></a>Pavia University scene</h3><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th></tr></thead><tbody><tr><td align="left">610 * 610 * 103</td><td>9</td><td>1</td><td>ROSIS sensor)</td><td>2011</td><td>Pavia university</td><td>0.43-0.86μm</td></tr></tbody></table><ul><li>源地址：<a href="http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes">http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/81260">https://aistudio.baidu.com/aistudio/datasetdetail/81260</a></li></ul><h2 id="Washington-DC-MALL"><a href="#Washington-DC-MALL" class="headerlink" title="Washington DC MALL"></a>Washington DC MALL</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th></tr></thead><tbody><tr><td align="left">1280 * 307 * 191</td><td>7</td><td>1</td><td>机载高光谱数据</td><td>2013</td><td>Spectral Information Technology Application Center of Virginia</td><td>0.4到2.4 µm</td></tr></tbody></table><ul><li>源地址：<a href="http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes">http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/83251">https://aistudio.baidu.com/aistudio/datasetdetail/83251</a></li></ul><h2 id="IKennedy-Space-Center-KSC"><a href="#IKennedy-Space-Center-KSC" class="headerlink" title="IKennedy Space Center (KSC)"></a>IKennedy Space Center (KSC)</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th></tr></thead><tbody><tr><td align="left">512 * 614 * 176</td><td>13</td><td>1</td><td>NASA AVIRIS</td><td>2014</td><td>Pavia university</td><td>400–2500 nm</td></tr></tbody></table><ul><li>源地址：<a href="http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes">http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/81260">https://aistudio.baidu.com/aistudio/datasetdetail/81260</a></li></ul><h2 id="Botswana"><a href="#Botswana" class="headerlink" title="Botswana"></a>Botswana</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th></tr></thead><tbody><tr><td align="left">1476 * 256 * 145</td><td>14</td><td>1</td><td>Hyperion sensor on EO-1</td><td>2014</td><td>未知</td><td>未知</td></tr></tbody></table><ul><li>源地址：<a href="http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes">http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/82578">https://aistudio.baidu.com/aistudio/datasetdetail/82578</a></li></ul><h2 id="Indian-Pines"><a href="#Indian-Pines" class="headerlink" title="Indian Pines"></a>Indian Pines</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>波谱范围</th></tr></thead><tbody><tr><td align="left">145 * 145 * 224；614 * 1848 * 224；2678 * 614 * 224</td><td>16</td><td>3</td><td>AVIRIS sensor</td><td>2015</td><td>Pursue univeristy</td><td>400–2500 nm</td></tr></tbody></table><ul><li>源地址：<a href="https://purr.purdue.edu/publications/1947/1">https://purr.purdue.edu/publications/1947/1</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/80970">https://aistudio.baidu.com/aistudio/datasetdetail/80970</a></li></ul><h2 id="HyRANK"><a href="#HyRANK" class="headerlink" title="HyRANK"></a>HyRANK</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">250± * 1000± * 176</td><td>14</td><td>2train, 3val</td><td>Hyperion sensor (EO-1, USGS)</td><td>2018</td><td>National Technical University of Athen</td></tr></tbody></table><ul><li>源地址：<a href="https://www2.isprs.org/commissions/comm3/wg4/hyrank/">https://www2.isprs.org/commissions/comm3/wg4/hyrank/</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/80840">https://aistudio.baidu.com/aistudio/datasetdetail/80840</a></li></ul><h2 id="RIT-18"><a href="#RIT-18" class="headerlink" title="RIT-18"></a>RIT-18</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">9,393 * 5,642 * 7；8,833 * 6,918 * 7；12,446 * 7,654 * 7</td><td>18</td><td>3</td><td>Tetracam Micro-MCA6</td><td>2018</td><td>Rochester Institute of Technology</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/rmkemker/RIT-18">https://github.com/rmkemker/RIT-18</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/98991">https://aistudio.baidu.com/aistudio/datasetdetail/98991</a></li></ul><h1 id="遥感影像多标签分类"><a href="#遥感影像多标签分类" class="headerlink" title="遥感影像多标签分类"></a>遥感影像多标签分类</h1><h2 id="MLRSNet"><a href="#MLRSNet" class="headerlink" title="MLRSNet"></a>MLRSNet</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>分辨率</th><th>最大标签数</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>46</td><td>109,161</td><td>Google Earth</td><td>2020</td><td>中国地质大学</td><td>0.1~10m</td><td>13</td></tr></tbody></table><ul><li>源地址：<a href="https://data.mendeley.com/datasets/7j9bv9vwsx/2">https://data.mendeley.com/datasets/7j9bv9vwsx/2</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/76804">https://aistudio.baidu.com/aistudio/datasetdetail/76804</a></li></ul><h2 id="BigEarthNet"><a href="#BigEarthNet" class="headerlink" title="BigEarthNet"></a>BigEarthNet</h2><table><thead><tr><th align="left">图像大小</th><th>类别数</th><th>总样本数</th><th>分辨率</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>备注</th></tr></thead><tbody><tr><td align="left">120 * 120 <br> 60 * 60 <br> 20 * 20;</td><td>43</td><td>590326</td><td>10 m <br> 20 m <br> 60m</td><td>哨兵2</td><td>2019</td><td>柏林工业大学</td><td>多标签</td></tr></tbody></table><ul><li>源地址：<a href="http://bigearth.net/">http://bigearth.net/</a></li></ul><h1 id="遥感影像图像标题"><a href="#遥感影像图像标题" class="headerlink" title="遥感影像图像标题"></a>遥感影像图像标题</h1><p><strong>收集网络中开源的、关于图像标题的深度学习数据集</strong></p><p>main ref </p><blockquote><p><a href="https://github.com/201528014227051/RSICD_optimal">https://github.com/201528014227051/RSICD_optimal</a></p></blockquote><h2 id="UCM-caption"><a href="#UCM-caption" class="headerlink" title="UCM caption"></a>UCM caption</h2><table><thead><tr><th align="left">图像大小</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>名词数</th><th>句子总数</th><th>单样本句数</th></tr></thead><tbody><tr><td align="left">256 * 256 * 3</td><td>2100</td><td>USGS National Map</td><td>2016</td><td>中科院</td><td>21</td><td>10500</td><td>5</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/201528014227051/RSICD_optimal">https://github.com/201528014227051/RSICD_optimal</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/90740">https://aistudio.baidu.com/aistudio/datasetdetail/90740</a></li></ul><h2 id="Sydney-caption"><a href="#Sydney-caption" class="headerlink" title="Sydney caption"></a>Sydney caption</h2><table><thead><tr><th align="left">图像大小</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>名词数</th><th>句子总数</th><th>单样本句数</th></tr></thead><tbody><tr><td align="left">500 * 500 * 3</td><td>613</td><td>Google Earth</td><td>2016</td><td>中科院</td><td>7</td><td>3065</td><td>5</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/201528014227051/RSICD_optimal">https://github.com/201528014227051/RSICD_optimal</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/91126">https://aistudio.baidu.com/aistudio/datasetdetail/91126</a></li></ul><h2 id="RSICD"><a href="#RSICD" class="headerlink" title="RSICD"></a>RSICD</h2><table><thead><tr><th align="left">图像大小</th><th>图像总数</th><th>数据源</th><th>发布时间</th><th>发布组织</th><th>名词数</th><th>句子总数</th><th>单样本句数</th></tr></thead><tbody><tr><td align="left">224 * 244 * 3</td><td>10921</td><td>Google Earth, Baidu Map, MapABC, Tianditu</td><td>2017</td><td>IEEE</td><td>30</td><td>2433</td><td>5</td></tr></tbody></table><ul><li>源地址：<a href="https://github.com/201528014227051/RSICD_optimal">https://github.com/201528014227051/RSICD_optimal</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/90307">https://aistudio.baidu.com/aistudio/datasetdetail/90307</a></li></ul><h1 id="遥感影像视频目标检测"><a href="#遥感影像视频目标检测" class="headerlink" title="遥感影像视频目标检测"></a>遥感影像视频目标检测</h1><h1 id="遥感影像视频跟踪"><a href="#遥感影像视频跟踪" class="headerlink" title="遥感影像视频跟踪"></a>遥感影像视频跟踪</h1><p><strong>收集网络中开源的、关于图像标题的深度学习数据集</strong></p><h2 id="UAV123"><a href="#UAV123" class="headerlink" title="UAV123"></a>UAV123</h2><table><thead><tr><th align="left">视频大小</th><th>视频帧数</th><th>数据源</th><th>发布时间</th><th>发布组织</th></tr></thead><tbody><tr><td align="left">720 * 1280 * 3, jpg; 123 video sub-sequences</td><td>110,000 + frames, 10/30 FPS</td><td>UAV (DJIS1000)、UAV simulator</td><td>2017</td><td>King Abdullah University of Science and Technology</td></tr></tbody></table><ul><li>源地址：<a href="https://cemse.kaust.edu.sa/ivul/uav123">https://cemse.kaust.edu.sa/ivul/uav123</a> </li><li>aistudio备份链接：<a href="https://aistudio.baidu.com/aistudio/datasetdetail/91853">https://aistudio.baidu.com/aistudio/datasetdetail/91853</a></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>文章介绍</title>
      <link href="/2018/09/07/shi-li/"/>
      <url>/2018/09/07/shi-li/</url>
      
        <content type="html"><![CDATA[<p>主题使用指南：<a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md">https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md</a></p>]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Typora </tag>
            
            <tag> Markdown </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
